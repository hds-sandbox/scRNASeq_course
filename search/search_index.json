{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction to single cell RNAseq sequencing data analysis","text":"<p>Updated: 28/02/2023</p> <p>This course is organized as the analysis project of a dataset with cells from human testis representing the spermatogenesis process. We will also make a comparative analysis against a dataset from a person affected by azoospermia (missing or faulty production of spermatozoa). You will learn to use the tools for integrating and analyzing multiple datasets in both <code>python</code> and <code>R</code> with the use of interactive coding on <code>Rstudio</code> or <code>Jupyter Notebooks</code>. </p> <p></p> Authors <p>Samuele Soraggi</p> <p>  Data Scientist </p> <p>J.A. Romero Herrera</p> <p>  Data Scientist </p> <p>Overview</p> <p> Syllabus: </p> <ol> <li>Data explanation</li> <li>Read normalization and QC (python: scanpy | R: Seurat)</li> <li>Exploratory analysis and clustering (python: scanpy | R: Seurat)</li> <li>Differential Expression Analysis (python: scanpy | R: Seurat)</li> <li>Pseudotime and trajectories (python: scanpy/Palantir | R: Seurat/Slingshot)</li> <li>Functional Analysis (python: scanpy/gseapy) </li> </ol> <p> Total Time Estimation: 6 hours  </p> <p> Supporting Materials: </p> <p> Target Audience: PhD, MsC, etc.</p> <p> Level: Beginner/Intermediate/Advanced</p> <p> License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.</p> <p> Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).</p> <p>Course Requirements</p> <ul> <li>Knowledge of R, Rstudio and Rmarkdown or Python and Jupyter Notebooks.</li> <li>Basic knowledge of scRNAseq technology</li> <li>Basic knowledge of data science and statistics such as PCA, clustering and statistical testing</li> </ul> <p>Goals</p> <p>By the end of this workshop you will be able to:</p> <ul> <li>Understand your single cell count matrix</li> <li>QC and normalize your data</li> <li>Visualize scRNAseq data using UMAP</li> <li>Cluster your cells</li> <li>Perform Differential Expression Analysis</li> <li>Perform Pseudotime and Cell Fate Analysis</li> <li>Perform Functional Analysis</li> </ul>"},{"location":"index.html#introduction-to-single-cell-rnaseq-sequencing-data-analysis","title":"Introduction to single cell RNAseq sequencing data analysis","text":""},{"location":"index.html#biological-background-and-motivation","title":"Biological background and motivation","text":"<p>The testis is a complex organ composed of multiple cell types: germ cells at different stages of maturation and several somatic cell types supporting testicular structure and spermatogen-esis; Sertoli cells, peritubular cells, Leydig cells and other interstitial cells, as outlined in the figure below.</p> <p></p> <p>Technological developments have recently made it possible to perform single-cell RNA sequencing (scRNAseq) of all cell types in a tissue. Understanding how scRNAseq data is processed and how to interpret the data is crucial for our ability to draw correct biological conclusions.</p> <p>In this introduction we will preprocess and integrate multiple datasets together and perform an analysis to detect cell types, developmental stages, genes dominating in the various cell types, in a way that the spermatogenic process can be framed and characterized as an output of this course.</p> <p>Infertility is a growing problem, especially in the Western world, where approximately 10\u201315% of couples are infertile. In line with this, we have observed a tremendous increase in the use of assisted reproductive techniques (ART) and espe-cially Intracytoplasmic Sperm Injection.  In about half of the infertile couples, the cause involves a male-factor (Agarwal et\u00a0al. 2015; Barratt et\u00a0al. 2017). One of the most severe forms of male infertility is azoo-spermia (from Greek azo - meaning \u201cwithout life\u201d) where no spermatozoa can be detected in the ejaculate, which ren-ders biological fatherhood difficult. Azoospermia is found in approximately 10\u201315% of infertile men (Jarow et\u00a0al. 1989; Olesen et\u00a0al. 2017) and the aetiology is thought to be primarily genetic.  </p> <p>As the last step of this course, we will use a dataset of azoospermic cells to make a comparative analysis against the healthy cells. We will try to find which developmental processes are more highlighted in only one of the two datasets, and see which other differences characterize the azoospermic single cell data.</p>"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<ul> <li>Center for Health Data Science, University of Copenhagen.</li> </ul>"},{"location":"contributors.html","title":"Contributors","text":"<p>Jose Alejandro Romero Herrera  </p>","tags":["authors","contributors"]},{"location":"contributors.html#credit-table","title":"CRediT table","text":"CRediT role Initials Conceptualization Data curation Formal Analysis Funding acquisition Investigation Methodology Project administration Resources Software Supervision Validation Visualization Writing - original draft Writing - review &amp; editing","tags":["authors","contributors"]},{"location":"keywords.html","title":"Keywords","text":"<p>Here's a lit of used keywords:</p>"},{"location":"keywords.html#authors","title":"authors","text":"<ul> <li>Contributors</li> </ul>"},{"location":"keywords.html#contributors","title":"contributors","text":"<ul> <li>Contributors</li> </ul>"},{"location":"R/Part01_read_the_data.html","title":"Read the data","text":""},{"location":"R/Part01_read_the_data.html#read-the-data","title":"Read the data","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 30-60 minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Get an overview of the <code>Seurat</code> package and the <code>R</code> language syntax</li> <li>Learn and explore the data structure containing a single cell dataset</li> <li>Understand and apply basic interactions with the transcript matrix and the components of a dataset</li> </ol> <p>We will use <code>Seurat</code> as the main tool for the analysis, where we will also apply some other packages. Seurat has a comprehensive manual webpage that includes many different tutorials you can use for further practicing. Needed packages for this workshop are loaded with the function <code>library()</code>, which will make the tools available for us.</p> <p>An alternative and well-established tool for <code>R</code> users is scanpy. This is used in the <code>python</code> version of this course.</p>"},{"location":"R/Part01_read_the_data.html#setup","title":"Setup","text":"<pre><code>library(tidyverse)\nlibrary(Seurat)\nlibrary(SeuratDisk)\nlibrary(patchwork)\n</code></pre> <p>The Seurat functions will help us run all the necessary steps of the analysis from preprocessing, to clustering and visualization. There are also several helper and wrapper functions from other libraries that we will use to run the analysis. Use the <code>help()</code> function to see what another function does, and it should appear on the <code>help tab</code></p> <pre><code>help(FindClusters)\n</code></pre>"},{"location":"R/Part01_read_the_data.html#loading-datasets","title":"Loading datasets","text":"<p>Data can be loaded from different possible formats, which usually has as a dedicated function for loading. For example, the <code>Read10X()</code> function reads in the output of the cellranger pipeline from 10X, returning a unique molecular identified (UMI) count matrix. The values in this matrix represent the number of molecules for each feature (i.e.\u00a0gene; row) that are detected in each cell (column).</p> <pre><code>sample_2 &lt;- Read10X(data.dir = \"../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/cellranger_sample2/outs/filtered_feature_bc_matrix/\")\nsample_3 &lt;- Read10X(data.dir = \"../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/cellranger_sample3/outs/filtered_feature_bc_matrix/\")\n</code></pre> <p>We next use the count matrix to create a <code>Seurat</code> object, which is the main data structure used in the study.</p> <pre><code># Initialize the Seurat object with the raw (non-normalized data).\nsample_2 &lt;- CreateSeuratObject(counts = sample_2, project = \"spermatogenesis\")\nsample_3 &lt;- CreateSeuratObject(counts = sample_3, project = \"spermatogenesis\")\n</code></pre> What does data in a count matrix look like? <pre><code># Lets examine a few genes in the first thirty cells\nsample_2[c(1:3), 1:30]\n</code></pre> <p>The <code>.</code> values in the matrix represent 0s (no molecules detected). Since most values in an scRNA-seq matrix are 0, Seurat uses a sparse-matrix representation whenever     possible. This results in significant memory and speed savings for Drop-seq/inDrop/10x data.</p> <pre><code>dense.size &lt;- object.size(as.matrix(sample_2)) dense.size sparse.size &lt;- object.size(sample_2) sparse.size dense.size / sparse.size\n</code></pre>"},{"location":"R/Part01_read_the_data.html#seurat-object-structure","title":"Seurat object structure","text":"<p>Info</p> <p>For a technical discussion of the <code>Seurat</code> object structure, check out the Seurat GitHub Wiki.</p> <p>The <code>Seurat</code> object structure is a bit technical, but in a nutshell, it serves as a container that holds separate parts of single cell analyses:</p> <ul> <li>The <code>Assay</code> sub-object contains data (like the count matrix).</li> <li>The <code>DimReduc</code> sub-object holds the analyses performed on the data (like PCA, or clustering results).</li> </ul> <p>In addition, each of these objects (<code>Seurat</code>, <code>Assay</code> and <code>DimReduc</code>) are also divided in separate <code>Slots</code> that contain specific information within them. These slots can be accessed using the <code>@</code> operator (e.g <code>sample_2@assays</code>).</p> <pre><code>slotNames(sample_2)\nsample_2@assays\n</code></pre> <p>For example, the raw count matrix is stored in <code>sample_2[[\"RNA\"]]@counts</code>. <code>sample[[\"RNA\"]]</code> directs to the <code>Assay</code> sub-object called \"RNA\", which contains the slot <code>@counts</code>. Normalized and scaled counts will be saved in their specific slots (<code>data</code> and <code>scale.data</code>, respectively).</p> <pre><code>head(sample_2[[\"RNA\"]]@counts)\n</code></pre> <p>We will look into these slots as they are needed through the course, so do not worry about it! Nevertheless, here are some of the most useful ones for the <code>Seurat</code> object:</p> Slot Function <code>assays</code> A list of assays within this object, such as RNAseq <code>meta.data</code> Contains cell-level meta data <code>active.assay</code> Name of active, or default, assay <code>active.ident</code> Identity classes for the current object <code>reductions</code> A list of DimReduc objects, like PCA or UMAP <code>project.name</code> User-defined project name (optional)"},{"location":"R/Part01_read_the_data.html#exploring-the-seurat-object","title":"Exploring the Seurat Object","text":"<p>Summary information about <code>Seurat</code> objects can be had quickly and easily using standard R functions. Object shape/dimensions can be found using the <code>dim</code>, <code>ncol</code>, and <code>nrow</code> functions; cell and feature names can be found using the <code>colnames</code> and <code>rownames</code> functions, respectively, or the <code>dimnames</code> function. A vector of names of <code>Assay</code> and <code>DimReduc</code> objects contained in a <code>Seurat</code> object can be obtained by using <code>names</code>.</p> <pre><code>sample_2\n</code></pre> <p><code>nrow()</code> and <code>ncol()</code> provide the number of features and cells in the active assay, respectively. <code>dim()</code> provides both nrow and ncol at the same time:</p> <pre><code>dim(x = sample_2)\n</code></pre> <pre><code>head(x = rownames(x = sample_2))\n</code></pre> <p>In addition to rownames and colnames, one can use <code>dimnames()</code>, which provides a two-length list with both rownames and colnames:</p> <pre><code>head(x = colnames(x = sample_2))\n</code></pre>"},{"location":"R/Part01_read_the_data.html#assay-object","title":"Assay object","text":"<p>For typical scRNA-seq experiments, a Seurat object will have a single Assay (\"RNA\"). This assay will also store multiple \u2018transformations\u2019 of the data, including raw counts (@counts slot), normalized data (@data slot), and scaled data for dimensional reduction (@scale.data slot).</p> <p>For more complex experiments, an object could contain multiple assays. These could include multi-modal data types (CITE-seq antibody-derived tags, ADTs), or imputed/batch-corrected measurements. Each of those assays has the option to store the same data transformations as well.</p> <p>The slots of the <code>Assay</code> object contain the count matrix and its transformations, but it can also contain gene metadata</p> Slot Function <code>counts</code> Stores unnormalized data such as raw counts or TPMs <code>data</code> Normalized data matrix <code>scale.data</code> Scaled data matrix <code>meta.features</code> Feature-level meta data"},{"location":"R/Part01_read_the_data.html#accessing-data-from-the-seurat-object","title":"Accessing data from the Seurat Object","text":"<p>Pulling specific <code>Assay</code>, objects can be done with the double <code>[[</code> extract operator. Adding new objects to a <code>Seurat</code> object is also done with the double <code>[[</code> extract operator; Seurat will figure out where in the <code>Seurat</code> object a new associated object belongs.</p> <p>If you want to know all the objects inside the <code>Seurat</code> object, you can use the <code>names()</code> function. These can be passed to the double <code>[[</code> extract operator to pull them from the Seurat object:</p> <pre><code>names(x = sample_2)\nsample_2[[\"RNA\"]]\n</code></pre> <p>Accessing data from an <code>Seurat</code> object is done with the <code>GetAssayData</code> function. Adding expression data to either the <code>counts</code>, <code>data</code>, or <code>scale.data</code> slots can be done with <code>SetAssayData</code>. New data must have the same cells in the same order as the current expression data. Data added to <code>counts</code> or <code>data</code> must have the same features as the current expression data.</p> <pre><code>GetAssayData(object = sample_2, slot = 'scale.data')[1:3, 1:3]\n</code></pre> <p>Cell-level meta data can be accessed with the single <code>[[</code> extract operator or using the <code>$</code> sigil. Pulling with the <code>$</code> sigil means only one bit of meta data can be pulled at a time, though tab-autocompletion has been enabled for it, making it ideal for interactive use. Adding cell-level meta data can be set using the single <code>[[</code> extract operator as well, or by using <code>AddMetaData</code>.</p> <pre><code># Cell-level meta data is stored as a data frame\n# Standard data frame functions work on the meta data data frame\ncolnames(x = sample_2[[]])\n</code></pre> <pre><code># The $ sigil can only pull bit of meta data at a time; however, tab-autocompletion\n# has been enabled for the $ sigil, making it ideal for interactive use\nhead(x = sample_2)\n</code></pre> <p>Note</p> <p>We will take a look at the <code>DimReduc</code> object in the next sessions!</p>"},{"location":"R/Part01_read_the_data.html#subsetting-data","title":"Subsetting data","text":"<p>You can subset Seurat objects using the function <code>subset()</code>. You can subset based on metadata, expression level of a gene or an identity class. Here you can find some examples:</p> <pre><code># Subset Seurat object based on a cell identity class\nsubset(x = sample_2, idents = \"B cells\")\nsubset(x = sample_2, idents = c(\"CD4 T cells\", \"CD8 T cells\"), invert = TRUE)\n# Subset on the expression level of a gene/feature\nsubset(x = sample_2, subset = MS4A1 &gt; 3)\n# Subset on a combination of criteria\nsubset(x = sample_2, subset = MS4A1 &gt; 3 &amp; PC1 &gt; 5)\nsubset(x = sample_2, subset = MS4A1 &gt; 3, idents = \"B cells\")\n# Subset on a value in the object meta data\nsubset(x = sample_2, subset = orig.ident == \"Replicate1\")\n# Downsample the number of cells per identity class\nsubset(x = sample_2, downsample = 100)\n</code></pre>"},{"location":"R/Part01_read_the_data.html#saving-and-loading-a-seurat-object","title":"Saving and loading a Seurat object","text":"<p>You can save the Seurat object using the H5Seurat file format:</p> <pre><code>SaveH5Seurat(sample_2, filename = \"sample_2.h5Seurat\")\n</code></pre> <p>If you plan to work with <code>scanpy</code>, you might want to convert H5Seurat into h5ad format. This is easily done using the <code>Convert()</code> function. You can, of course, transform an h5ad file into H5Seurat using the same function</p> <pre><code>Convert(\"sample_2.h5Seurat\", dest = \"h5ad\")\nConvert(\"sample_2.h5ad\", dest = \"h5seurat\", overwrite = TRUE)\n</code></pre> <p>To load a H5Seurat object, use the following function <code>LoadH5Seurat()</code>:</p> <pre><code>LoadH5Seurat(\"sample_2.h5seurat\")\n</code></pre> <p>Then you can use it in python!</p> <pre><code>import scanpy\nadata = scanpy.read_h5ad(\"sample_2.h5ad\")\nadata\n</code></pre>"},{"location":"R/Part02_filtering_sample2.html","title":"Quality Control (QC) and filtering","text":""},{"location":"R/Part02_filtering_sample2.html#qc-and-cell-filtering","title":"QC and cell filtering","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 40 minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Understand and discuss QC issues and measures from single cell data</li> <li>Explore QC graphs and set filtering tools and thresholds</li> <li>Analyze the results of QC filters and evaluate necessity for different filtering </li> </ol> <p>Quality control and filtering is the most important steps of single cell data analysis. Allowing low quality cells into your analysis will compromise/mislead your conclusions by adding hundreds of meaningless data points to your workflow. The main sources of low quality cells are:</p> <ul> <li>broken cells for which some of their transcripts get lost</li> <li>cells isolated together with too much ambient RNA</li> <li>missing cell during isolation (e.g.\u00a0empty droplet in microfluidic machines)</li> <li>multiple cells isolated together (multiplets, usually only two cells - doublets)</li> </ul>"},{"location":"R/Part02_filtering_sample2.html#qc-metrics","title":"QC metrics","text":"<p>Seurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria. A few QC metrics commonly used by the community include:</p> <ul> <li> <p>The number of unique genes detected in each cell.</p> </li> <li> <p>Low-quality cells or empty droplets will often have very few genes</p> </li> <li> <p>Cell doublets or multiplets may exhibit an aberrantly high gene count</p> </li> <li> <p>Similarly, the total number of molecules detected within a cell (correlates strongly with unique genes)</p> </li> <li> <p>The percentage of reads that map to the mitochondrial genome</p> </li> <li> <p>Low-quality / dying cells often exhibit extensive mitochondrial contamination</p> </li> </ul> <p>The number of unique genes and total molecules are automatically calculated during <code>CreateSeuratObject()</code>. However, we calculate mitochondrial QC metrics ourselves, since we need an identifier for mitochondrial genes, and there is not a standard one. In our case, we can use the <code>PercentageFeatureSet()</code> function, which calculates the percentage of counts originating from a set of features, using all genes starting with <code>MT-</code> as a set of mitochondrial genes</p> <pre><code># The [[ operator can add columns to object metadata. This is a great place to stash QC stats\nsample_2[[\"percent.mt\"]] &lt;- PercentageFeatureSet(sample_2, pattern = \"^MT-\")\n</code></pre> Where are QC metrics stored in Seurat? <p>Cell QC metrics are stored in the @meta.data slot of a Seurat Object</p> <pre><code>head(sample_2@meta.data, 5)\n</code></pre>"},{"location":"R/Part02_filtering_sample2.html#visualize-and-evaluate-quality-measures","title":"Visualize and evaluate quality measures","text":""},{"location":"R/Part02_filtering_sample2.html#violin-plots","title":"Violin plots","text":"<p>We can make use of violin plots to show the distribution of values for different metrics in our data. This is done using the <code>VlnPlot()</code> function. The function can take several metrics all together and produce a combined plot with our metrics of interest:</p> <pre><code>VlnPlot(sample_2, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\n</code></pre> <p>Here we simply look at the distribution of transcripts per cells, detected genes per cell and mithocondrial %. Note how the distribution is bimodal. This usually denotes a cluster of low-quality cells and viable cells. Sometimes filtering out the data points on the bottom-most sides of those graphs removes a lot of cells from a dataset, but this is quite a normal thing, and there is no need to be worried about it. The top side of the distributions show a tail with few cells having a lot of transcripts and genes. It is also good to filter out some of those extreme values - for technical reasons, it will also help in having a better normalization of the data later on.</p> <p>In this dataset there are few cell with a high percentage of mitocondrial content. Those are precisely 245 if we set 0.1 (that is 10%) as a threshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> <p>We can do some plots to have a look at quality measures combined together.</p>"},{"location":"R/Part02_filtering_sample2.html#counts-vs-genes","title":"Counts vs Genes","text":"<p>This is a typical plot, where you look at the total transcripts per cells (x axis) and detected genes per cell (y axis). Usually, those two measures grow together. Points with a lot of transcripts and genes might be multiplets (multiple cells sequenced together as one), while very few transcripts and genes denote the presence of only ambient RNA or very low quality sequencing of a cell.</p> <p>In order to create this plot, we make use of the <code>FeatureScatter()</code> function:</p> <pre><code># FeatureScatter is typically used to visualize feature-feature relationships, but can be used for anything calculated by the object, i.e. columns in object metadata, PC scores etc.\nFeatureScatter(sample_2, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") </code></pre> <p>??? How to color dots with a continuous variable?</p> <pre><code>Unfortunately, Seurat does not support yet that functionality. We will need to use the function `FetchData()` (works with gene expression values as well) or select the @meta.data object. Then, we can create a custom plot with ggplot2.\n\n\n```r\ndata &lt;- FetchData(sample_2, vars = c(\"nCount_RNA\", \"nFeature_RNA\", \"percent.mt\"))\ndata &lt;- sample_2@meta.data\n\ndata %&gt;% ggplot(aes(x = nCount_RNA, y = nFeature_RNA, color = percent.mt)) + geom_point() + scale_color_viridis_c()\n```\n</code></pre>"},{"location":"R/Part02_filtering_sample2.html#filtering-cells-and-choosing-thresholds","title":"Filtering cells and choosing thresholds","text":"<p>Mitocondrial content: In this dataset there are few cells with a high percentage of mitocondrial content. Those are precisely 245 if we set 10% as a threshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> <p>In addition, we will filter cells that are in both extremes of the distributions for number of detected genes and number of total transcripts per cell. This this case, it seems reasonable to exclude:</p> <ul> <li>Cells that have unique counts over 30000 or less than 5000</li> <li>Cells that have unique detected genes over 6000 or below 2000.</li> </ul> <p>Once we have chosen our thresholds, we can filter our dataset using the <code>subset()</code> function. The argument <code>subset</code> allows us to create a filtering parameter using our QC metrics.</p> <pre><code>MIN_COUNTS = 5000  #minimum number of transcripts per cell\nMAX_COUNTS = 30000 #maximum number of transcripts per cell\nMIN_GENES = 2000   #minimum number of genes per cell\nMAX_GENES = 6000   #maximum number of genes per cell\nMAX_MITO = 10      #mitocondrial percentage treshold)\nsample_2 &lt;- subset(sample_2, subset = nFeature_RNA &gt; MIN_GENES &amp; nFeature_RNA &lt; MAX_GENES &amp;\nnCounts_RNA &gt; MIN_COUNTS &amp; nCounts_RNA &lt; MAX_COUNTS &amp;\npercent.mt &lt; 10)\n</code></pre> <p>We can now recheck our QC plots:</p> <pre><code>VlnPlot(sample_2, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\nFeatureScatter(sample_2, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") </code></pre>"},{"location":"R/Part02_filtering_sample2.html#doublet-filtering","title":"Doublet filtering","text":"<p>Another important step consists in filtering out multiplets. Those are in the almost totality of the cases doublets, because triplets and above multiplets are extremely rare. Read this more technical blog post for more explanations about this.</p> <p>The external tool <code>scrublet</code> simulates doublets by putting together the transcripts of random pairs of cells from the dataset. Then, it assigns a score to each cell in the data, based on the similarity with the simulated doublets. An <code>expected_doublet_rate</code> of 0.06 (6%) is quite a typical value for single cell data, but if you have a better estimate from laboratory work, microscope imaging or a specific protocol/sequencing machine, you can also tweak the value.</p> <pre><code>sample_2 &lt;- scrublet_R(pbmc3k, python_home = \"/opt/miniconda3/bin/python\")\n</code></pre> <p>It seems that the doublet rate is likely to be lower than 6%, meaning that in this regard the data has been produced pretty well. We now plot the doublet scores assigned to each cell by the algorithm. We can see that most cells have a low score (the score is a value between 0 and 1). Datasets with many doublets show a more bimodal distribution, while here we just have a light tail beyond 0.1.</p> <pre><code>VlnPlot(sample_2, features = \"doublet_score\")\n</code></pre> <p>We can choose 0.1 as filtering treshold for the few detected doublets or alternatively use the automatic selection of doublets by the algorithm. We will choose the last option and use the automatically chosen doublets.</p> <pre><code>sample_2 &lt;- subset(sample_2, subset = doublet_score == FALSE)\n</code></pre>"},{"location":"R/Part02_filtering_sample2.html#filtering-evaluation","title":"Filtering evaluation","text":"<p>A quite basic but easy way to look at the results of our filtering is to normalize and plot the dataset on some projections. We will see these steps in the next lessons with more detail, but just to be able to follow up, we use a standard normalization technique:</p> <ul> <li>TPM normalization: the transcripts of each cell are normalized, so that their total amounts to the same value in each cell. This should make cells more comparable independently of how many transcripts has been retained during cell isolation.</li> <li>Logarithmization: the logarithm of the normalized transcripts is calculated. This reduce the variability of transcripts values and highlights variations due to biological factors.</li> <li>Scaling: Each gene is scaled across all cells. This is useful, for example, for projecting the data onto a PCA.</li> </ul> <p>TPM normalization and its logarithm are done within a single step, using the <code>NormalizeData()</code> function, while scaling is done using the <code>ScaleData()</code> function:</p> <pre><code>sample_2 &lt;- NormalizeData(sample_2, normalization.method = \"LogNormalize\")\nsample_2 &lt;- ScaleData(sample_2)\n</code></pre> <p>Then, in order to reduce computational times, we select the most variable genes (MVG) to perform PCA and visualization. MVG are calculated based on the LogNormalized reads. We will select the top 2000 MVGs</p> <pre><code>sample_2 &lt;- FindVariableFeatures(sample_2, nfeatures = 2000)\n</code></pre> <p>Now we perform PCA. We select our calculated variable genes using the <code>VariableFeatures()</code> function, and visualize the PCA projection with the <code>FeaturePlot()</code> function. We can color the dots using one of our QC metrics using the <code>features</code> parameter:</p> <pre><code>sample_2 &lt;- RunPCA(sample_2, features = VariableFeatures(object = sample_2))\nFeaturePlot(sample_2, reduction = 'pca', features = \"nCount_RNA\")\n</code></pre> <p>We can already see how the PCA has a clear structure with only a few dots sparsed around. It seems the filtering gave a good result.</p> <p>Next, we need to select how many Principal Components we should use for visualizing the data using UMAP. We plot the variance ratio to see how each component of the PCA changes in variability using the Elbow method. The explained variance of each component can be plot using the function <code>ElbowPlot()</code>:</p> <pre><code>ElbowPlot(sample_2)\n</code></pre> <p>We can choose a threshold to be used in all algorithms that use PCA to calculate any quantity. for example, we can select 15 PCA components, since the amount of variability explained dramatically drops after 10 PCs, more or less.</p> <p>Finally, We project the data using the UMAP algorithm. This is very good in preserving the structure of a dataset in low dimension, if any is present. We first calculate the neighbors of each cell (that is, its most similar cells), those are then used for the UMAP. The neighbors are calculated using the PCA matrix instead of the full data matrix, so we can choose the number of PCA components to use. Many algorithms work on the PCA, so you will see the parameter used again in other places.</p> <pre><code>sample_2 &lt;- RunUMAP(sample_2, dims = 1:10)\nFeaturePlot(sample_2, reduction = \"umap\", features = \"nCount_RNA\")\n</code></pre> <p>The UMAP plot gives a pretty well-structured output for this dataset. We will keep working further with this filtering.</p> <pre><code>SaveH5Seurat(sample_2, filename = \"../../Data/notebooks_data/sample_2.filt.h5Seurat\")\n</code></pre>"},{"location":"R/Part02_filtering_sample2.html#wrapping-up","title":"Wrapping up","text":"<p>We have succesfully gone through the filtering of a single cell dataset with good results that can be used further in the data analysis. In the next notebook <code>Normalize and Integrate</code>, we will integrate this dataset (testis cells from a healthy adult man) with the same type of sample from another man. Filtering of the other dataset is in the notebook <code>Part02_filtering_sample3.ipynb</code>. Run the notebook to generate the filtered dataset. The procedure follows tightly what happens for the dataset we just filtered.</p> <p>Filtering a low quality sample</p> <p>As you could see, this dataset seemed pretty ok to handle. Another dataset of much lower quality is present, and is not going to be integrated in the coming data analysis. Its preprocessing is shown as the submenu <code>Filtering a low quality sample</code> in the section <code>Extra</code> of the course webpage. In it we will also show an aggressive filtering done using a combination of PCA technique and automatic outliers detection.</p>"},{"location":"R/Part02_filtering_sample3.html","title":"Quality Control (QC) and filtering","text":""},{"location":"R/Part02_filtering_sample3.html#qc-and-cell-filtering","title":"QC and cell filtering","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 40 minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Understand and discuss QC issues and measures from single cell data</li> <li>Explore QC graphs and set filtering tools and thresholds</li> <li>Analyze the results of QC filters and evaluate necessity for different filtering </li> </ol> <p>Quality control and filtering is the most important steps of single cell data analysis. Allowing low quality cells into your analysis will compromise/mislead your conclusions by adding hundreds of meaningless data points to your workflow. The main sources of low quality cells are:</p> <ul> <li>broken cells for which some of their transcripts get lost</li> <li>cells isolated together with too much ambient RNA</li> <li>missing cell during isolation (e.g.\u00a0empty droplet in microfluidic machines)</li> <li>multiple cells isolated together (multiplets, usually only two cells - doublets)</li> </ul>"},{"location":"R/Part02_filtering_sample3.html#qc-metrics","title":"QC metrics","text":"<p>Seurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria. A few QC metrics commonly used by the community include:</p> <ul> <li> <p>The number of unique genes detected in each cell.</p> </li> <li> <p>Low-quality cells or empty droplets will often have very few genes</p> </li> <li> <p>Cell doublets or multiplets may exhibit an aberrantly high gene count</p> </li> <li> <p>Similarly, the total number of molecules detected within a cell (correlates strongly with unique genes)</p> </li> <li> <p>The percentage of reads that map to the mitochondrial genome</p> </li> <li> <p>Low-quality / dying cells often exhibit extensive mitochondrial contamination</p> </li> </ul> <p>The number of unique genes and total molecules are automatically calculated during <code>CreateSeuratObject()</code>. However, we calculate mitochondrial QC metrics ourselves, since we need an identifier for mitochondrial genes, and there is not a standard one. In our case, we can use the <code>PercentageFeatureSet()</code> function, which calculates the percentage of counts originating from a set of features, using all genes starting with <code>MT-</code> as a set of mitochondrial genes</p> <pre><code># The [[ operator can add columns to object metadata. This is a great place to stash QC stats\nsample_3[[\"percent.mt\"]] &lt;- PercentageFeatureSet(sample_3, pattern = \"^MT-\")\n</code></pre> Where are QC metrics stored in Seurat? <p>Cell QC metrics are stored in the @meta.data slot of a Seurat Object</p> <pre><code>head(sample_3@meta.data, 5)\n</code></pre>"},{"location":"R/Part02_filtering_sample3.html#visualize-and-evaluate-quality-measures","title":"Visualize and evaluate quality measures","text":""},{"location":"R/Part02_filtering_sample3.html#violin-plots","title":"Violin plots","text":"<p>We can make use of violin plots to show the distribution of values for different metrics in our data. This is done using the <code>VlnPlot()</code> function. The function can take several metrics all together and produce a combined plot with our metrics of interest:</p> <pre><code>VlnPlot(sample_3, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\n</code></pre> <p>Here we simply look at the distribution of transcripts per cells, detected genes per cell and mithocondrial %. Note how the distribution is bimodal. This usually denotes a cluster of low-quality cells and viable cells. Sometimes filtering out the data points on the bottom-most sides of those graphs removes a lot of cells from a dataset, but this is quite a normal thing, and there is no need to be worried about it. The top side of the distributions show a tail with few cells having a lot of transcripts and genes. It is also good to filter out some of those extreme values - for technical reasons, it will also help in having a better normalization of the data later on.</p> <p>In this dataset there are few cell with a high percentage of mitocondrial content. Those are precisely 245 if we set 0.1 (that is 10%) as a threshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> <p>We can do some plots to have a look at quality measures combined together.</p>"},{"location":"R/Part02_filtering_sample3.html#counts-vs-genes","title":"Counts vs Genes","text":"<p>This is a typical plot, where you look at the total transcripts per cells (x axis) and detected genes per cell (y axis). Usually, those two measures grow together. Points with a lot of transcripts and genes might be multiplets (multiple cells sequenced together as one), while very few transcripts and genes denote the presence of only ambient RNA or very low quality sequencing of a cell.</p> <p>In order to create this plot, we make use of the <code>FeatureScatter()</code> function:</p> <pre><code># FeatureScatter is typically used to visualize feature-feature relationships, but can be used for anything calculated by the object, i.e. columns in object metadata, PC scores etc.\nFeatureScatter(sample_3, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") </code></pre> <p>??? How to color dots with a continuous variable?</p> <pre><code>Unfortunately, Seurat does not support yet that functionality. We will need to use the function `FetchData()` (works with gene expression values as well) or select the @meta.data object. Then, we can create a custom plot with ggplot2.\n\n\n```r\ndata &lt;- FetchData(sample_3, vars = c(\"nCount_RNA\", \"nFeature_RNA\", \"percent.mt\"))\ndata &lt;- sample_3@meta.data\n\ndata %&gt;% ggplot(aes(x = nCount_RNA, y = nFeature_RNA, color = percent.mt)) + geom_point() + scale_color_viridis_c()\n```\n</code></pre>"},{"location":"R/Part02_filtering_sample3.html#filtering-cells-and-choosing-thresholds","title":"Filtering cells and choosing thresholds","text":"<p>Mitocondrial content: In this dataset there are few cells with a high percentage of mitocondrial content. Those are precisely 245 if we set 10% as a threshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> <p>In addition, we will filter cells that are in both extremes of the distributions for number of detected genes and number of total transcripts per cell. This this case, it seems reasonable to exclude:</p> <ul> <li>Cells that have unique counts over 30000 or less than 5000</li> <li>Cells that have unique detected genes over 6000 or below 2000.</li> </ul> <p>Once we have chosen our thresholds, we can filter our dataset using the <code>subset()</code> function. The argument <code>subset</code> allows us to create a filtering parameter using our QC metrics.</p> <pre><code>MIN_COUNTS = 5000  #minimum number of transcripts per cell\nMAX_COUNTS = 30000 #maximum number of transcripts per cell\nMIN_GENES = 2000   #minimum number of genes per cell\nMAX_GENES = 6000   #maximum number of genes per cell\nMAX_MITO = 10      #mitocondrial percentage treshold)\nsample_3 &lt;- subset(sample_3, subset = nFeature_RNA &gt; MIN_GENES &amp; nFeature_RNA &lt; MAX_GENES &amp;\nnCounts_RNA &gt; MIN_COUNTS &amp; nCounts_RNA &lt; MAX_COUNTS &amp;\npercent.mt &lt; 10)\n</code></pre> <p>We can now recheck our QC plots:</p> <pre><code>VlnPlot(sample_3, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\nFeatureScatter(sample_3, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") </code></pre>"},{"location":"R/Part02_filtering_sample3.html#doublet-filtering","title":"Doublet filtering","text":"<p>Another important step consists in filtering out multiplets. Those are in the almost totality of the cases doublets, because triplets and above multiplets are extremely rare. Read this more technical blog post for more explanations about this.</p> <p>The external tool <code>scrublet</code> simulates doublets by putting together the transcripts of random pairs of cells from the dataset. Then, it assigns a score to each cell in the data, based on the similarity with the simulated doublets. An <code>expected_doublet_rate</code> of 0.06 (6%) is quite a typical value for single cell data, but if you have a better estimate from laboratory work, microscope imaging or a specific protocol/sequencing machine, you can also tweak the value.</p> <pre><code>sample_3 &lt;- scrublet_R(pbmc3k, python_home = \"/opt/miniconda3/bin/python\")\n</code></pre> <p>It seems that the doublet rate is likely to be lower than 6%, meaning that in this regard the data has been produced pretty well. We now plot the doublet scores assigned to each cell by the algorithm. We can see that most cells have a low score (the score is a value between 0 and 1). Datasets with many doublets show a more bimodal distribution, while here we just have a light tail beyond 0.1.</p> <pre><code>VlnPlot(sample_3, features = \"doublet_score\")\n</code></pre> <p>We can choose 0.1 as filtering treshold for the few detected doublets or alternatively use the automatic selection of doublets by the algorithm. We will choose the last option and use the automatically chosen doublets.</p> <pre><code>sample_3 &lt;- subset(sample_3, subset = doublet_score == FALSE)\n</code></pre>"},{"location":"R/Part02_filtering_sample3.html#filtering-evaluation","title":"Filtering evaluation","text":"<p>A quite basic but easy way to look at the results of our filtering is to normalize and plot the dataset on some projections. We will see these steps in the next lessons with more detail, but just to be able to follow up, we use a standard normalization technique:</p> <ul> <li>TPM normalization: the transcripts of each cell are normalized, so that their total amounts to the same value in each cell. This should make cells more comparable independently of how many transcripts has been retained during cell isolation.</li> <li>Logarithmization: the logarithm of the normalized transcripts is calculated. This reduce the variability of transcripts values and highlights variations due to biological factors.</li> <li>Scaling: Each gene is scaled across all cells. This is useful, for example, for projecting the data onto a PCA.</li> </ul> <p>TPM normalization and its logarithm are done within a single step, using the <code>NormalizeData()</code> function, while scaling is done using the <code>ScaleData()</code> function:</p> <pre><code>sample_3 &lt;- NormalizeData(sample_3, normalization.method = \"LogNormalize\")\nsample_3 &lt;- ScaleData(sample_3)\n</code></pre> <p>Then, in order to reduce computational times, we select the most variable genes (MVG) to perform PCA and visualization. MVG are calculated based on the LogNormalized reads. We will select the top 2000 MVGs</p> <pre><code>sample_3 &lt;- FindVariableFeatures(sample_3, nfeatures = 2000)\n</code></pre> <p>Now we perform PCA. We select our calculated variable genes using the <code>VariableFeatures()</code> function, and visualize the PCA projection with the <code>FeaturePlot()</code> function. We can color the dots using one of our QC metrics using the <code>features</code> parameter:</p> <pre><code>sample_3 &lt;- RunPCA(sample_3, features = VariableFeatures(object = sample_3))\nFeaturePlot(sample_3, reduction = 'pca', features = \"nCount_RNA\")\n</code></pre> <p>We can already see how the PCA has a clear structure with only a few dots sparsed around. It seems the filtering gave a good result.</p> <p>Next, we need to select how many Principal Components we should use for visualizing the data using UMAP. We plot the variance ratio to see how each component of the PCA changes in variability using the Elbow method. The explained variance of each component can be plot using the function <code>ElbowPlot()</code>:</p> <pre><code>ElbowPlot(sample_3)\n</code></pre> <p>We can choose a threshold to be used in all algorithms that use PCA to calculate any quantity. for example, we can select 15 PCA components, since the amount of variability explained dramatically drops after 10 PCs, more or less.</p> <p>Finally, We project the data using the UMAP algorithm. This is very good in preserving the structure of a dataset in low dimension, if any is present. We first calculate the neighbors of each cell (that is, its most similar cells), those are then used for the UMAP. The neighbors are calculated using the PCA matrix instead of the full data matrix, so we can choose the number of PCA components to use. Many algorithms work on the PCA, so you will see the parameter used again in other places.</p> <pre><code>sample_3 &lt;- RunUMAP(sample_3, dims = 1:10)\nFeaturePlot(sample_3, reduction = \"umap\", features = \"nCount_RNA\")\n</code></pre> <p>The UMAP plot gives a pretty well-structured output for this dataset. We will keep working further with this filtering.</p> <pre><code>SaveH5Seurat(sample_3, filename = \"../../Data/notebooks_data/sample_3.filt.h5Seurat\")\n</code></pre>"},{"location":"R/Part02_filtering_sample3.html#wrapping-up","title":"Wrapping up","text":"<p>We have succesfully gone through the filtering of a single cell dataset with good results that can be used further in the data analysis. In the next notebook <code>Normalize and Integrate</code>, we will integrate this dataset (testis cells from a healthy adult man) with the same type of sample from another man. Filtering of the other dataset is in the notebook <code>Part02_filtering_sample3.ipynb</code>. Run the notebook to generate the filtered dataset. The procedure follows tightly what happens for the dataset we just filtered.</p> <p>Filtering a low quality sample</p> <p>As you could see, this dataset seemed pretty ok to handle. Another dataset of much lower quality is present, and is not going to be integrated in the coming data analysis. Its preprocessing is shown as the submenu <code>Filtering a low quality sample</code> in the section <code>Extra</code> of the course webpage. In it we will also show an aggressive filtering done using a combination of PCA technique and automatic outliers detection.</p>"},{"location":"R/Part03_normalize_and_dim_reduc.html","title":"Normalization and dimensionality reduction","text":""},{"location":"R/Part03_normalize_and_dim_reduc.html#normalization-and-dimensionality-reduction","title":"Normalization and dimensionality reduction","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 40 minutes if using at least 2 cores (that is, at least 16 virtual-CPUs). Expect longer time with slower hardware, and a RAM usage of some 40-100GB.</p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Normalization of datasets</li> <li>Perform dimensionality reduction</li> <li>Select an adequate number of principal components</li> <li>Perform UMAP and visualization</li> <li>Understand and apply scTransformation of the data.</li> </ol> <p>Biologically similar cells are not necessarily directly comparable in a dataset because of different technical biases, amongst many the different percentage of captured transcripts (capture efficiency), the presence of technical replicates, the presence of noisy transcripts. The capture efficiency can be influenced by many factors, i.e.\u00a0the different transcript tags leading to different capture efficiency, the type of protocol used in the laboratory, the amount of PCR performed on different transcripts.</p> <p>To avoid these differences, a normalization approach is needed. Normalization is one of the main topics of scRNAseq data preprocessing, and many advanced techniques takes into account the statistical distribution of counts and the presence of technical/biological features of interest.</p> <p>The most standard approach is the TMP (Transcript Per Million) normalization followed by logarithmization and standardization. We have applied this technique to have a double-check on our data filtering in our previous notebook.</p> <p>As a rule of thumb, TPM+log+standardization is no longer consider a very good normalization technique, especially when you are integrating multiple datasets together. Instead, it is suggested to use more advanced methods for considering technical and biological covariates as part of a statistical model for the transcripts. One of the current state-of-the-art method is scTransform. We will apply it in this notebook.</p> <p>The difference between the two methods will be easily observed when performing dimensionality reduction methods and visualizing the dataset. It is therefore very important to select an adequate number of Principal Components for other more advanced techniques such as dataset integration and clustering.</p>"},{"location":"R/Part03_normalize_and_dim_reduc.html#setup","title":"Setup","text":"<pre><code>library(tidyverse)\nlibrary(patchwork)\nlibrary(Seurat)\nlibrary(SeuratDisk)\nlibrary(sctransform)\nLoadH5Seurat(\"../../Data/notebooks_data/sample_2.filt.h5Seurat\")\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#standard-approach-for-normalization-and-scaling","title":"Standard approach for normalization and scaling","text":""},{"location":"R/Part03_normalize_and_dim_reduc.html#normalizing-the-data","title":"Normalizing the data","text":"<p>After removing unwanted cells from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method \"LogNormalize\" that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in <code>sample_2[[\"RNA\"]]@data</code>.</p> <pre><code>sample_2 &lt;- NormalizeData(sample_2, normalization.method = \"LogNormalize\", scale.factor = 1e4)\n</code></pre> <p>For clarity, in this previous line of code (and in future commands), we provide the default values for certain parameters in the function call. However, this isn\u2019t required and the same behavior can be achieved with:</p> <pre><code>sample_2 &lt;- NormalizeData(sample_2)\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#identification-of-highly-variable-features-feature-selection","title":"Identification of highly variable features (feature selection)","text":"<p>We next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Seurat authors and others others have found that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.</p> <p>The procedure in Seurat is described in detail here, and improves on previous versions by directly modeling the mean-variance relationship inherent in single-cell data, and is implemented in the <code>FindVariableFeatures()</code> function. By default, it returns 2,000 features per dataset. These will be used in downstream analysis, like PCA.</p> <pre><code>sample_2 &lt;- FindVariableFeatures(sample_2, selection.method = 'vst', nfeatures = 2000)\n</code></pre> <p>We can access a dataframe with our variable features using the function <code>VariableFeatures()</code>:</p> <pre><code># Identify the 10 most highly variable genes\ntop10 &lt;- head(VariableFeatures(sample_2), 10)\n</code></pre> <p>We can plot variable features with the function <code>VariableFeaturePlot()</code>, and add labels using <code>LabelPoints()</code> function.</p> <pre><code>plot1 &lt;- VariableFeaturePlot(sample_2) # Plot without labels\nplot2 &lt;- LabelPoints(plot = plot1, points = top10, repel = TRUE) # Plot with labels. repel = TRUE will jitter the dots\nplot1 + plot2\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#scaling-the-data","title":"Scaling the data","text":"<p>Next, we apply a linear transformation (\u2018scaling\u2019) that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The <code>ScaleData()</code> function:</p> <ul> <li>Shifts the expression of each gene, so that the mean expression across cells is 0</li> <li>Scales the expression of each gene, so that the variance across cells is 1</li> <li>This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate</li> <li>The results of this are stored in <code>sample_2[[\"RNA\"]]@scale.data</code></li> </ul> <pre><code>all.genes &lt;- rownames(sample_2)\nsample_2 &lt;- ScaleData(sample_2, features = all.genes)\n</code></pre> This step takes too long! Can I make it faster? <p>Scaling is an essential step in the Seurat workflow, but only on genes that will be used as input to PCA. Therefore, the default in <code>ScaleData()</code> is only to perform scaling on the previously identified variable features (2,000 by default). To do this, omit the <code>features</code> argument in the previous function call, i.e.</p> <pre><code>sample_2 &lt;- ScaleData(sample_2)\n</code></pre> <p>Your PCA and clustering results will be unaffected. However, Seurat heatmaps (produced as shown below with <code>DoHeatmap()</code>) require genes in the heatmap to be scaled, to make sure highly-expressed genes don't dominate the heatmap. To make sure we don't leave any genes out of the heatmap later, we are scaling all genes in this tutorial. </p> <p>Remove unwanted sources of variation**</p> <p>In Seurat we can also use the <code>ScaleData()</code> function to remove unwanted sources of variation from a single-cell dataset. For example, we could 'regress out' heterogeneity associated with (for example) cell cycle stage, or mitochondrial contamination:</p> <pre><code>sample_2 &lt;- ScaleData(sample_2, vars.to.regress = 'percent.mt')\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#perform-linear-dimensional-reduction","title":"Perform linear dimensional reduction","text":"<p>Next we perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using <code>features</code> argument if you wish to choose a different subset.</p> <pre><code>sample_2 &lt;- RunPCA(sample_2, features = VariableFeatures(object = sample_2))\n</code></pre> <p>Seurat provides several useful ways of visualizing both cells and features that define the PCA, including <code>VizDimReduction()</code>, <code>DimPlot()</code>, and <code>DimHeatmap()</code></p> <pre><code># Examine and visualize PCA results a few different ways\nprint(sample_2[['pca']], dims = 1:5, nfeatures = 5)\nVizDimLoadings(sample_2, dims = 1:2, reduction = 'pca')\nDimPlot(sample_2, reduction = 'pca')\n</code></pre> <p>In particular <code>DimHeatmap()</code> allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting <code>cells</code> to a number plots the \u2018extreme\u2019 cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, we find this to be a valuable tool for exploring correlated feature sets.</p> <pre><code>DimHeatmap(sample_2, dims = 1, cells = 500,\nnfeatures = 30, # number of genes to plot\nbalanced = TRUE) # balanced = TRUE will show equal number of positive and negative loadings\n</code></pre> <pre><code>DimHeatmap(sample_2, dims = 1:15, cells = 500, nfeatures = 30, balanced = TRUE)\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#determine-the-dimensionality-of-the-dataset","title":"Determine the \u2018dimensionality\u2019 of the dataset","text":"<p>To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a \u2018metafeature\u2019 that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. However, how many components should we choose to include? 10? 20? 100?</p> <p>In Macosko et al, Seurat implemented a resampling test inspired by the JackStraw procedure. It randomly permutes a subset of the data (1% by default) and rerun PCA, constructing a \u2018null distribution\u2019 of feature scores, and repeat this procedure. Thus, it identifies \u2018significant\u2019 PCs as those who have a strong enrichment of low p-value features.</p> <pre><code># NOTE: This process can take a long time for big datasets, comment out for expediency. More approximate techniques such as those implemented in ElbowPlot() can be used to reduce computation time\nsample_2 &lt;- JackStraw(sample_2, num.replicate = 100)\nsample_2 &lt;- ScoreJackStraw(sample_2, dims = 1:20)\n</code></pre> <p>The <code>JackStrawPlot()</code> function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). \u2018Significant\u2019 PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line). In this case it appears that there is a sharp drop-off in significance after the first 10-12 PCs.</p> <pre><code>JackStrawPlot(sample_2, dims = 1:15)\n</code></pre> <p>An alternative heuristic method generates an \u2018Elbow plot\u2019: a ranking of principle components based on the percentage of variance explained by each one (<code>ElbowPlot()</code> function). In this example, we can observe an \u2018elbow\u2019 around PC9-10, suggesting that the majority of true signal is captured in the first 10 PCs.</p> <pre><code>ElbowPlot(sample_2)\n</code></pre> <p>Identifying the true dimensionality of a dataset \u2013 can be challenging/uncertain for the user. It is therefore suggested to consider three approaches. The first is more supervised, exploring PCs to determine relevant sources of heterogeneity, and could be used in conjunction with GSEA for example. The second implements a statistical test based on a random null model, but is time-consuming for large datasets, and may not return a clear PC cutoff. The third is a heuristic that is commonly used, and can be calculated instantly. In this example, all three approaches yielded similar results, but we might have been justified in choosing anything between PC 7-12 as a cutoff.</p>"},{"location":"R/Part03_normalize_and_dim_reduc.html#run-non-linear-dimensional-reduction-umaptsne","title":"Run non-linear dimensional reduction (UMAP/tSNE)","text":"<p>Seurat offers several non-linear dimensional reduction techniques, such as tSNE and UMAP, to visualize and explore these datasets. The goal of these algorithms is to learn the underlying manifold of the data in order to place similar cells together in low-dimensional space. Cells within the graph-based clusters determined above should co-localize on these dimension reduction plots. As input to the UMAP and tSNE, we suggest using the same PCs we selected in the step before.</p> <pre><code>sample_2 &lt;- RunUMAP(sample_2, dims = 1:10)\n</code></pre> <pre><code>DimPlot(sample_2, reduction = 'umap')\n</code></pre> <p>You can save the object at this point so that it can easily be loaded back in without having to rerun the computationally intensive steps performed above, or easily shared with collaborators.</p> <pre><code>SaveH5Seurat(sample_2, filename = \"../../Data/notebooks_data/sample_2.filt.norm.h5Seurat\")\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#recommended-approach-using-sctransform","title":"Recommended approach using scTransform","text":"<p>Biological heterogeneity in single-cell RNA-seq data is often confounded by technical factors, including sequencing depth. The number of molecules detected in each cell can vary significantly between cells, even within the same celltype. Interpretation of scRNA-seq data requires effective pre-processing and normalization to remove this technical variability. In Hafemeister and Satija, 2019 Seurat introduces a modeling framework for the normalization and variance stabilization of molecular count data from scRNA-seq experiment. This procedure omits the need for heuristic steps including pseudocount addition or log-transformation and improves common downstream analytical tasks such as variable gene selection, dimensional reduction, and differential expression.</p> <p>By using sctransform based normalization, we enable recovering sharper biological distinction compared to log-normalization.</p> <p>We will reload first the filtered data</p> <pre><code>LoadH5Seurat(\"../../Data/notebooks_data/sample_2.filt.h5Seurat\")\n</code></pre>"},{"location":"R/Part03_normalize_and_dim_reduc.html#apply-sctransform-normalization","title":"Apply sctransform normalization","text":"<ul> <li>Note that this single command replaces <code>NormalizeData()</code>, <code>ScaleData()</code>, and <code>FindVariableFeatures()</code>.</li> <li>Transformed data will be available in the SCT assay, which is set as the default after running sctransform</li> <li>During normalization, we can also remove confounding sources of variation, for example, mitochondrial mapping percentage</li> </ul> <pre><code># run sctransform\nsample_2 &lt;- SCTransform(sample_2, vars.to.regress = \"percent.mt\", verbose = FALSE)\n</code></pre> <p>The latest version of <code>sctransform</code> also supports using glmGamPoi package which substantially improves the speed of the learning procedure. It can be invoked by specifying <code>method=\"glmGamPoi\"</code>.</p> <pre><code>sample_2 &lt;- SCTransform(sample_2, method=\"glmGamPoi\", vars.to.regress = \"percent.mt\", verbose = FALSE)\n</code></pre> <p>Perform dimensionality reduction by PCA and UMAP embedding</p> <pre><code># These are now standard steps in the Seurat workflow for visualization and clustering\nsample_2 &lt;- RunPCA(sample_2, verbose = FALSE)\nsample_2 &lt;- RunUMAP(sample_2, dims = 1:30, verbose = FALSE)\nDimPlot(sample_2, label = TRUE) + NoLegend()\n</code></pre> <p>Finally, we shoud can save the object at this point so that it can easily be loaded back in without having to rerun the computationally intensive steps performed above, or easily shared with collaborators.</p> <pre><code>SaveH5Seurat(sample_2, filename = \"../../Data/notebooks_data/sample_2.filt.norm.h5Seurat\")\n</code></pre> Why can we choose more PCs when using sctransform? <p>In the standard Seurat workflow], we focused on 10 PCs for this dataset, though we highlight that the results are similar with higher settings for this parameter. Interestingly, we've found that when using sctransform, we often benefit by pushing this parameter even higher. We believe this is because the sctransform workflow performs more effective normalization, strongly removing technical effects from the data. </p> <p>Even after standard log-normalization, variation in sequencing depth is still a confounding factor (see Figure 1), and this effect can subtly influence higher PCs. In sctransform, this effect is substantially mitigated (see Figure 3). This means that higher PCs are more likely to represent subtle, but biologically relevant, sources of heterogeneity -- so including them may improve downstream analysis.</p> <p>In addition, sctransform returns 3,000 variable features by default, instead of 2,000. The rationale is similar, the additional variable features are less likely to be driven by technical differences across cells, and instead may represent more subtle biological fluctuations. In general, we find that results produced with sctransform are less dependent on these parameters (indeed, we achieve nearly identical results when using all genes in the transcriptome, though this does reduce computational efficiency). This can help users generate more robust results, and in addition, enables the application of standard analysis pipelines with identical parameter settings that can quickly be applied to new datasets:</p> <p>For example, the following code replicates the full end-to-end workflow, in a single command:</p> <pre><code>sample_2 &lt;- CreateSeuratObject(sample_2) %&gt;% SCTransform(vars.to.regress = 'percent.mt') %&gt;%\nRunPCA() %&gt;% RunUMAP(dims = 1:30)\n</code></pre> Where are normalized values stored for sctransform? <p>sctransform calculates a model of technical noise in scRNA-seq data using 'regularized negative binomial regression'. The residuals for this model are normalized values, and can be positive or negative. Positive residuals for a given gene in a given cell indicate that we observed more UMIs than expected given the gene\u2019s average expression in the population and cellular sequencing depth, while negative residuals indicate the converse. </p> <p>The results of sctransfrom are stored in the \"SCT\" assay: You can learn more about multi-assay data and commands in Seurat the developer guide.</p> <ul> <li><code>sample_2[[\"SCT\"]]@scale.data</code> contains the residuals (normalized values), and is used directly as input to PCA. Please note that this matrix is non-sparse, and can therefore take up a lot of memory if stored for all genes. To save memory, it stores these values only for variable genes, by setting the return.only.var.genes = TRUE by default in the <code>SCTransform()</code> function call.</li> <li>To assist with visualization and interpretation, it also converts Pearson residuals back to \u2018corrected\u2019 UMI counts. You can interpret these as the UMI counts we would expect to observe if all cells were sequenced to the same depth. If you want to see exactly how it is done, please look at the correct function here.</li> <li>The 'corrected' UMI counts are stored in <code>sample_2[[\"SCT\"]]@counts</code>. It stores log-normalized versions of these corrected counts in <code>sample_2[[\"SCT\"]]@data</code>, which are very helpful for visualization.</li> <li>You can use the corrected log-normalized counts for differential expression and integration.</li> </ul>"},{"location":"R/Part03_normalize_and_dim_reduc.html#wrapping-up","title":"Wrapping up","text":"<p>In this notebook we learned two ways of normalizing scRNAseq data using the <code>scale.data()</code> and the <code>SCTransform()</code> functions. We have also learned how to determine the dimensionality of the dataset and how to select a proper number of PCs. Finally, we have introduced the concept of UMAPs to visualize scRNAseq datasets.</p>"},{"location":"R/Part04_integration.html","title":"Datasets integration","text":""},{"location":"R/Part04_integration.html#integration-of-datasets","title":"Integration of datasets","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 40 minutes if using at least 2 cores (that is, at least 16 virtual-CPUs). Expect longer time with slower hardware, and a RAM usage of some 40-100GB.</p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Create an 'integrated' data assay for downstream analysis</li> <li>Identify cell types that are present in both datasets</li> <li>Assess integration success.</li> </ol> <p>The joint analysis of two or more single-cell datasets poses unique challenges. In particular, identifying cell populations that are present across multiple datasets can be problematic under standard workflows. Seurat v4 includes a set of methods to match (or \u2018align\u2019) shared cell populations across datasets. These methods first identify cross-dataset pairs of cells that are in a matched biological state (\u2018anchors\u2019), can be used both to correct for technical differences between datasets (i.e.\u00a0batch effect correction), and to perform comparative scRNA-seq analysis of across experimental conditions.</p> <p> !!! info \"Figure caption\"</p> <pre><code>(A) Representation of two datasets, reference and query, each of which originates from a separate single-cell experiment. The two datasets share cells from similar biological states, but the query dataset contains a unique population (in black).\n\n(B) We perform canonical correlation analysis, followed by L2 normalization of the canonical correlation vectors, to project the datasets into a subspace defined by shared correlation structure across datasets.\n\n(C) In the shared space, we identify pairs of MNNs across reference and query cells. These should represent cells in a shared biological state across datasets (gray lines) and serve as anchors to guide dataset integration. In principle, cells in unique populations should not participate in anchors, but in practice, we observe \"incorrect\" anchors at low frequency (red lines).\n\n(D) For each anchor pair, we assign a score based on the consistency of anchors across the neighborhood structure of each dataset.\n\n(E) We utilize anchors and their scores to compute \"correction\" vectors for each query cell, transforming its expression so it can be jointly analyzed as part of an integrated reference.\n</code></pre> <p>Below, we demonstrate methods for scRNA-seq integration as described in Stuart*, Butler* et al, 2019 to perform a comparative analysis of human immune cells (PBMC) in either a resting or interferon-stimulated state.</p>"},{"location":"R/Part04_integration.html#setup","title":"Setup","text":"<pre><code>library(tidyverse)\nlibrary(patchwork)\nlibrary(Seurat)\nlibrary(SeuratDisk)\nlibrary(sctransform)\nlibrary(kBET)\nLoadH5Seurat(\"../../Data/notebooks_data/sample_2.filt.norm.h5Seurat\")\nLoadH5Seurat(\"../../Data/notebooks_data/sample_3.filt.norm.h5Seurat\")\n</code></pre> <p>First of all, we need to make a list of datasets.</p> <pre><code>sample.list &lt;- list(sample_2, sample_3)\n</code></pre> <p>Warning</p> <p>It is extremely important that the datasets have been filtered and normalized separately before integration. Otherwise we will be introducing artifacts to the datasets that will be very undesirable.</p>"},{"location":"R/Part04_integration.html#default-integration","title":"Default integration","text":"<p>Seurat default integration workflow uses two algorithms to merge datasets: canonical correlation analysis and mutual nearest neighbours. The former, Canonical correlation analysis (CCA), is an algorithm that enables the identification of shared sources of variation between datasets, while the latter is able to identify biologically matched cells in a pair of datasets. Thus, CCA-based integration therefore enables integrative analysis when experimental conditions or disease states introduce very strong expression shifts, or when integrating datasets across modalities and species. We will see in this section how to perform CCA and integrate our datasets.</p>"},{"location":"R/Part04_integration.html#feature-selection","title":"Feature selection","text":"<p>Before integrating the datasets, we will select features that are repeatedly variable across datasets for integration. As discussed in the previous lesson, we typically use 3,000 or more features for analyses downstream of <code>SCTransform</code>.</p> <p>Then we will use the function<code>PrepSCTIntegration()</code>. This function takes in our list of datasets normalized with <code>SCTransform</code> and performs the following steps:</p> <ul> <li>Ensures that the sctransform residuals for the features specified to anchor.features are present in each object in the list. This is necessary because the default behavior of <code>SCTransform</code> is to only store the residuals for the features determined to be variable.</li> <li>Subsets the <code>scale.data</code> slot to only contain the residuals for <code>anchor.features</code> for efficiency in downstream processing.</li> </ul> <pre><code>features &lt;- SelectIntegrationFeatures(object.list = sample.list, nfeatures = 3000)\nsample.list &lt;- PrepSCTIntegration(object.list = sample.list, anchor.features = features)\n</code></pre>"},{"location":"R/Part04_integration.html#selecting-anchors","title":"Selecting anchors","text":"<p>Through the identification of cell pairwise correspondences between single cells across datasets, termed \"anchors\", Seurat can transform datasets into a shared space, even in the presence of extensive technical and/or biological differences. This enables the construction of harmonized atlases at the tissue or organism scale. To identify anchors, we use the <code>FindIntegrationAnchors()</code> function, which takes our list of Seurat objects as input, and then we use these anchors to integrate the two datasets together with <code>IntegrateData()</code>.</p> <pre><code>sample.anchors &lt;- FindIntegrationAnchors(object.list = sample.list, normalization.method = 'SCT', anchor.features = features)\nsample.combined.sct &lt;- IntegrateData(anchorset = sample.anchors, normalization.method = 'SCT')\n</code></pre>"},{"location":"R/Part04_integration.html#combined-dimensionality-reduction","title":"Combined dimensionality reduction","text":"<p>Now we can continue with our PCA, select an adequate number of PCs (remember that we use an increased number of PCs due to sctransform!).</p> <pre><code>sample.combined.sct &lt;- RunPCA(sample.combined.sct, verbose = FALSE)\nsample.combined.sct &lt;- RunUMAP(sample.combined.sct, reduction = \"pca\", dims = 1:30)\n</code></pre> <p>And finally we can visualize the integrated dataset.</p> <pre><code>DimPlot(sample.combined.sct, reduction = \"umap\")\n</code></pre> <p>We should save our integrated object for further analysis.</p> <pre><code>SaveH5Seurat(sample.combined.sct, filename = \"../../Data/notebooks_data/sample_123.filt.norm.red.h5Seurat\")\n</code></pre>"},{"location":"R/Part04_integration.html#reciprocal-pca","title":"Reciprocal PCA","text":"<p>CCA is well-suited for identifying anchors when cell types are conserved. However, CCA-based integration may also lead to overcorrection, especially when a large proportion of cells are non-overlapping across datasets, and it is very time-consuming for large datasets. On the other hand, RPCA-based integration runs significantly faster, and also represents a more conservative approach where cells in different biological states are less likely to \u2018align\u2019 after integration. Therefore, it is recommend to use RPCA during integrative analysis where:</p> <ul> <li>A substantial fraction of cells in one dataset have no matching type in the other</li> <li>Datasets originate from the same platform (i.e.\u00a0multiple lanes of 10x genomics)</li> <li>There are a large number of datasets or cells to integrate</li> </ul> <p>RPCA works by determining anchors between any two datasets and project each dataset into the others PCA space. Then it constrains the anchors by the same mutual neighborhood requirement as CCA. While the list of commands is nearly identical, this workflow requires users to run principal components analysis (PCA) -<code>RunPCA()</code> individually on each dataset prior to integration. Users should also set the <code>reduction</code> argument to \u2018rpca\u2019, when running <code>FindIntegrationAnchors()</code>.</p> <pre><code># Reload the dataset list\nsample.list &lt;- list(sample_2, sample_3)\n</code></pre> <pre><code>features &lt;- SelectIntegrationFeatures(object.list = sample.list, nfeatures = 3000)\nsample.list &lt;- PrepSCTIntegration(object.list = sample.list, anchor.features = features)\nsample.list &lt;- lapply(X = sample.list, FUN = RunPCA, features = features)\n</code></pre> <pre><code>sample.anchors &lt;- FindIntegrationAnchors(object.list = sample.list, normalization.method = \"SCT\",\nanchor.features = features, dims = 1:30, reduction = \"rpca\", k.anchor = 20)\nsample.combined.sct &lt;- IntegrateData(anchorset = sample.anchors, normalization.method = \"SCT\", dims = 1:30)\n</code></pre> <pre><code>sample.combined.sct &lt;- RunPCA(sample.combined.sct, verbose = FALSE)\nsample.combined.sct &lt;- RunUMAP(sample.combined.sct, reduction = \"pca\", dims = 1:30)\n</code></pre> <pre><code># Visualization\nDimPlot(sample.combined.sct, reduction = \"umap\")\n</code></pre> <p>In general, we observe strikingly similar results between the standard workflow and the one demonstrated here, with substantial reduction in compute time and memory. However, if the datasets are highly divergent (for example, cross-modality mapping or cross-species mapping), where only a small subset of features can be used to facilitate integration, and you may observe superior results using CCA.</p>"},{"location":"R/Part04_integration.html#assessing-integration-quality","title":"Assessing integration quality","text":"<p>How well went the integration? We can see if, for every point in the UMAP plot, the other most similar points are a mix from the various samples, or if they are dominated by only one of them. The <code>R</code> package <code>kbet</code> does this test for each datapoint. The test is rejected if a datapoint is in an area where cells from all samples are not mixed/overlapping. A perfect integration has around zero rejection (left side of the plot). In reality, this does not happen, but a good integration stays in general below a 50% rejection (right side of the plot).</p> <pre><code>batch.estimate &lt;- kBET(sample.combined.sct[[\"SCT\"]]@data, sample.combined.sct$batch, plot=FALSE)\nplot.data &lt;- data.frame(class=rep(c('observed', 'expected'), each=length(batch.estimate$stats$kBET.observed)), data =  c(batch.estimate$stats$kBET.observed,\nbatch.estimate$stats$kBET.expected))\n</code></pre> <pre><code>ggplot(plot.data, aes(class, data)) + geom_boxplot() + labs(x='Test', y='Rejection rate',title='kBET test results') +\ntheme_bw() +  scale_y_continuous(limits=c(0,1))\n</code></pre>"},{"location":"R/Part04_integration.html#wrapping-up","title":"Wrapping up","text":"<p>This notebook completes the integration of the datasets. We have both used the standard PCA and then a version of the PCA that can model transcript distribution and takes batches into account. Finally, we used a statistical test to check that the integration has been successful.</p>"},{"location":"R/Part05_clustering.html","title":"Cell clustering and Differential Expression (DE)","text":""},{"location":"R/Part05_clustering.html#cell-clustering-and-differential-expression-de","title":"Cell clustering and Differential Expression (DE)","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 45-60 minutes</p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Perform unsupervised clustering on the dataset</li> <li>Identify potential cell clusters by visualizing marker genes on the UMAP plot</li> <li>Understanding and applying differential gene expression analysis to verify cluster identities</li> <li>Performing an analysis of subclusters in the dataset</li> </ol> <p>Spermatogenesis goes through different stages, starting from SpermatogoniaA cells, going into clonal expansion while keeping cells connected through cytoplasmic bridges (SpermatogoniaB), and then continuing with the meiotic process (Spermatocites I and II). Finally, cells become Round spermatids, which then elongate to become Elongated spermatids and sperm.</p> <p></p> <p>Detecting those cell types is essential to answer biological questions such as: - which genes are most expressed for each cell type (beyond well known ones)? - in which proportion is every cell type present? - are there unknown cell types that I can identify?</p>"},{"location":"R/Part05_clustering.html#setup","title":"Setup","text":"<pre><code>library(tidyverse)\nlibrary(patchwork)\nlibrary(Seurat)\nlibrary(SeuratDisk)\nLoadH5Seurat(\"../../Data/notebooks_data/sample_123.filt.norm.red.h5Seurat\")\n</code></pre>"},{"location":"R/Part05_clustering.html#cell-clustering","title":"Cell clustering","text":"<p>Seurat applies a graph-based clustering approach, building upon initial strategies in (Macosko et al). Importantly, the distance metric which drives the clustering analysis (based on previously identified PCs) remains the same. However, Seurat\u2019s approach to partitioning the cellular distance matrix into clusters is heavily inspired by graph-based clustering approaches to scRNA-seq data [SNN-Cliq, Xu and Su, Bioinformatics, 2015] and CyTOF data [PhenoGraph, Levine et al., Cell, 2015]. Briefly, these methods embed cells in a graph structure - for example a K-nearest neighbor (KNN) graph, with edges drawn between cells with similar feature expression patterns, and then attempt to partition this graph into highly interconnected \u2018quasi-cliques\u2019 or \u2018communities\u2019.</p> <p>As in PhenoGraph, Seurat first constructs a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). This step is performed using the <code>FindNeighbors()</code> function, and takes as input the previously defined dimensionality of the dataset (first 10 PCs).</p> <p>To cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM [SLM, Blondel et al., Journal of Statistical Mechanics], to iteratively group cells together, with the goal of optimizing the standard modularity function. The <code>FindClusters()</code> function implements this procedure, and contains a resolution parameter that sets the \u2018granularity\u2019 of the downstream clustering, with increased values leading to a greater number of clusters.</p> <p>It is recommended to set this parameter between 0.4-1.2, as it typically returns good results for single-cell datasets of around 3K cells.</p> <pre><code>sample.combined.sct &lt;- FindNeighbors(sample.combined.sct, dims = 1:10)\nsample.combined.sct &lt;- FindClusters(sample.combined.sct, resolution = 0.5)\n</code></pre> <p>The clusters can be found using the <code>Idents()</code> function or in the metadata of the assay.</p> <pre><code>head(Idents(sample.combined.sct), 5)\nhead(sample.combined.sct@meta.data)\nhead(sample.combined.sct$seurat.clusters)\n</code></pre>"},{"location":"R/Part05_clustering.html#visualization","title":"Visualization","text":"<p>Finally we can visualize our clusters in the data using the <code>DimPlot()</code> function.</p> <pre><code>DimPlot(sample.combined.sct, reduction = 'umap')\n</code></pre> <p>Exercise</p> <p>Play with the <code>resolution</code> argument of FindClusters. How is the parameter related to the number of clusters identified? Note: The new clusters are saved in a new column of the metadata dataframe.</p> <pre><code>sample.combined.sct &lt;- FindClusters(sample.combined.sct, resolution = ??)\nDimPlot(sample.combined.sct, reduction = 'umap')\n</code></pre>"},{"location":"R/Part05_clustering.html#finding-differentially-expressed-markers","title":"Finding differentially expressed markers","text":"<p>Seurat can help you find markers that define clusters via differential expression using the <code>FindAllMarkers()</code> function. By default, it identifies positive and negative markers of a single cluster (specified in <code>ident.1</code>), compared to all other cells. <code>FindAllMarkers()</code> automates this process for all clusters, but you can also test groups of clusters vs.\u00a0each other, or against all cells using the <code>FindMarkers()</code> function. <code>FindMarkers()</code> function is specially useful to identify specific markers for subclusters of cells.</p> <pre><code># Find all markers\nall.markers &lt;- FindAllMarkers(pbmc)\n# A cluster vs another\ncluster0v1.markers &lt;- FindMarkers(pbmc, ident.1 = 0, ident.2 = c(1))\n# A cluster vs other clusters\ncluster0v1_2.markers &lt;- FindMarkers(pbmc, ident.1 = 0, ident.2 = c(1,2))\n</code></pre> <p>The results dataframe looks like this:</p> <pre><code>head(all.markers)\n</code></pre> <ul> <li>p_val : p_val of the test used for differential expression (unadjusted)</li> <li>avg_log2FC : log fold-change of the average expression between the two groups. Positive values indicate that the feature is more highly expressed in the first group.</li> <li>pct.1 : The percentage of cells where the feature is detected in the first group</li> <li>pct.2 : The percentage of cells where the feature is detected in the second group</li> <li>p_val_adj : Adjusted p-value, based on Bonferroni correction using all features in the dataset.</li> <li>gene : feature names will appear as row names using <code>FindMarkers()</code>, or as a column with <code>FindAllMarkers()</code></li> <li>cluster : using the <code>FindAllMarkers()</code> will return which cluster had each marker</li> </ul> <p>The <code>min.pct</code> argument requires a feature to be detected at a minimum percentage in either of the two groups of cells, and the <code>logfc.threshold</code> argument requires a feature to be differentially expressed (on average) by some amount between the two groups. You can set both of these to 0, but with a dramatic increase in time - since this will test a large number of features that are unlikely to be highly discriminatory. As another option to speed up these computations, <code>max.cells.per.ident</code> can be set. This will downsample each identity class to have no more cells than whatever this is set to. While there is generally going to be a loss in power, the speed increases can be significant and the most highly differentially expressed features will likely still rise to the top.</p> <pre><code># find markers for every cluster compared to all remaining cells, report only the positive ones with a LFC above 0.25 and at least 25% of cells express that feature.\nall.markers &lt;- FindAllMarkers(pbmc3k, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)\nall.markers %&gt;% group_by(cluster) %&gt;% slice_max(n = 2, order_by = avg_log2FC) # Show top 2 markers for each cluster\n</code></pre> <p>You can easily save your marker dataframes using <code>write.table()</code></p> <pre><code>write.table(x = all.markers, file = \"../../Data/results/all_markers.tsv\", col.names = TRUE, row.names = FALSE, sep = \"\\t\", quote = FALSE)\n</code></pre>"},{"location":"R/Part05_clustering.html#alternate-testing-for-differential-expression","title":"Alternate testing for differential expression","text":"<p>As a default, Seurat performs differential expression based on the non-parametric Wilcoxon rank sum test. Nontheless, the following differential expression tests are currently supported:</p> <ul> <li>\"wilcox\" : Wilcoxon rank sum test (default)</li> <li>\"bimod\" : Likelihood-ratio test for single cell feature expression, (McDavid et al., Bioinformatics, 2013)</li> <li>\"roc\" : Standard AUC classifier</li> <li>\"t\" : Student\u2019s t-test</li> <li>\"poisson\" : Likelihood ratio test assuming an underlying negative binomial distribution. Use only for UMI-based datasets</li> <li>\"negbinom\" : Likelihood ratio test assuming an underlying negative binomial distribution. Use only for UMI-based datasets</li> <li>\"LR\" : Uses a logistic regression framework to determine differentially expressed genes. Constructs a logistic regression model predicting group membership based on each feature individually and compares this to a null model with a likelihood ratio test.</li> <li>\"MAST\" : GLM-framework that treates cellular detection rate as a covariate (Finak et al, Genome Biology, 2015) (Installation instructions)</li> <li>\"DESeq2\" : DE based on a model using the negative binomial distribution (Love et al, Genome Biology, 2014) (Installation instructions)</li> </ul> <p>For MAST and DESeq2, please ensure that these packages are installed separately in order to use them as part of Seurat. Once installed, use the <code>test.use</code> parameter can be used to specify which DE test to use. For example, the ROC test returns the \u2018classification power\u2019 for any individual marker (ranging from 0 - random, to 1 - perfect).</p> <pre><code>cluster0.markers &lt;- FindMarkers(pbmc, ident.1 = 0, logfc.threshold = 0.25, test.use = \"roc\", only.pos = TRUE)\n</code></pre>"},{"location":"R/Part05_clustering.html#visualization-of-marker-genes","title":"Visualization of marker genes","text":"<p>Seurat includes several tools for visualizing marker expression. <code>VlnPlot()</code> (shows expression probability distributions across clusters), and <code>FeaturePlot()</code> (visualizes feature expression on a UMAP or PCA plot) are some of the most commonly used visualizations. It is also suggested that you exploring <code>RidgePlot()</code>, and <code>DotPlot()</code> as additional methods to view your dataset.</p> <pre><code># You can visualize several markers at the same time using the features argument\nVlnPlot(pbmc3k, features = c(\"ID4\", \"MKI67\",\"CD14\"))\nFeaturePlot(pbmc3k, features = c(\"ID4\", \"MKI67\",\"CD14\"))\nRidgePlot(pbmc3k, features = c(\"ID4\", \"MKI67\",\"CD14\"))\nDotPlot(object = pbmc3k, features = c(\"ID4\", \"MKI67\",\"CD14\"))\n</code></pre> <p><code>DoHeatmap()</code> generates an expression heatmap for given cells and features. In this case, we are plotting the top 20 markers (or all markers if less than 20) for each cluster.</p> <pre><code>all.markers %&gt;%\ngroup_by(cluster) %&gt;%\ntop_n(n = 10, wt = avg_log2FC) -&gt; top10\nDoHeatmap(sample.combined.sct, features = top10$gene) + NoLegend()\n</code></pre>"},{"location":"R/Part05_clustering.html#cluster-annotation-through-known-markers","title":"Cluster annotation through known markers","text":"<p>We try to identify clusters of cells by looking at the expression of relevant marker genes. This requires a previous biological knowledge of those cell types, such that we can input the markers. Below, we define a list of markers, where for each cell type we define a list of markers. Then we will plot every list of markers on the UMAP plot:</p> <pre><code>markers &lt;- list() #make an empty dictionary\n### SPERMATOCYTOGENESIS\nmarkers['SpermatogoniaA'] = c('ID4')\nmarkers['SpermatogoniaB'] = c('MKI67','DMRT1','STRA8')\nmarkers['SpermatocytesI'] = c('MEIOB','SYCP1','TEX101')\nmarkers['SpermatocytesII'] = c('PIWIL1','SPATA16','CLGN')\n### SPERMIOGENESIS\nmarkers['Round.Spt'] = c('SPATA9','SPAM1') #Round spermatids\nmarkers['Elong.Spt'] = c('PRM1','PRM2','PRM3','AKAP4') #Elongated spermatids\n### SOMATIC CELLS\nmarkers['Sertoli'] = c('VIM','CTSL')\nmarkers['Macroph'] = c('CD14')\nmarkers['Leydig'] = c('CFD')\nmarkers['Endothelial'] = c('CD34')\nmarkers['Myoid'] = c('ACTA2')\n# remove markers missing in the dataset\nfor (i in names(markers)){\nmarkers[i] = intersect(markers[i], rownames(sample.combined.sct))\n}\n</code></pre> <pre><code>FeaturePlot(pbmc3k, features = markers['SpermatogoniaA'])\n</code></pre> <pre><code>FeaturePlot(pbmc3k, features = markers['SpermatogoniaB'])\n</code></pre> <pre><code>FeaturePlot(pbmc3k, features = markers['SpermatocytesI'])\n</code></pre> <pre><code>FeaturePlot(pbmc3k, features = markers['SpermatocytesII'])\n</code></pre> <pre><code>FeaturePlot(pbmc3k, features = markers['Round.Spt'])\n</code></pre> <pre><code>FeaturePlot(pbmc3k, features = markers['Elong.Spt'])\n</code></pre> <p>Sertoli are often not possible to identify. They are big in size, meaning they are often not isolated successfully. Many of their markers are in common with other somatic cells. Also, their function as nurse cells for germ cells of the testis means that their marker genes are also expressed. We can see that CTSL is expressed in some germ cells, but not in other clusters, while VIM is expressed in a likely somatic cluster (but it is common to other somatic cell types).</p> <pre><code>FeaturePlot(pbmc3k, features = markers['Sertoli'])\n</code></pre> <p>Macrophage cells seem to be absent.</p> <pre><code>FeaturePlot(pbmc3k, features = markers['Macroph'])\n</code></pre> <p>Leydig cells are likely to be missing as well.</p> <pre><code>FeaturePlot(pbmc3k, features = markers['Leydig'])\n</code></pre> <p>There is a little endothelial cluster.</p> <pre><code>FeaturePlot(pbmc3k, features = markers['Endothelial'])\n</code></pre> <p>And also a myoid cluster.</p> <pre><code>FeaturePlot(pbmc3k, features = markers['Myoid'])\n</code></pre>"},{"location":"R/Part05_clustering.html#calculating-cell-scores","title":"Calculating cell scores","text":"<p>Using the <code>AddModuleScore()</code> function, you can use the previous list of different cell markers to give a \"score\" to each cell, instead of exploring every gene from your list of markers. This is extremely useful when you want to assign cell types if you have biological knowledge (markers). Nonetheless, you will have to make sure the list of markers will be specific to those clusters, otherwise there is a chance you cannot properly identify clusters using this list.</p> <pre><code>for (i in names(markers)){\npbmc3k &lt;- AddModuleScore(pbmc3k, features = markers[i], name = i)\n}\n</code></pre> <p>We can visualize the scores either in a <code>FeaturePlot()</code> or a <code>VlnPlot()</code>. For example, let\u2019s take a look at the SpermatogoniaB cell score:</p> <pre><code>FeaturePlot(pbmc3k, reduction = \"umap\", features = \"SpermatogoniaB1\")\nVlnPlot(pbmc3k, features = \"SpermatogoniaB1\", group.by = \"seurat_clusters\")\n</code></pre> <p>It looks pretty good! However, lets check the SpermatogoniaA</p> <pre><code>FeaturePlot(pbmc3k, reduction = \"umap\", features = \"SpermatogoniaA1\")\nVlnPlot(pbmc3k, features = \"SpermatogoniaA1\", group.by = \"seurat_clusters\")\n</code></pre> <p>As we can see, the set of features used to calculate module scores may not be specific enough to accurately depict a cell type. Some clusters seem to have just enough expression of those genes to have positive scores, and single cell dropouts makes it very difficult to score a cell based only on a single gene. However, it works very well when selecting marker genes from the <code>FindMarkers()</code> function.</p>"},{"location":"R/Part05_clustering.html#assigning-names-to-clusters","title":"Assigning names to clusters","text":"<p>Now that we know what kind of cell types our clusters are, we can name them properly. Write the names in the list <code>new_names</code>. You should be able to give a name for each cell type. Below is an example, but the names are not in the right position. If there is more than one cluster with same cell type, just write the name followed by a dot <code>.</code> and a number. For example, by writing for example <code>.1</code> and <code>.2</code> at the end of the names. We will remove the numbers afterwords.</p> <pre><code>new_names = c(\n'0'='SpermatocitesII',\n'1'='SpermatogoniaA',\n'2'='RoundSpermatids',\n'3'='RoundSpermatids',\n'4'='RoundSpermatids',\n'5'='ElongSpermatids',\n'6'='SpermatocitesI',\n'7'='RoundSpermatids',\n'8'='ElongSpermatids',\n'9'='SpermatogoniaB',\n'10'='Somatic',\n'11'='SpermatocitesII'\n)\n</code></pre> <p>Now we apply the new names and save them in a new meta.data variable called \"cluster_names\":</p> <pre><code>sample.combined.sct$cluster_names &lt;- recode(Idents(sample.combined.sct), !!!new_names)\nIdents(sample.combined.sct) &lt;- \"cluster_names\"\n</code></pre> <p>Finally, we visualize the new cluster names!</p> <pre><code>DimPlot(sample.combined.sct, reduction = \"umap\", label = TRUE)\nDoHeatmap(sample.combined.sct, features = unlist(markers))\n</code></pre>"},{"location":"R/Part05_clustering.html#selecting-and-subclustering","title":"Selecting and subclustering","text":"<p>If we suspect that a cluster might be composed of an heterogeneous population of cells, we can subset that group of cells and find subclusters of cells. This is the case for the cell types found in late spermatogonia and spermatocites.</p>"},{"location":"R/Part05_clustering.html#subsetting-the-data","title":"Subsetting the data","text":"<p>We can subset our Seurat object using the <code>subset()</code> function and selecting our clusters of interest from the <code>idents</code> argument. Note that you have to have selected our \"cluster_names\" as your <code>Ident()</code>!</p> <pre><code># Clusters of interest and their markers\nclusters &lt;- c('SpermatogoniaB','SpermatocitesI','SpermatocitesII')\nsample_subset &lt;- subset(x = sample.combined.sct, idents = clusters)\nmarkers['Leptotene'] = c('SYCE2','SCML1')\nmarkers['Zygotene'] = c('LY6K', 'SYCP1')\nmarkers['Pachytene'] = c('PIWIL1','CCDC112')\nmarkers['Diplotene'] = c('OVOL2','CCNA1', 'CDK1','AURKA')\n</code></pre> <p>Let\u2019s look at the marker plots of these specific clusters:</p> <pre><code>FeaturePlot(sample_subset, features = markers['Leptotene'] )\n</code></pre> <pre><code>FeaturePlot(sample_subset, features = markers['Zygotene'] )\n</code></pre> <pre><code>FeaturePlot(sample_subset, features = markers['Pachytene'] )\n</code></pre> <pre><code>FeaturePlot(sample_subset, features = markers['Diplotene'] )\n</code></pre>"},{"location":"R/Part05_clustering.html#recalculate-clusters","title":"Recalculate clusters","text":"<p>Now we can recalculate our neighbours and the clusters. What should be the <code>resolution</code> argument?</p> <pre><code>sample_subset &lt;- FindNeighbors(sample_subset)\nsample_subset &lt;- FindClusters(sample_subset, resolution = 0.4)\n</code></pre> <p>And we can visualize our subclustering using <code>DimPlot()</code>:</p> <pre><code>DimPlot(sample_subset, reduction = \"umap\")\n</code></pre> <p>It would be more convenient to rename our clusters. We should be able to do it using the same code snippet as before!</p> <pre><code>new_names = c(\n'0'='SpermatogoniaB',\n'1'='Zygotene',\n'2'='Diplotene',\n'3'='Diplotene',\n'4'='Diplotene',\n'5'='Leptotene',\n'6'='Pachytene',\n'7'='Pachytene',\n'8'='Diplotene')\nsample_subset$cluster_names &lt;- recode(Idents(sample_subset), !!!new_names)\n</code></pre>"},{"location":"R/Part05_clustering.html#differential-expression","title":"Differential expression","text":"<p>We can find specific differences between the subclusters found in here:</p> <pre><code>subset_markers &lt;- FindAllMarkers(sample_subset, logfc.threshold = 0.25, only.pos = TRUE)\nsubset_markers %&gt;%\ngroup_by(cluster) %&gt;%\ntop_n(n = 10, wt = avg_log2FC) -&gt; subset_top10\nhead(subset_top10)\n</code></pre> <p>We should be able to find at least some of the markers used in the plots. However, it can be that those do not appear because there are many other coexpressed genes with high expression values.</p> <pre><code>DoHeatmap(sample_subset, features = subset_top10$gene) + NoLegend()\n</code></pre> <p>We can find some of the marker genes. For example <code>SCML1</code> for leptotene cells, <code>SYCP1</code> and <code>LY6K</code> for zygotene, <code>CCDC112</code> and <code>PIWIL1</code> for pachitene, <code>CCNA1</code> and <code>AURKA</code> for zygotene.</p> <pre><code>DoHeatmap(sample_subset, features = unlist(markers[c(\"Leptotene\",\"Zygotene\",\"Pachytene\",\"Diplotene\")])) + NoLegend()\n</code></pre>"},{"location":"R/Part05_clustering.html#reintegrating-the-subclusters","title":"Reintegrating the subclusters","text":"<p>Since we have created a subset of the original data, how can we integrate the new subclusters back into the dataset? We will need to do some data wrangling</p> <p>First we create a new metadata variable in the original Seurat object.</p> <pre><code>sample.combined.sct$subclusters &lt;- sample.combined.sct$cluster_names\n</code></pre> <p>The new subclusters are found in the subset metadata. We only need to get the cell names and change the original dataset accordingly.</p> <pre><code>subset_cells &lt;- rownames(sample_subset@meta.data)\nsample.combined.sct$subclusters&lt;- sample_subset$seurat_clusters\n</code></pre> <p>We can visualize the before and after very easily now:</p> <pre><code>p1 &lt;- DimPlot(sample.combined.sct, reduction = \"umap\", group.by = \"cluster_names\", label = T)\np2 &lt;- DimPlot(sample.combined.sct, reduction = \"umap\", group.by = \"subclusters\", label = T)\np1 + p2\n</code></pre>"},{"location":"R/Part05_clustering.html#cluster-correlations","title":"Cluster correlations","text":"<p>An useful method we can use to check the quality of our clusters is to compute cluster correlations. Unfortunately, Seurat does not provide a straight forward method, but it is fairly simple to calculate. First we make use of the <code>AverageExpression()</code> function, which, as the name implies, will compute the average expression of the groups of cells (clusters or other types of groups).</p> <pre><code>av.exp &lt;- AverageExpression(sample.combined.sct, assay = \"SCT\", slot = \"data\")\nhead(av.exp)\n</code></pre> <p>Then we calculate the correlations between the columns (clusters) and plot it using the <code>pheatmap()</code> package.</p> <pre><code>pheatmap(cor(av.exp), treeheight_col = 0, breaks = seq(-1,1,length.out = 100), display_numbers = T)\n</code></pre>"},{"location":"R/Part05_clustering.html#save-our-clustered-data","title":"Save our clustered data","text":"<pre><code>SaveH5Seurat(sample.combined.sct, filename = \"../../Data/notebooks_data/sample_123.filt.norm.red.clust.h5Seurat\")\n</code></pre>"},{"location":"R/Part05_clustering.html#wrapping-up","title":"Wrapping up","text":"<p>We have been showing how to identify potential cell clusters. In this dataset, cells change from one type to another in a continuous process, so such a hard clustering may not completely reflect biological reality. However, it is a good approximation, as it is illustrated by the differentially expressed genes that we checked in each cluster.</p> <p>We performed differential expression and what are the useful values that we get from it (p-value of the test, magnitude of the gene expression compared to all other clusters). Finally, we subsetted the data and subject it to a more fine grained cell identification.</p>"},{"location":"R/Part06_pseudotime.html","title":"Pseudotimes and cell fates","text":""},{"location":"R/Part06_pseudotime.html#pseudotimes-and-cell-fates","title":"Pseudotimes and cell fates","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 45 minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Understand and determine the pseudotimes on a single cell dataset</li> <li>Infer cell fates and distinguish between differentiation stages or actual final developmental stages</li> <li>Compare gene expressions along differentiation</li> <li>Cluster genes with similar gene expression</li> </ol> <p>While clustering is an useful type of analysis to try giving a structure to the development of cells towards their final stage (spermatozoa), it does not give an understanding of how the development \"stretches\" from start to end. For example, a cluster can have many cells and look \"big\" on UMAP, but actually its variability in terms of gene expressions could be low. Also, a developmental process can branches towards different ends (cell fates) or developmental checkpoints (e.g.\u00a0stages where damaged cells express specific genes for apoptosis/cell death). Pseudotime and cell fates analysis can be used to hgihlight exactly those processes.</p> <ul> <li>Pseudotimes assigns to each cell the value of a timeline, starting from 0 for the cells at the beginning of the development. This value is purely a reference for ordering the cells development, but pseudotimes at specific stages can be assigned to real times, using previous biological knowledge.</li> <li>Cell fates analysis looks at the PCA projection of the data and the pseudotime of each data point on the PCA. From this, it tries to create a tree connecting the cells, so that the end branches of the tree are different end points or stages of the developmental process.</li> </ul> <p></p> <p>Figure: cell fates tree on a 3D pca plot. Circles represent the middle point of each cluster. From Perredaeau et al.\u00a0(2017)</p>"},{"location":"R/Part06_pseudotime.html#setup","title":"Setup","text":"<pre><code>library(Seurat)\nlibrary(SeuratDisk)\nlibrary(tidyverse)\nlibrary(slingshot)\nLoadH5Seurat(\"../../Data/notebooks_data/sample_123.filt.norm.red.clust.h5Seurat\")\n</code></pre>"},{"location":"R/Part06_pseudotime.html#pseudotime","title":"Pseudotime","text":"<p>We will use the package Slignshot. Slingshot was designed to model developmental trajectories in single-cell RNA sequencing data and serve as a component in an analysis pipeline after dimensionality reduction and clustering.</p> <pre><code>sample.combined.SCE &lt;- as.SingleCellExperiment(sample.combined.sct)\nsde &lt;- slingshot(sample.combined.SCE, reducedDim = 'PCA')\n</code></pre> <p>The using the <code>slingshot</code> package it is possible to plot our components and the calculated trajectory. Unfortunately, it does not work with ggplot.</p> <pre><code>cols &lt;- colorRampPalette(viridis::viridis(3))\ncols &lt;- cols(50)[cut(sde$slingPseudotime_1, breaks=50)]\nplot(reducedDim(sample.combined.SCE, \"PCA\"), pch=16, col = cols) # sample.combined.SCE contains, PC1 and PC2, and colored by pseudotime\nlines(slingshot::SlingshotDataSet(sde), lwd = 2) # here we plot the trajectory\n</code></pre> <p>It is possible to create a <code>FeaturePlot()</code> with both pseudotime and the trajectory by extracting the information from our <code>slingshot</code> object and add it to our <code>Seurat</code> object, but it is a bit inconvenient. First, pseudotime can be extracted with the function <code>slingPseudotime()</code>. The trajectory curves are stored inside the slingshot object <code>sde</code>; in this case, it is easier since there is only one trajectory.</p> <pre><code>sample.combined.sct$pseudotime &lt;- slingPseudotime(sde)\ntrajectory &lt;- data.frame(SlingshotDataSet(sde)@curves[[\"Lineage1\"]][[\"s\"]])\nhead(trajectory)\n</code></pre> <p>Now we can use Seurat functions to plot the pseudotime and the calculated trajectory!</p> <pre><code>FeaturePlot(sample.combined.sct, features = \"pseudotime\", reduction = \"pca\") + scale_color_viridis_c() + geom_path(data = trajectory, aes(x = PC_1, y = PC_2))\n</code></pre> <pre><code>DimPlot(sample.combined.sct, group.by = \"Cell_type\", reduction = \"pca\")\n</code></pre> <p>We can create a plot that sorts our cells by pseudotime and check that it corresponds to their cell type:</p> <pre><code>sample.combined.SCE$pseudotime &lt;- as.numeric(slingPseudotime(sde))\nggplot(as.data.frame(colData(sample.combined.SCE)), aes(x = pseudotime,\ny = Cell_type,\ncolour = Cell_type)) +\nggbeeswarm::geom_quasirandom(groupOnX = FALSE) +\ntheme_classic() +\nxlab(\"Slingshot pseudotime\") + ylab(\"Timepoint\") +\nggtitle(\"Cells ordered by Slingshot pseudotime\")\n</code></pre>"},{"location":"R/Part06_pseudotime.html#wrapping-up","title":"Wrapping up","text":"<p>This notebook shows how to do pseudotimes analysis and exploring cell fates and gene expressions. We have seen how to distinguish between an actual differentiation branch and a differentiation stage. Basically, all cells before (i.e.\u00a0earlier in pseudotime) a differentiation stage will be associated to such stage with high probability, because they must go through that developmental stage. Finding a developmental stage around meiosis in spermatogenic samples is a common result across single cell datasets of many species, such as humans, primates and mice. Using the <code>palantir</code> software, we can look at differences between gene expressions for different fates, and cluster together genes of interest for further analysis.</p>"},{"location":"R/rmd/modify_md.html","title":"Modify md","text":"In\u00a0[1]: Copied! <pre>import os\nimport shutil\nimport re\n</pre> import os import shutil import re In\u00a0[2]: Copied! <pre>source_folder = \"./knit/\"\nimages_folder = \"./img/\"\ndestination_folder = \"../\"\nfiles = os.listdir(source_folder)\n\nprint(files)\n</pre> source_folder = \"./knit/\" images_folder = \"./img/\" destination_folder = \"../\" files = os.listdir(source_folder)  print(files) <pre>['Part06_pseudotime.md', 'Part03_normalize_and_dim_reduc.md', 'Part01_read_the_data.md', 'Part04_integration.md', 'Part05_clustering.md', 'Part02_filtering_sample3.md', 'Part02_filtering_sample2.md']\n</pre> In\u00a0[3]: Copied! <pre>for i in files:\n    if i.endswith(\".md\"):\n        with open(source_folder + i) as f:\n            print(\"Processing:\", i, \"\\n\")\n            text = f.read()\n            text = re.sub(\"\\nknit((.|\\n)*)\\n[0-9]{4}-[0-9]{2}-[0-9]{2}\\n\", \"\\n---\\n\" ,text)\n            text = re.sub(\"(\u201d|\u201c)\",\"\\\"\",text)\n        with open(source_folder + i, \"w\") as f:\n            f.write(text)\n</pre> for i in files:     if i.endswith(\".md\"):         with open(source_folder + i) as f:             print(\"Processing:\", i, \"\\n\")             text = f.read()             text = re.sub(\"\\nknit((.|\\n)*)\\n[0-9]{4}-[0-9]{2}-[0-9]{2}\\n\", \"\\n---\\n\" ,text)             text = re.sub(\"(\u201d|\u201c)\",\"\\\"\",text)         with open(source_folder + i, \"w\") as f:             f.write(text) <pre>Processing: Part06_pseudotime.md \n\nProcessing: Part03_normalize_and_dim_reduc.md \n\nProcessing: Part01_read_the_data.md \n\nProcessing: Part04_integration.md \n\nProcessing: Part05_clustering.md \n\nProcessing: Part02_filtering_sample3.md \n\nProcessing: Part02_filtering_sample2.md \n\n</pre> In\u00a0[4]: Copied! <pre>for file_name in os.listdir(source_folder):\n    # construct full file path\n    source = source_folder + file_name\n    destination = destination_folder + file_name\n    shutil.move(source, destination)\n    print('Moved:', file_name)\n</pre> for file_name in os.listdir(source_folder):     # construct full file path     source = source_folder + file_name     destination = destination_folder + file_name     shutil.move(source, destination)     print('Moved:', file_name) <pre>Moved: Part06_pseudotime.md\nMoved: Part03_normalize_and_dim_reduc.md\nMoved: Part01_read_the_data.md\nMoved: Part04_integration.md\nMoved: Part05_clustering.md\nMoved: Part02_filtering_sample3.md\nMoved: Part02_filtering_sample2.md\n</pre> In\u00a0[5]: Copied! <pre>def copy_directory(src_dir, dest_dir):\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    for item in os.listdir(src_dir):\n        src_path = os.path.join(src_dir, item)\n        dest_path = os.path.join(dest_dir, item)\n        if os.path.isdir(src_path):\n            copy_directory(src_path, dest_path)\n        else:\n            shutil.copy2(src_path, dest_path)\n\ncopy_directory(images_folder, destination_folder+\"img/\")\n</pre>  def copy_directory(src_dir, dest_dir):     if not os.path.exists(dest_dir):         os.makedirs(dest_dir)     for item in os.listdir(src_dir):         src_path = os.path.join(src_dir, item)         dest_path = os.path.join(dest_dir, item)         if os.path.isdir(src_path):             copy_directory(src_path, dest_path)         else:             shutil.copy2(src_path, dest_path)  copy_directory(images_folder, destination_folder+\"img/\")"},{"location":"R/rmd/old/scRNAseq_Seurat.html","title":"scRNAseq analysis for Deng et al. (2014)","text":""},{"location":"R/rmd/old/scRNAseq_Seurat.html#introduction","title":"Introduction","text":"<p>Section Overview</p> <p>\ud83d\udd70 Time Estimation: 100 minutes  </p> <p>\ud83d\udcac Learning Objectives: </p> <ol> <li>Load scRNAseq data into R using the Seurat package</li> <li>QC of scRNAseq data</li> <li>Normalization of scRNAseq data</li> <li>Dimensionality reduction and visualization</li> <li>Cell clustering</li> <li>Differential Expression Analysis</li> <li>Functional Analysis with the gprofiler2 package</li> <li>Pseudotime with the slingshot package</li> </ol> <p>scRNAseq data analysis of Deng et al.\u00a0(2014). This study follows the early embryonic development of mouse cells from zygote to late blastocists (and some adult cells).</p> <p></p>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#load-libraries","title":"Load libraries","text":"<pre><code>library(rhdf5)\nlibrary(tidyverse)\nlibrary(Seurat)\nlibrary(SeuratWrappers)\nlibrary(patchwork)\nlibrary(biomaRt)\nlibrary(slingshot)\nlibrary(gprofiler2)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#load-dataset","title":"Load dataset","text":"<p>Data is a matrix of cells in columns and genes in rows. We specify that the row names (our genes) are in the first column.</p> <pre><code>raw_data &lt;- read.delim(\"../../Data/example_data/GSE45719_cts.txt\", sep = \"\\t\", row.names = 1)\nhead(raw_data, n = 5)\n</code></pre> <p>Here is the metadata. The column names of <code>raw_data</code> are the same as the row names of metadata.</p> <pre><code>metadata &lt;- read.delim(\"../../Data/example_data/GSE45719_metadata.tsv\", header = T, sep = \"\\t\", quote = \"\", stringsAsFactors = F, row.names = 1)\nhead(metadata, n = 5)\n</code></pre> <p>For the purposes of this workshop, the metadata of this project has been simplified. The most interesting variable is Cell_type, which shows the developmental stage of our cells. Cell type is not ordered by its differentiation stage, but we can do that now:</p> <pre><code>metadata$Cell_type &lt;- factor(metadata$Cell_type, levels = c(\"MII Oocyte\",\"Zygote\",\n\"Early 2-cell\",\"Mid 2-cell\",\"Late 2-cell\",\n\"4-cell\",\"8-cell\",\"16-cell\",\n\"Early blastocyst\",\"Mid blastocyst\",\"Late blastocyst\", \"Fibroblast\",\"Adult\"))\n</code></pre> <p>There are some cell types that we are not really interested in, and will be removed in the next step below.</p>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#seurat-object","title":"Seurat object","text":"<p>Now that we have our data loaded, we can create a Seurat Object, from which we will perform our analysis. The function <code>CreateSeuratObject()</code> takes as arguments our count matrix, metadata and several options for filtering the data:</p> <ul> <li><code>min.cells</code> will keep features (genes) that are detected in at least this many cells.</li> <li><code>min.features</code> will keep cells with at least this many features detected.</li> </ul> <p>This will prevent from keeping cells and genes with an immense majority of 0\u2019s as values. In addition, we are removing some cell types we do not want to analyse.</p> <pre><code>raw_ann &lt;- CreateSeuratObject(counts = raw_data, meta.data = metadata, min.cells = 3, min.features = 200,)\nraw_ann &lt;- subset(raw_ann, cells = colnames(raw_ann)[!raw_ann$Cell_type %in% c(\"MII Oocyte\",\"Fibroblast\",\"Adult\")])\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#quality-control","title":"Quality Control","text":"<p>The Seurat object initialization step above only considered cells that expressed at least 300 genes and genes detected in at least 3 cells. Here is how many cells and genes we start with:</p> <pre><code>print(paste0(\"Before filtering: \", dim(raw_ann)[2], \" cells \",  dim(raw_ann)[1], \" genes\"))\n</code></pre> <p>Additionally, we would like to exclude cells that are damaged. A common metric to judge this (although by no means the only one) is the relative expression of mitochondrially derived genes. When the cells apoptose due to stress, their mitochondria becomes leaky and there is widespread RNA degradation. Thus a relative enrichment of mitochondrially derived genes can be a tell-tale sign of cell stress. Here, we compute the proportion of transcripts that are of mitochondrial origin for every cell (percent.mito), and visualize its distribution as a violin plot. We also use the <code>GenePlot()</code> function to observe how percent.mito correlates with other metrics.</p> <pre><code>raw_ann[['percent.mito']] &lt;- PercentageFeatureSet(raw_ann, pattern = \"^mt-\")\n</code></pre> <p>Now, sometimes, mitochondrial genes are a bit tricky to find, specially if your genes are not gene names, but gene IDs. You might have already a collection of gene names you want to use, but it is not always the case. Therefore, it might very useful to have some extra annotation on our genes which will help to select mitochondrial genes (or ERCC genes and ribosomal genes). Since this dataset was aligned using the mouse genome version mm9, we will use the mm9 annotation from Biomart. The package biomaRt will do the job!</p> <pre><code>ensembl67=useMart(host='http://feb2014.archive.ensembl.org/', # select latest version of mm9 genome annotation\nbiomart='ENSEMBL_MART_ENSEMBL', dataset = \"mmusculus_gene_ensembl\")\n# we get a list of annotations we would like to fetch\nmm9.gene.annotations &lt;- biomaRt::getBM(mart = ensembl67, attributes=c(\"ensembl_gene_id\", \"external_gene_id\", \"description\", \"chromosome_name\"))\nhead(mm9.gene.annotations)\n</code></pre> <p>We select that mitochondrial genes are those with the chromosome_name annotation equal to \"MT\".</p> <pre><code>mt_genes &lt;- mm9.gene.annotations %&gt;% filter(chromosome_name == \"MT\") %&gt;% pull(external_gene_id)\nmt_genes &lt;- mt_genes %in% rownames(raw_ann)\nraw_ann[['percent.mito']] &lt;- PercentageFeatureSet(raw_ann, features = mt_genes)\n</code></pre> <p>If your data contains spike-ins (ERCC genes), you can also compute the proportion of transcripts belonging to them. Furthermore, it might be useful to calculate the percentage of reads aligned to ribosomal genes, since it has been shown that they can skew the data due to their high variability.</p> <pre><code>raw_ann[['percent.ercc']] &lt;- PercentageFeatureSet(raw_ann, pattern = \"^ERCC-\")\nraw_ann[['percent.ribo']] &lt;- PercentageFeatureSet(raw_ann, pattern = \"^Rp[ls]\")\n</code></pre> <p>Nonetheless, in this experiment there aren\u2019t ERCC genes or mitochondrial genes:</p> <pre><code>sum(raw_ann$percent.ercc)\nsum(raw_ann$percent.mito)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#visualizations","title":"Visualizations","text":"<p>It is extremely useful to visualize QC measurements calculated so far. Violin plots (fancy boxplots) are a great way to check the distribution of values of all our QC measurements. When initializing the Seurat Object, Seurat calculates also the number of genes detected and the total library size per cell.</p> <pre><code>VlnPlot(raw_ann, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.ribo\"),\nncol = 4, group.by = \"Cell_type\")\n</code></pre> <p>It seems that the late 2-cells have a very high total number of reads. We should not filter them out, there are very few! In addition, our percentage of ribosomal counts is also quite low (maximum is ~5%).</p> <p>We can check the relationship between library size and number of genes detected:</p> <pre><code>FeatureScatter(raw_ann, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\", group.by = \"Cell_type\")\n</code></pre> <p>In other experiment with more cells, we might consider removing cells with an unexpectedly high number of detected genes and reads. These cells might be doublets, that is, two cells that were sequenced together!</p>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#filtering","title":"Filtering","text":"<p>Some automatic filtering can be made using quantiles! Using the extreme upper and lower quantiles (0.99 and 0.01) we can make sure that outliers are removed. In this case, we will remove cells with very low library size and genes detected (lower than quantile 0.01).</p> <pre><code>feature_min &lt;- quantile(raw_ann$nFeature_RNA, probs = 0.01)\ncount_min &lt;- quantile(raw_ann$nCount_RNA, probs = 0.01)\n</code></pre> <p>We can subset our dataset using the function <code>subset()</code>.</p> <pre><code>adata &lt;- subset(raw_ann, subset = nFeature_RNA &gt; feature_min  &amp; nCount_RNA &gt; count_min)\nrm(raw_ann) # we remove the initial unfiltered dataset to reduce computational resources, this is not necessary!\n</code></pre> <p>Finally, this is how many cells and genes we have after filtering:</p> <pre><code>print(paste0(\"After filtering: \", dim(adata)[2], \" cells \",  dim(adata)[1], \" genes\"))\n</code></pre> <p>And this is how our filtered data looks like:</p> <pre><code>VlnPlot(adata, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.ribo\"),\nncol = 4, group.by = \"Cell_type\")\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#exploratory-analysis","title":"Exploratory analysis","text":""},{"location":"R/rmd/old/scRNAseq_Seurat.html#normalization","title":"Normalization","text":"<p>Now that the data is filtered, we can proceed to normalize our count matrix. Seurat normalizes the gene expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. There have been many methods to normalize the data, but this is the simplest and the most intuitive. The division by total expression is done to change all expression counts to a relative measure, since experience has suggested that technical factors (e.g.\u00a0capture rate, efficiency of reverse transcription) are largely responsible for the variation in the number of molecules per cell, although genuine biological factors (e.g.\u00a0cell cycle stage, cell size) also play a smaller, but non-negligible role. The log-transformation is a commonly used transformation that has many desirable properties, such as variance stabilization (can you think of others?).</p> <pre><code>adata &lt;- NormalizeData(adata)\nadata &lt;- FindVariableFeatures(adata, selection.method = \"vst\", nfeatures = 2000)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#identification-of-variable-genes","title":"Identification of Variable Genes","text":"<p>Identify most variable genes and label top 5 most highly variable.</p> <pre><code>top10 &lt;- head(VariableFeatures(adata), 5)\nLabelPoints(plot = VariableFeaturePlot(adata), points = top10, repel = T)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#scale","title":"Scale","text":"<p>Gene expression scaling is necessary for proper clustering of our cells. Since genes may be expressed in very different orders of magnitude, extreme expression levels may drive the separation between cells and bias the results.</p> <pre><code>adata &lt;- ScaleData(adata)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#pca","title":"PCA","text":"<p>We can perform a Principal Component analysis using the following function. PCA can be used for visualization of our cells, as well as clustering and other dimensionality reduction methods such as t-SNE or UMAP.</p> <pre><code>adata &lt;- RunPCA(adata)\n</code></pre> <p>We can also calculate and visualize the importance of each gene for each Principal Component.</p> <pre><code>pcalod_1 &lt;- VizDimLoadings(object = adata, dims = 1) + theme(axis.text.y = element_text(size = 8)) pcalod_2 &lt;- VizDimLoadings(object = adata, dims = 2) + theme(axis.text.y = element_text(size = 8))\n</code></pre> <pre><code>CombinePlots(plots = list(pcalod_1, pcalod_2), ncol = 2)\n</code></pre> <p>Visualization only show negative loadings cause there are many more than positives. We can see a balanced plot using balance = TRUE:</p> <pre><code>pcalod_1 &lt;- VizDimLoadings(object = adata, dims = 1, balanced = TRUE) + theme(axis.text.y = element_text(size = 8)) pcalod_2 &lt;- VizDimLoadings(object = adata, dims = 2, balanced = TRUE) + theme(axis.text.y = element_text(size = 8))\n</code></pre> <pre><code>CombinePlots(plots = list(pcalod_1, pcalod_2), ncol = 2)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#cell-clustering","title":"Cell clustering","text":"<p>Now that we have calculated our components, we can proceed to select the number of PCs necessary to perform clustering. There are two methods that we can use to determine the proper number of dimensions:</p>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#pc-selection","title":"PC selection","text":"<p>Jackstraw method</p> <p>The Jackstraw method randomly permutes a subset of data, and calculates projected PCA scores for these \u2018random\u2019 genes. Then compares the PCA scores for the \u2018random\u2019 genes with the observed PCA scores to determine statistical significance. End result is a p-value for each gene\u2019s association with each principal component.</p> <pre><code>adata &lt;- JackStraw(adata, num.replicate = 100)\nadata &lt;- ScoreJackStraw(adata, dims = 1:20)\nJackStrawPlot(adata, dims = 1:20)\n</code></pre> <p>As you can see in the plot above, there is a change in the orders of magnitude of the PCs\u2019 p-values. Suggesting that we may cut off around PC8.</p> <p>Elbow method</p> <p>The elbow method allows us to explore the explained variation of each of the Principal Components. The plot usually looks like an \"elbow\", where adding more PCs does not really contribute to the amount of explained variation. We can see again that we reach a plateau around PC6 or PC8.</p> <pre><code>ElbowPlot(adata)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#clustering","title":"Clustering","text":"<p>We could include 6 PCs, but it does not hurt to use more; there are very few cells in this experiment and we would like to include as much information as possible. Use the <code>resolution</code> argument of the <code>FindClusters()</code> function to fine-tune the number of clusters to find. The larger the number the less clusters it will find.</p> <pre><code>adata &lt;- FindNeighbors(adata, dims = 1:20)\nadata &lt;- FindClusters(adata, resolution = 0.8)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#visualization","title":"Visualization","text":"<pre><code>adata &lt;- RunTSNE(adata)\nadata &lt;- RunUMAP(adata, dims = 1:20)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#pca_1","title":"PCA","text":"<pre><code>DimPlot(adata, reduction = \"pca\", group.by = c('Cell_type')) + DimPlot(adata, reduction = \"pca\", group.by = c('seurat_clusters'))\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#umap","title":"UMAP","text":"<pre><code>p1 &lt;- DimPlot(adata, reduction = \"umap\", group.by = 'Cell_type')\np2 &lt;- DimPlot(adata, reduction = \"umap\")\np1 + p2\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#tsne","title":"TSNE","text":"<pre><code>p3 &lt;- DimPlot(adata, reduction = \"tsne\", group.by = 'Cell_type')\np4 &lt;- DimPlot(adata, reduction = \"tsne\")\np3 + p4\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#differential-expression-analysis","title":"Differential Expression Analysis","text":"<p>Differential testing works a bit different than in bulk RNA-Seq analysis. Usually, your experiment will include dozens or hundreds of cells per cluster/condition. We can make use of Wilcox tests and multiple testing correction to identify statistically significant genes using the <code>FindAllMarkers()</code> function. This function will gather all \"markers\" for each of your Ident variable values (your conditions or identified clusters).</p> <pre><code>markers &lt;- FindAllMarkers(adata,logfc.threshold = 0.5, only.pos = T)\n</code></pre> <pre><code>markers %&gt;%\ngroup_by(cluster) %&gt;%\ntop_n(n = 10, wt = avg_log2FC) -&gt; top10\nhead(top10)\n</code></pre> <p>On the other hand, if you want to make a specific comparison, you can use the <code>FindMarkers()</code> function:</p> <pre><code>cluster2.markers &lt;- FindMarkers(adata, ident.1 = 2, min.pct = 0.5, only.pos = T) # Only cluster 2 markers\ncluster2.markers %&gt;% arrange(desc(avg_log2FC)) %&gt;% head(10)\n</code></pre> <p>Even compare specific clusters against others by selecting <code>ident.1</code> and <code>ident.2</code>:</p> <pre><code>cluster5.markers &lt;- FindMarkers(adata, ident.1 = 5, ident.2 = c(0, 3), # Differences between cluster 5 and clusters 0 and 3\nmin.pct = 0.5, only.pos = T) </code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#visualizations_1","title":"Visualizations","text":"<p>We can visualize all markers in a heatmap just like this! To go easy on the visualization, we only use the top 10 markers.</p> <pre><code>DoHeatmap(adata, features = top10$gene) + NoLegend()\n</code></pre> <p>We can also plot the expression on our PCA, tSNE or UMAP. We only need to pass a vector of genes to the <code>features</code> argument of <code>FeaturePlot()</code>.</p> <pre><code>FeaturePlot(adata, features = top10$gene[1:4], reduction = \"pca\")\n</code></pre> <p>Or as a violin plot, a ridge plot or a dot plot.</p> <pre><code>VlnPlot(adata, features = top10$gene[1]) + RidgePlot(adata, features = top10$gene[1])\n</code></pre> <pre><code>DotPlot(adata, features = top10$gene[1:10])\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#functional-analysis-with-gprofiler2","title":"Functional analysis with gprofiler2","text":"<p><code>gost()</code> function allows us to do functional profiling of gene lists, such as our differentially expressed genes. The function performs statistical enrichment analysis to find over-representation of terms from Gene Ontology, biological pathways like KEGG and Reactome, human disease annotations, etc. This is done by using hypergeometric tests that are corrected for multiple testing.</p>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#single-query","title":"Single query","text":"<p>A standard input of the <code>gost()</code> function is a (named) list of gene identifiers. The list can consist of mixed types of identifiers (proteins, transcripts, microarray IDs, etc), SNP IDs, chromosomal intervals or functional term IDs.</p> <p>The result is a named list where result is a data.frame with the enrichment analysis results and meta containing a named list with all the metadata for the query.</p> <pre><code>gostres &lt;- gost(query = rownames(cluster2.markers), organism = \"mmusculus\", ordered_query = FALSE, multi_query = FALSE, significant = FALSE, exclude_iea = FALSE, measure_underrepresentation = FALSE, evcodes = FALSE, user_threshold = 0.05, correction_method = \"g_SCS\", domain_scope = \"annotated\", custom_bg = NULL, numeric_ns = \"\", sources = NULL, as_short_link = FALSE)\nhead(gostres$result)\n</code></pre> <p>The result data.frame contains the following columns:</p> <pre><code>names(gostres$meta)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#multiple-queries","title":"Multiple queries","text":"<p>The function <code>gost()</code> also allows to perform enrichment on multiple input gene lists. Multiple queries are automatically detected if the input query is a list of vectors with gene identifiers and the results are combined into identical data.frame as in case of single query.</p> <pre><code>multi_gostres1 &lt;- gost(query = list(\"chromX\" = c(\"X:1000:1000000\", \"rs17396340\", \"GO:0005005\", \"ENSG00000156103\", \"NLRP1\"),\n\"chromY\" = c(\"Y:1:10000000\", \"rs17396340\", \"GO:0005005\", \"ENSG00000156103\", \"NLRP1\")), multi_query = FALSE)\nhead(multi_gostres1$result, 3)\n</code></pre> <p>The column \"query\" in the result dataframe will now contain the corresponding name for the query. If no name is specified, then the query name is defined as the order of query with the prefix \"query_.\" Another option for multiple gene lists is setting the parameter <code>multiquery = TRUE</code>. Then the results from all of the input queries are grouped according to term IDs for better comparison.</p> <pre><code>multi_gostres2 &lt;- gost(query = list(\"chromX\" = c(\"X:1000:1000000\", \"rs17396340\",\n\"GO:0005005\", \"ENSG00000156103\", \"NLRP1\"),\n\"chromY\" = c(\"Y:1:10000000\", \"rs17396340\", \"GO:0005005\", \"ENSG00000156103\", \"NLRP1\")), multi_query = TRUE)\nhead(multi_gostres2$result, 3)\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#visualization_1","title":"Visualization","text":"<p>The enrichment results are visualized with a Manhattan-like-plot using the function <code>gostplot()</code> and the previously found gost results gostres:</p> <pre><code>#gostplot(gostres, capped = TRUE, interactive = TRUE)\n</code></pre> <p>The function <code>publish_gostplot()</code> takes the static plot object as an input and enables to highlight a selection of interesting terms from the results with numbers and table of results. These can be set with parameter highlight_terms listing the term IDs in a vector or as a data.frame with column \"term_id\" such as a subset of the result dataframe.</p> <p>First we create the static plot:</p> <pre><code>p &lt;- gostplot(gostres, capped = FALSE, interactive = FALSE)\np\n</code></pre> <p>Then we make it in high quality. We can add highlighted terms if we want with the highlight_terms argument.</p> <pre><code>pp &lt;- publish_gostplot(p, highlight_terms = c(\"CORUM:3047\"),\nwidth = NA, height = NA, filename = NULL )\n</code></pre> <p>The gost results can also be visualized with a table. The <code>publish_gosttable()</code> function will create a nice-looking table with the result statistics for the highlight_terms from the result data.frame. The highlight_terms can be a vector of term IDs or a subset of the results.</p> <pre><code>publish_gosttable(gostres, highlight_terms = gostres$result[c(1:10),],\nuse_colors = TRUE, show_columns = c(\"source\", \"term_name\", \"term_size\", \"intersection_size\"),\nfilename = NULL) </code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#pseudotime","title":"Pseudotime","text":"<p>We will use the package Slignshot. Slingshot was designed to model developmental trajectories in single-cell RNA sequencing data and serve as a component in an analysis pipeline after dimensionality reduction and clustering.</p> <pre><code>adata_SCE &lt;- as.SingleCellExperiment(adata)\nsde &lt;- slingshot(adata_SCE, reducedDim = 'PCA')\n</code></pre> <p>The using the <code>slingshot</code> package it is possible to plot our components and the calculated trajectory. Unfortunately, it does not work with ggplot.</p> <pre><code>cols &lt;- colorRampPalette(viridis::viridis(3))\ncols &lt;- cols(50)[cut(sde$slingPseudotime_1, breaks=50)]\nplot(reducedDim(adata_SCE, \"PCA\"), pch=16, col = cols) # adata_SCE contains, PC1 and PC2, and colored by pseudotime\nlines(slingshot::SlingshotDataSet(sde), lwd = 2) # here we plot the trajectory\n</code></pre> <p>It is possible to create a <code>FeaturePlot()</code> with both pseudotime and the trajectory by extracting the information from our <code>slingshot</code> object and add it to our <code>Seurat</code> object, but it is a bit inconvenient. First, pseudotime can be extracted with the function <code>slingPseudotime()</code>. The trajectory curves are stored inside the slingshot object <code>sde</code>; in this case, it is easier since there is only one trajectory.</p> <pre><code>adata$pseudotime &lt;- slingPseudotime(sde)\ntrajectory &lt;- data.frame(SlingshotDataSet(sde)@curves[[\"Lineage1\"]][[\"s\"]])\nhead(trajectory)\n</code></pre> <p>Now we can use Seurat functions to plot the pseudotime and the calculated trajectory!</p> <pre><code>FeaturePlot(adata, features = \"pseudotime\", reduction = \"pca\") + scale_color_viridis_c() + geom_path(data = trajectory, aes(x = PC_1, y = PC_2))\n</code></pre> <pre><code>DimPlot(adata, group.by = \"Cell_type\", reduction = \"pca\")\n</code></pre> <p>We can create a plot that sorts our cells by pseudotime and check that it corresponds to their cell type:</p> <pre><code>adata_SCE$pseudotime &lt;- as.numeric(slingPseudotime(sde))\nggplot(as.data.frame(colData(adata_SCE)), aes(x = pseudotime,\ny = Cell_type,\ncolour = Cell_type)) +\nggbeeswarm::geom_quasirandom(groupOnX = FALSE) +\ntheme_classic() +\nxlab(\"Slingshot pseudotime\") + ylab(\"Timepoint\") +\nggtitle(\"Cells ordered by Slingshot pseudotime\")\n</code></pre>"},{"location":"R/rmd/old/scRNAseq_Seurat.html#session-info","title":"Session info","text":"<p>Finally, we create a <code>session_info()</code> table that will allow anyone to check what versions of R and packages are we using for reproducibility purposes.</p> <pre><code>devtools::session_info()\n</code></pre>"},{"location":"python/index.html","title":"Index","text":""},{"location":"python/index.html#notebooks","title":"Notebooks","text":"<p>Folder with interactive notebook files e.g. for python (jupyter notebooks) or R (Rstudio notebooks).</p>"},{"location":"python/Part01_read_the_data.html","title":"Read the data","text":"In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn <p>Commands from scanpy are under different categories: preprocessing (pp), tools (tl), plotting (pl). Each category contains some functions to work on single cell data. Scanpy has also a category called <code>external</code>, where a few external packages have been integrated to work with scanpy. Use the <code>help()</code> command to see what a command does in <code>python</code></p> In\u00a0[2]: Copied! <pre>help(sc.preprocessing.calculate_qc_metrics)\n</pre> help(sc.preprocessing.calculate_qc_metrics) <pre>Help on function calculate_qc_metrics in module scanpy.preprocessing._qc:\n\ncalculate_qc_metrics(adata: anndata._core.anndata.AnnData, *, expr_type: str = 'counts', var_type: str = 'genes', qc_vars: Collection[str] = (), percent_top: Union[Collection[int], NoneType] = (50, 100, 200, 500), layer: Union[str, NoneType] = None, use_raw: bool = False, inplace: bool = False, log1p: bool = True, parallel: Union[bool, NoneType] = None) -&gt; Union[Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame], NoneType]\n    Calculate quality control metrics.\n    \n    Calculates a number of qc metrics for an AnnData object, see section\n    `Returns` for specifics. Largely based on `calculateQCMetrics` from scater\n    [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix.\n    \n    Note that this method can take a while to compile on the first call. That\n    result is then cached to disk to be used later.\n    \n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    expr_type\n        Name of kind of values in X.\n    var_type\n        The kind of thing the variables are.\n    qc_vars\n        Keys for boolean columns of `.var` which identify variables you could\n        want to control for (e.g. \"ERCC\" or \"mito\").\n    percent_top\n        Which proportions of top genes to cover. If empty or `None` don't\n        calculate. Values are considered 1-indexed, `percent_top=[50]` finds\n        cumulative proportion to the 50th most expressed gene.\n    layer\n        If provided, use `adata.layers[layer]` for expression values instead\n        of `adata.X`.\n    use_raw\n        If True, use `adata.raw.X` for expression values instead of `adata.X`.\n    inplace\n        Whether to place calculated metrics in `adata`'s `.obs` and `.var`.\n    log1p\n        Set to `False` to skip computing `log1p` transformed annotations.\n    \n    Returns\n    -------\n    Depending on `inplace` returns calculated metrics\n    (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`.\n    \n    Observation level metrics include:\n    \n    `total_{var_type}_by_{expr_type}`\n        E.g. \"total_genes_by_counts\". Number of genes with positive counts in a cell.\n    `total_{expr_type}`\n        E.g. \"total_counts\". Total number of counts for a cell.\n    `pct_{expr_type}_in_top_{n}_{var_type}` \u2013 for `n` in `percent_top`\n        E.g. \"pct_counts_in_top_50_genes\". Cumulative percentage of counts\n        for 50 most expressed genes in a cell.\n    `total_{expr_type}_{qc_var}` \u2013 for `qc_var` in `qc_vars`\n        E.g. \"total_counts_mito\". Total number of counts for variabes in\n        `qc_vars`.\n    `pct_{expr_type}_{qc_var}` \u2013 for `qc_var` in `qc_vars`\n        E.g. \"pct_counts_mito\". Proportion of total counts for a cell which\n        are mitochondrial.\n    \n    Variable level metrics include:\n    \n    `total_{expr_type}`\n        E.g. \"total_counts\". Sum of counts for a gene.\n    `n_genes_by_{expr_type}`\n        E.g. \"n_genes_by_counts\". The number of genes with at least 1 count in a cell. Calculated for all cells.\n    `mean_{expr_type}`\n        E.g. \"mean_counts\". Mean expression over all cells.\n    `n_cells_by_{expr_type}`\n        E.g. \"n_cells_by_counts\". Number of cells this expression is\n        measured in.\n    `pct_dropout_by_{expr_type}`\n        E.g. \"pct_dropout_by_counts\". Percentage of cells this feature does\n        not appear in.\n    \n    Example\n    -------\n    Calculate qc metrics for visualization.\n    \n    .. plot::\n        :context: close-figs\n    \n        import scanpy as sc\n        import seaborn as sns\n    \n        pbmc = sc.datasets.pbmc3k()\n        pbmc.var[\"mito\"] = pbmc.var_names.str.startswith(\"MT-\")\n        sc.pp.calculate_qc_metrics(pbmc, qc_vars=[\"mito\"], inplace=True)\n        sns.jointplot(\n            data=pbmc.obs,\n            x=\"log1p_total_counts\",\n            y=\"log1p_n_genes_by_counts\",\n            kind=\"hex\",\n        )\n    \n    .. plot::\n        :context: close-figs\n    \n        sns.histplot(pbmc.obs[\"pct_counts_mito\"])\n\n</pre> In\u00a0[3]: Copied! <pre>sample_2 = sc.read_10x_mtx('../../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/cellranger_sample2/outs/filtered_feature_bc_matrix/', cache=True)\n</pre> sample_2 = sc.read_10x_mtx('../../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/cellranger_sample2/outs/filtered_feature_bc_matrix/', cache=True) In\u00a0[4]: Copied! <pre>sample_3 = sc.read_10x_mtx('../../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/cellranger_sample3/outs/filtered_feature_bc_matrix/', cache=True)\n</pre> sample_3 = sc.read_10x_mtx('../../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/cellranger_sample3/outs/filtered_feature_bc_matrix/', cache=True) <p>The datasets <code>sample_2</code> and <code>sample_3</code> are now created. They are so-called <code>Annotated datasets</code>. Each annotated dataset contains:</p> <ul> <li>The data matrix <code>X</code> of size $N\\_cells \\times N\\_genes$</li> <li>Vectors of cells-related quantities in the table <code>.obs</code>(for example, how many transcripts there are in each cell)</li> <li>Vectors of genes-related quantities in the table <code>.var</code> (for example, in how many cells the each gene is detected)</li> <li>Matrices of size $N\\_cells \\times N\\_genes$ in <code>.layers</code> (for example, normalized data matrix, imputed data matrix, ....)</li> </ul> <p>We will often call the cells for observations (obs) and the genes for variables (var) when it is practical in relation to the annotated dataset</p> <p>During the analysis we will encounter other components of the annotated datasets. They will be explained when it is necessary, so you might want to skip this explanation if you want.</p> <ul> <li>Matrices where each line is cell-related in <code>.obsm</code> (for example, the PCA coordinates of each cell)</li> <li>Matrices where each line is gene-related in <code>.varm</code> (for example, mean of the gene in each cell type)</li> <li>Anything else useful is in <code>.uns</code></li> </ul> <p></p> <p>Above: a representation of the data matrix, variable and observations in an annotated dataset.</p> <p>Each component of the annotated dataset is called by using a <code>dot</code>. For example, we can see the data matrix by</p> In\u00a0[5]: Copied! <pre>sample_2.X\n</pre> sample_2.X Out[5]: <pre>&lt;8583x36601 sparse matrix of type '&lt;class 'numpy.float32'&gt;'\n\twith 22578947 stored elements in Compressed Sparse Row format&gt;</pre> <p>The matrix is in compressed format. We can reassign it as a dense matrix, so that we can see what it contains.</p> In\u00a0[6]: Copied! <pre>sample_2.X = np.array( sample_2.X.todense() )\n</pre> sample_2.X = np.array( sample_2.X.todense() ) In\u00a0[7]: Copied! <pre>sample_2.X\n</pre> sample_2.X Out[7]: <pre>array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)</pre> In\u00a0[8]: Copied! <pre>sample_3.X = np.array( sample_3.X.todense() )\n</pre> sample_3.X = np.array( sample_3.X.todense() ) In\u00a0[9]: Copied! <pre>sample_3.X\n</pre> sample_3.X Out[9]: <pre>array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)</pre> <p>When the matrix is no longer compressed, we can calculate some statistics for both cells and genes with the following <code>scanpy</code> command. Note that all scanpy commands follow a similar format. The two commands used below are the same, but in the second we used the short form for the <code>preprocessing</code> category.</p> In\u00a0[10]: Copied! <pre>sc.preprocessing.calculate_qc_metrics(sample_2, inplace=True)\nsc.pp.calculate_qc_metrics(sample_3, inplace=True)\n</pre> sc.preprocessing.calculate_qc_metrics(sample_2, inplace=True) sc.pp.calculate_qc_metrics(sample_3, inplace=True) <p>We can see that <code>obs</code> and <code>var</code> now contains a lot of different values whose names, that are mostly self-explicative. For example</p> <ul> <li><code>n_genes_by_counts</code> is the number of detected genes in each cell</li> <li><code>total_counts</code> is the number of transcripts in each cell</li> <li><code>mean_counts</code> is the average of counts of each gene across all cells</li> </ul> In\u00a0[11]: Copied! <pre>sample_2\n</pre> sample_2 Out[11]: <pre>AnnData object with n_obs \u00d7 n_vars = 8583 \u00d7 36601\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n    var: 'gene_ids', 'feature_types', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'</pre> <p>You can access directly all observations/variables or some of them specifically. Each observation line is named with the cell barcode, while variables have gene names in each line</p> In\u00a0[12]: Copied! <pre>sample_2.obs\n</pre> sample_2.obs Out[12]: n_genes_by_counts log1p_n_genes_by_counts total_counts log1p_total_counts pct_counts_in_top_50_genes pct_counts_in_top_100_genes pct_counts_in_top_200_genes pct_counts_in_top_500_genes AAACCTGAGATCCTGT-1 473 6.161207 694.0 6.543912 32.853026 46.253602 60.662824 100.000000 AAACCTGAGCCTATGT-1 2016 7.609367 6868.0 8.834774 47.408270 53.567268 61.560862 74.635993 AAACCTGAGCTTTGGT-1 3670 8.208219 9188.0 9.125762 17.599042 25.555072 35.154549 51.273400 AAACCTGAGGATGGTC-1 2175 7.685244 4021.0 8.299535 13.603581 21.139020 32.131311 51.927381 AAACCTGAGTACGACG-1 1622 7.392032 3237.0 8.082711 20.883534 30.552981 42.570281 63.608279 ... ... ... ... ... ... ... ... ... TTTGTCATCACGACTA-1 3234 8.081784 6918.0 8.842027 13.356461 20.381613 29.719572 46.863255 TTTGTCATCAGTTTGG-1 1312 7.180070 2895.0 7.971086 32.918826 44.006908 55.613126 71.951641 TTTGTCATCCAAACTG-1 1009 6.917706 1461.0 7.287560 20.123203 29.295003 42.984257 65.160849 TTTGTCATCCGTCAAA-1 1399 7.244228 2712.0 7.905810 28.945428 40.007375 50.884956 66.851032 TTTGTCATCGTTACAG-1 3294 8.100161 7757.0 8.956480 25.125693 33.518113 42.671136 56.323321 <p>8583 rows \u00d7 8 columns</p> In\u00a0[13]: Copied! <pre>sample_2.obs[ ['total_counts','n_genes_by_counts'] ]\n</pre> sample_2.obs[ ['total_counts','n_genes_by_counts'] ] Out[13]: total_counts n_genes_by_counts AAACCTGAGATCCTGT-1 694.0 473 AAACCTGAGCCTATGT-1 6868.0 2016 AAACCTGAGCTTTGGT-1 9188.0 3670 AAACCTGAGGATGGTC-1 4021.0 2175 AAACCTGAGTACGACG-1 3237.0 1622 ... ... ... TTTGTCATCACGACTA-1 6918.0 3234 TTTGTCATCAGTTTGG-1 2895.0 1312 TTTGTCATCCAAACTG-1 1461.0 1009 TTTGTCATCCGTCAAA-1 2712.0 1399 TTTGTCATCGTTACAG-1 7757.0 3294 <p>8583 rows \u00d7 2 columns</p> In\u00a0[14]: Copied! <pre>sample_2.var\n</pre> sample_2.var Out[14]: gene_ids feature_types n_cells_by_counts mean_counts log1p_mean_counts pct_dropout_by_counts total_counts log1p_total_counts MIR1302-2HG ENSG00000243485 Gene Expression 13 0.001515 0.001513 99.848538 13.0 2.639057 FAM138A ENSG00000237613 Gene Expression 0 0.000000 0.000000 100.000000 0.0 0.000000 OR4F5 ENSG00000186092 Gene Expression 0 0.000000 0.000000 100.000000 0.0 0.000000 AL627309.1 ENSG00000238009 Gene Expression 40 0.004777 0.004766 99.533962 41.0 3.737670 AL627309.3 ENSG00000239945 Gene Expression 0 0.000000 0.000000 100.000000 0.0 0.000000 ... ... ... ... ... ... ... ... ... AC141272.1 ENSG00000277836 Gene Expression 0 0.000000 0.000000 100.000000 0.0 0.000000 AC023491.2 ENSG00000278633 Gene Expression 587 0.249680 0.222887 93.160899 2143.0 7.670429 AC007325.1 ENSG00000276017 Gene Expression 212 0.040196 0.039409 97.530001 345.0 5.846439 AC007325.4 ENSG00000278817 Gene Expression 1949 0.436910 0.362495 77.292322 3750.0 8.229777 AC007325.2 ENSG00000277196 Gene Expression 2 0.000233 0.000233 99.976698 2.0 1.098612 <p>36601 rows \u00d7 8 columns</p> <p>We store the matrix <code>X</code> to save the raw values. We will be able to see it in <code>layers</code>, independently of how we transform the matrix <code>X</code></p> In\u00a0[15]: Copied! <pre>sample_2.layers[ 'umi_raw' ] = sample_2.X.copy()\n</pre> sample_2.layers[ 'umi_raw' ] = sample_2.X.copy() In\u00a0[16]: Copied! <pre>sample_3.layers[ 'umi_raw' ] = sample_3.X.copy()\n</pre> sample_3.layers[ 'umi_raw' ] = sample_3.X.copy() <p>We can see that the matrix is stored in <code>.layers['umi_raw']</code>, and we can reassign it to <code>.X</code> or use it if needed in some future analysis</p> In\u00a0[17]: Copied! <pre>sample_2\n</pre> sample_2 Out[17]: <pre>AnnData object with n_obs \u00d7 n_vars = 8583 \u00d7 36601\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n    var: 'gene_ids', 'feature_types', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    layers: 'umi_raw'</pre> In\u00a0[18]: Copied! <pre>sample_2.layers['umi_raw']\n</pre> sample_2.layers['umi_raw'] Out[18]: <pre>array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)</pre> <p>You can always subset a dataset by using a selection of cells and genes, and assign it as a new dataset (or to itself if you want to filter out some cells or genes)</p> <p>An annotated dataset can be subsetted by cells, for example using a quality measure as the number of transcripts per cell</p> In\u00a0[19]: Copied! <pre>sample_2_qc = sample_2[ sample_2.obs['total_counts']&lt;10000, : ].copy()\n</pre> sample_2_qc = sample_2[ sample_2.obs['total_counts']&lt;10000, : ].copy() In\u00a0[20]: Copied! <pre>sample_2_qc\n</pre> sample_2_qc Out[20]: <pre>AnnData object with n_obs \u00d7 n_vars = 5452 \u00d7 36601\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n    var: 'gene_ids', 'feature_types', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    layers: 'umi_raw'</pre> <p>In a similar way, you can use values calculated on the genes to subset the data by genes, for example in how many cells each gene is detected</p> In\u00a0[21]: Copied! <pre>sample_2_qc = sample_2[ :, sample_2.var['n_cells_by_counts']&gt;3 ].copy()\n</pre> sample_2_qc = sample_2[ :, sample_2.var['n_cells_by_counts']&gt;3 ].copy() In\u00a0[22]: Copied! <pre>sample_2_qc\n</pre> sample_2_qc Out[22]: <pre>AnnData object with n_obs \u00d7 n_vars = 8583 \u00d7 28220\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n    var: 'gene_ids', 'feature_types', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    layers: 'umi_raw'</pre> <p>Note how <code>sample_2_qc</code> has first a reduced number of cells and then a reduced number of genes.</p> <p>Remember that you cannot subset at the same time by cells and genes, for example</p> <pre><code>sample_2[ sample_2.obs['total_counts']&lt;10000, sample_2.var['mean_counts']&gt;1 ]\n</code></pre> <p>but those two steps have to be done separately as shown before.</p> <p>The annotated datasets can be easily saved by using <code>write</code>. The format to be used in the file name is <code>h5ad</code>.</p> In\u00a0[23]: Copied! <pre>!mkdir -p ../../Data/notebooks_data\n</pre> !mkdir -p ../../Data/notebooks_data In\u00a0[24]: Copied! <pre>sample_2.write('../../Data/notebooks_data/sample_2.h5ad')\n</pre> sample_2.write('../../Data/notebooks_data/sample_2.h5ad') <pre>... storing 'feature_types' as categorical\n</pre> In\u00a0[25]: Copied! <pre>sample_3.write('../../Data/notebooks_data/sample_3.h5ad')\n</pre> sample_3.write('../../Data/notebooks_data/sample_3.h5ad') <pre>... storing 'feature_types' as categorical\n</pre>"},{"location":"python/Part01_read_the_data.html#read-the-data","title":"Read the data\u00b6","text":""},{"location":"python/Part01_read_the_data.html#learning-objectives","title":"Learning objectives:\u00b6","text":"<ul> <li>Get an overview of the <code>scanpy</code> package and the <code>python</code> language syntax</li> <li>Learn and explore the data structure containing a single cell dataset</li> <li>Understand and apply basic interactions with the transcript matrix and the components of a dataset</li> </ul> <p>Execution time: 30-60 minutes</p>"},{"location":"python/Part01_read_the_data.html#import-the-packages","title":"Import the packages\u00b6","text":"<p>We will use <code>scanpy</code> as the main analysis tool for the analysis, where we will also apply some other packages. Scanpy has a comprehensive manual webpage that includes many different tutorial you can use for further practicing. Packages are imported with the command <code>import</code>, and their name is shortened with the command <code>as</code>, so that we can write shorter names in our code</p> <p>An alternative and well-established tool for <code>R</code> users is Seurat. This is used in the <code>R</code> version of this course.</p>"},{"location":"python/Part01_read_the_data.html#loading-and-understanding-the-dataset-structure","title":"Loading and understanding the dataset structure\u00b6","text":"<p>Data can be loaded from many different possible formats. Each format has a dedicated reading command, for example <code>read_h5ad</code>, <code>read_10X_mtx</code>, <code>read_txt</code>. We are going to use <code>read_10X_mtx</code> to load the output of the 10X software that produces the aligned data.</p> <p>Note the option <code>cache=True</code>. If you are going to read again the same data, it will be loaded extremely fast, because it has been stored in a convenient format for large datasets (<code>h5ad</code> format)</p>"},{"location":"python/Part02_filtering_sample1.html","title":"Filtering a low quality sample","text":"<p>Import the packages</p> In\u00a0[77]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn In\u00a0[78]: Copied! <pre>sample_1 = sc.read_h5ad('../../../Data/notebooks_data/sample_1.h5ad')\n</pre> sample_1 = sc.read_h5ad('../../../Data/notebooks_data/sample_1.h5ad') In\u00a0[79]: Copied! <pre>sample_1\n</pre> sample_1 Out[79]: <pre>AnnData object with n_obs \u00d7 n_vars = 23794 \u00d7 36601\n    obs: 'initial_size_spliced', 'initial_size_unspliced', 'initial_size'\n    var: 'gene_ids', 'feature_types', 'Accession', 'Chromosome', 'End', 'Start', 'Strand'\n    layers: 'ambiguous', 'spliced', 'unspliced'</pre> <p>The <code>cellRanger</code> aligner used for 10X data provides a report for the user. This contains a lot of useful statistics including the so-called knee plot, shown below on the right side of the picture. The knee plot is very useful to have an overview of the data quality. The x axis represents the barcode of each cell (ordered by decreasing number of UMIs), while the y axis shows how many UMIs are present in each cells. The line is blue where data points have been identified as cells of good quality, and it becomes lighter when the data point is less likely to be a cell.</p> <p></p> <p>This knee plot is clearly representing a low quality dataset (look at the three typical scenarios below). Moreover, we have a very low proportion of reads in each cell. Filtering this dataset might not keep many cells at the end of the process.</p> <p> Representation of knee plots scenario. From the authors of the cellBender package.</p> <p>Percentage of mitocondrial genes</p> <p>We calculate the percentage of mitocondrial genes into each cell. A high percentage denotes the possibility that material from broken cells has been captured during cell isolation, and then sequenced. Mitocondrial percentage is not usually calculated by <code>scanpy</code>, because there is need for an identifier for mitocondrial genes, and there is not a standard one. In our case, we look at genes that contain <code>MT-</code> into their ID, and calculate their transcript proportion into each cell. We save the result as an observation into <code>.obs['perc_mito']</code></p> In\u00a0[80]: Copied! <pre>sample_1.X = np.array(sample_1.X.todense()).copy()\n</pre> sample_1.X = np.array(sample_1.X.todense()).copy() In\u00a0[81]: Copied! <pre>MT = ['MT' in i for i in sample_1.var_names]\nperc_mito = np.sum( sample_1[:,MT].X, 1 ) / np.sum( sample_1.X, 1 )\nsample_1.obs['perc_mito'] = perc_mito.copy()\n</pre> MT = ['MT' in i for i in sample_1.var_names] perc_mito = np.sum( sample_1[:,MT].X, 1 ) / np.sum( sample_1.X, 1 ) sample_1.obs['perc_mito'] = perc_mito.copy() In\u00a0[82]: Copied! <pre>sc.pp.calculate_qc_metrics(sample_1, inplace=True)\n</pre> sc.pp.calculate_qc_metrics(sample_1, inplace=True) <p>Counts vs Genes: this is a typical plot, where you look at the total transcripts per cells (x axis) and detected genes per cell (y axis). Usually, those two measures grow together. Points with a lot of transcripts and genes might be multiplets (multiple cells sequenced together as one), while very few transcripts and genes denote the presence of only ambient RNA or very low quality sequencing of a cell. Below, the dots are coloured based on the percentage of mitocondrial transcripts. Note how a high proportion is often on cells with very low transcripts and genes (bottom left corner of the plot)</p> In\u00a0[83]: Copied! <pre>plt.rcParams['figure.figsize'] = (8,6)\nsc.pl.scatter(sample_1, x='total_counts', y='n_genes_by_counts', color='perc_mito', \n              title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content',\n              size=30)\n</pre> plt.rcParams['figure.figsize'] = (8,6) sc.pl.scatter(sample_1, x='total_counts', y='n_genes_by_counts', color='perc_mito',                title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content',               size=30) In\u00a0[84]: Copied! <pre>sc.pl.scatter(sample_1[sample_1.obs['total_counts']&lt;10000], x='total_counts', y='n_genes_by_counts', color='perc_mito', \n              title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content (Zoom)',\n             size=30)\n</pre> sc.pl.scatter(sample_1[sample_1.obs['total_counts']&lt;10000], x='total_counts', y='n_genes_by_counts', color='perc_mito',                title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content (Zoom)',              size=30) <p>Transcripts and Genes distribution: Here we simply look at the distribution of transcripts per cells and detected genes per cell. Note how the distribution is bimodal. This usually denotes a cluster of low-quality cells and viable cells. Sometimes filtering out the data points on the left-most modes of those graphs removes a lot of cells from a dataset, but this is quite a normal thing not to be worried about. The right side of the distributions show a tail with few cells having a lot of transcripts and genes. It is also good to filter out some of those extreme values - for technical reasons, it will also help in having a better normalization of the data later on.</p> In\u00a0[85]: Copied! <pre>ax = sns.distplot(sample_1.obs['total_counts'], bins=50)\nax.set_title('Cells Transcripts distribution')\n</pre> ax = sns.distplot(sample_1.obs['total_counts'], bins=50) ax.set_title('Cells Transcripts distribution') Out[85]: <pre>Text(0.5, 1.0, 'Cells Transcripts distribution')</pre> In\u00a0[86]: Copied! <pre>ax = sns.distplot(sample_1.obs['n_genes_by_counts'], bins=50)\nax.set_title('Distribution of detected genes per cell')\n</pre> ax = sns.distplot(sample_1.obs['n_genes_by_counts'], bins=50) ax.set_title('Distribution of detected genes per cell') Out[86]: <pre>Text(0.5, 1.0, 'Distribution of detected genes per cell')</pre> <p>In this dataset there are few cell with a high percentage of mitocondrial content. Those are precisely 77 if we set 0.1 (that is 10%) as a treshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> In\u00a0[87]: Copied! <pre>#subsetting to see how many cells have percentage of mitocondrial genes above 10%\nsample_1[ sample_1.obs['perc_mito']&gt;0.1, : ].shape\n</pre> #subsetting to see how many cells have percentage of mitocondrial genes above 10% sample_1[ sample_1.obs['perc_mito']&gt;0.1, : ].shape Out[87]: <pre>(77, 36601)</pre> In\u00a0[88]: Copied! <pre>ax = sns.distplot(sample_1.obs['perc_mito'], bins=50)\nax.set_title('Distribution of mitocondrial content per cell')\n</pre> ax = sns.distplot(sample_1.obs['perc_mito'], bins=50) ax.set_title('Distribution of mitocondrial content per cell') Out[88]: <pre>Text(0.5, 1.0, 'Distribution of mitocondrial content per cell')</pre> In\u00a0[89]: Copied! <pre>import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\nfrom sklearn.ensemble import IsolationForest\nplt.rcParams['figure.figsize'] = (16,6)\n\n#write here which quality measures you want to use\ndf = sample_1.obs[ ['n_genes_by_counts', 'total_counts', \n                    'perc_mito', 'pct_counts_in_top_50_genes'] ]\n\ndf2 = scale(df, axis=0)\n\npca = PCA(n_components=2)\n\nY = pca.fit_transform(df2)\n\nclf = IsolationForest(random_state=0)\n\npred = clf.fit_predict(df2)\n\nsample_1 = sample_1[pred==1].copy()\n\npred = pd.Categorical(pred)\npred = pred.rename_categories(['Outlier','Cell'])\ndf['Category'] = pred\n\nsns.scatterplot(Y[:,0],Y[:,1], hue = df.total_counts, style = df.Category, \n                palette=\"Blues\", sizes=(20, 200), hue_norm=(0, 100))\n</pre> import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import scale from sklearn.ensemble import IsolationForest plt.rcParams['figure.figsize'] = (16,6)  #write here which quality measures you want to use df = sample_1.obs[ ['n_genes_by_counts', 'total_counts',                      'perc_mito', 'pct_counts_in_top_50_genes'] ]  df2 = scale(df, axis=0)  pca = PCA(n_components=2)  Y = pca.fit_transform(df2)  clf = IsolationForest(random_state=0)  pred = clf.fit_predict(df2)  sample_1 = sample_1[pred==1].copy()  pred = pd.Categorical(pred) pred = pred.rename_categories(['Outlier','Cell']) df['Category'] = pred  sns.scatterplot(Y[:,0],Y[:,1], hue = df.total_counts, style = df.Category,                  palette=\"Blues\", sizes=(20, 200), hue_norm=(0, 100)) Out[89]: <pre>&lt;AxesSubplot:&gt;</pre> <p>we have removed around 3000 cells</p> In\u00a0[90]: Copied! <pre>sample_1\n</pre> sample_1 Out[90]: <pre>AnnData object with n_obs \u00d7 n_vars = 20984 \u00d7 36601\n    obs: 'initial_size_spliced', 'initial_size_unspliced', 'initial_size', 'perc_mito', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n    var: 'gene_ids', 'feature_types', 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'\n    layers: 'ambiguous', 'spliced', 'unspliced'</pre> <p>We use some thresholds by looking at the previous QC plots. Those will eventually remove some remaining low quality cells.</p> In\u00a0[92]: Copied! <pre>MIN_COUNTS = 5000   #minimum number of transcripts per cell\nMAX_COUNTS = 15000  #maximum number of transcripts per cell\nMIN_GENES = 2500    #minimum number of genes per cell\nMAX_GENES = 6000    #maximum number of genes per cell\nMAX_MITO = .1       #mitocondrial percentage treshold\n</pre> MIN_COUNTS = 5000   #minimum number of transcripts per cell MAX_COUNTS = 15000  #maximum number of transcripts per cell MIN_GENES = 2500    #minimum number of genes per cell MAX_GENES = 6000    #maximum number of genes per cell MAX_MITO = .1       #mitocondrial percentage treshold <p>We can do some subsetting to zoom into the plots we did before</p> In\u00a0[93]: Copied! <pre>plt.rcParams['figure.figsize'] = (8,6)\nsc.pl.scatter(sample_1[ sample_1.obs['total_counts']&lt;MAX_COUNTS ], \n              x='total_counts', y='n_genes_by_counts', color='perc_mito',\n              title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with threshold MAX_COUNTS={MAX_COUNTS}',\n             size=30)\n</pre> plt.rcParams['figure.figsize'] = (8,6) sc.pl.scatter(sample_1[ sample_1.obs['total_counts'] In\u00a0[94]: Copied! <pre>sc.pl.scatter(sample_1[ sample_1.obs['n_genes_by_counts'] &gt; MIN_GENES ], \n              x='total_counts', y='n_genes_by_counts', color='perc_mito',\n              title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with treshold MIN_GENES={MIN_GENES}',\n             size=30)\n</pre> sc.pl.scatter(sample_1[ sample_1.obs['n_genes_by_counts'] &gt; MIN_GENES ],                x='total_counts', y='n_genes_by_counts', color='perc_mito',               title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with treshold MIN_GENES={MIN_GENES}',              size=30) <p>The following commands filter using the chose tresholds. Again, scanpy does not do the mitocondrial QC filtering, so we do that on our own by subsetting the data.</p> <p>Note for the last two filterings: the parameter <code>min_cells</code> remove all those cells showing transcripts for only 10 genes or less - standard values for this parameter are usually between 3 and 10, and do not come from looking at the QC plots. The last command uses the standard value for the mitocondrial content treshold.</p> In\u00a0[95]: Copied! <pre>sc.preprocessing.filter_cells(sample_1, max_counts=MAX_COUNTS)\n\nsc.preprocessing.filter_cells(sample_1, min_counts=MIN_COUNTS)\n\nsc.preprocessing.filter_cells(sample_1, min_genes=MIN_GENES)\n\nsc.preprocessing.filter_cells(sample_1, max_genes=MAX_GENES)\n\nsc.preprocessing.filter_genes(sample_1, min_cells=10)\n\nsample_1 = sample_1[sample_1.obs['perc_mito']&lt;MAX_MITO].copy()\n</pre> sc.preprocessing.filter_cells(sample_1, max_counts=MAX_COUNTS)  sc.preprocessing.filter_cells(sample_1, min_counts=MIN_COUNTS)  sc.preprocessing.filter_cells(sample_1, min_genes=MIN_GENES)  sc.preprocessing.filter_cells(sample_1, max_genes=MAX_GENES)  sc.preprocessing.filter_genes(sample_1, min_cells=10)  sample_1 = sample_1[sample_1.obs['perc_mito'] <p>We can see how only a low number of cells is left.</p> In\u00a0[97]: Copied! <pre>sample_1\n</pre> sample_1 Out[97]: <pre>AnnData object with n_obs \u00d7 n_vars = 243 \u00d7 10203\n    obs: 'initial_size_spliced', 'initial_size_unspliced', 'initial_size', 'perc_mito', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'n_counts', 'n_genes'\n    var: 'gene_ids', 'feature_types', 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    layers: 'ambiguous', 'spliced', 'unspliced'</pre> <p>Another important step consists in filtering out multiplets. Those are in the almost totality of the cases doublets, because triplets and above multiplets are extremely rare. Read this more technical blog post for more explanations about this.</p> <p>The external tool <code>scrublet</code> simulates doublets by putting together the transcripts of random pairs of cells from the dataset. Then it assigns a score to each cell in the data, based on the similarity with the simulated doublets. An <code>expected_doublet_rate</code> of 0.06 (6%) is quite a typical value for single cell data, but if you have a better estimate from laboratory work, microscope imaging or a specific protocol/sequencing machine, you can also tweak the value.  <code>random_state</code> is a number choosing how the simulations are done. using a specific random state means that you will always simulate the same doublets whenever you run this code. This allows you to reproduce exactly the same results every time and is a great thing for reproducibility in your own research.</p> In\u00a0[98]: Copied! <pre>sc.external.pp.scrublet(sample_1, \n                        expected_doublet_rate=0.06,\n                        random_state=12345)\n</pre>  sc.external.pp.scrublet(sample_1,                          expected_doublet_rate=0.06,                         random_state=12345) <pre>Automatically set threshold at doublet score = 0.24\nDetected doublet rate = 7.0%\nEstimated detectable doublet fraction = 17.5%\nOverall doublet rate:\n\tExpected   = 6.0%\n\tEstimated  = 40.0%\n</pre> <p>It seems that the doublet rate is likely to be lower than 6%, meaning that in this regard the data has been produced pretty well. We now plot the doublet scores assigned to each cell by the algorithm. We can see that most cells have a low score (the score is a value between 0 and 1). Datasets with many doublets show a more bimodal distribution, while here we just have a light tail beyond 0.1.</p> In\u00a0[99]: Copied! <pre>sns.distplot(sample_1.obs['doublet_score'])\n</pre> sns.distplot(sample_1.obs['doublet_score']) Out[99]: <pre>&lt;AxesSubplot:xlabel='doublet_score', ylabel='Density'&gt;</pre> <p>We can choose 0.1 as filtering treshold for the few detected doublets or alternatively use the automatic selection of doublets by the algorithm. We will choose the last option and use the automatically chosen doublets.</p> In\u00a0[100]: Copied! <pre>sample_1 = sample_1[np.invert(sample_1.obs['predicted_doublet'])].copy()\n</pre> sample_1 = sample_1[np.invert(sample_1.obs['predicted_doublet'])].copy() <p>A quite basic but easy way to look at the results of our filtering is to normalize and plot the dataset on some projections. Here we use a standard normalization technique that consists of:</p> <ul> <li>TPM normalization: the transcripts of each cell are normalized, so that their total amounts to the same value. This should make cells more comparable independently of how many transcripts their has been retained during cell isolation.</li> <li>Logarithmization: the logarithm of the normalized transcripts is calculated. This reduce the variability of transcripts values and highlights variations due to biological factors.</li> <li>Standardization: Each gene is standardized across all cells. This is useful for example  for projecting the data onto a PCA.</li> </ul> In\u00a0[101]: Copied! <pre># TPM normalization and storage of the matrix\nsc.pp.normalize_per_cell(sample_1)\nsample_1.layers['umi_tpm'] = sample_1.X.copy()\n\n# Logarithmization and storage\nsc.pp.log1p(sample_1)\nsample_1.layers['umi_log'] = sample_1.X.copy()\n\n# Select some of the most meaningful genes to calculate the PCA plot later\n# This must be done on logarithmized values\nsc.pp.highly_variable_genes(sample_1, n_top_genes=15000)\n\n# save the dataset\nsample_1.write('../Data/notebooks_data/sample_1.filt.h5ad')\n\n# standardization and matrix storage\nsc.pp.scale(sample_1)\nsample_1.layers['umi_gauss'] = sample_1.X.copy()\n</pre> # TPM normalization and storage of the matrix sc.pp.normalize_per_cell(sample_1) sample_1.layers['umi_tpm'] = sample_1.X.copy()  # Logarithmization and storage sc.pp.log1p(sample_1) sample_1.layers['umi_log'] = sample_1.X.copy()  # Select some of the most meaningful genes to calculate the PCA plot later # This must be done on logarithmized values sc.pp.highly_variable_genes(sample_1, n_top_genes=15000)  # save the dataset sample_1.write('../Data/notebooks_data/sample_1.filt.h5ad')  # standardization and matrix storage sc.pp.scale(sample_1) sample_1.layers['umi_gauss'] = sample_1.X.copy() <p>Now we calculate the PCA projection</p> In\u00a0[102]: Copied! <pre>sc.preprocessing.pca(sample_1, svd_solver='arpack', random_state=12345)\n</pre> sc.preprocessing.pca(sample_1, svd_solver='arpack', random_state=12345) <p>We can look at the PCA plot and color it by some quality measure and gene expression. Even though we filtered a lot, there is still some structure left. But with so few cells, the dataset is not worth of usage, since the other samples have thousands of cells of better quality</p> In\u00a0[104]: Copied! <pre>sc.pl.pca(sample_1, color=['total_counts','SYCP1'])\n</pre> sc.pl.pca(sample_1, color=['total_counts','SYCP1']) <p>We plot the variance ratio to see how each component of the PCA changes in variability. Small changes in variability denote that the components are mostly modeling noise in the data. We can choose a threshold (for example 15 PCA components) to be used in all algorithms that use PCA to calculate any quantity.</p> In\u00a0[105]: Copied! <pre>sc.plotting.pca_variance_ratio(sample_1)\n</pre> sc.plotting.pca_variance_ratio(sample_1) <p>We project the data using the UMAP algorithm. This is very good in preserving the structure of a dataset in low dimension, if any is present. We first calculate the neighbors of each cell (that is, its most similar cells), those are then used for the UMAP. The neighbors are calculated using the PCA matrix instead of the full data matrix, so we can choose the number of PCA components to use (parameter <code>n_pcs</code>). Many algorithms work on the PCA, so you will see the parameter used again in other places.</p> In\u00a0[106]: Copied! <pre>sc.pp.neighbors(sample_1, n_pcs=15, random_state=12345)\n</pre> sc.pp.neighbors(sample_1, n_pcs=15, random_state=12345) In\u00a0[107]: Copied! <pre>sc.tools.umap(sample_1, random_state=54321)\n</pre> sc.tools.umap(sample_1, random_state=54321) <p>The UMAP plot gives a pretty well-structured output for this dataset. We will keep working further with this filtering.</p> In\u00a0[108]: Copied! <pre>sc.plotting.umap(sample_1, color=['total_counts','SYCP1'])\n</pre> sc.plotting.umap(sample_1, color=['total_counts','SYCP1']) <p>We have looked through a low quality dataset, and applied PCA-based and threshold-based filterings. The filtering resulted in removing almost all available data points.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"python/Part02_filtering_sample1.html#filtering-a-low-quality-sample","title":"Filtering a low quality sample\u00b6","text":"<p>Application of a PCA-based technique for filtering</p> <p>Motivation:</p> <p>Quality control and filtering is the most important steps of single cell data analysis. Allowing low quality cells into your analysis will compromise/mislead your conclusions by adding hundreds of meaningless data points to your workflow. The main sources of low quality cells are</p> <ul> <li>broken cells for which some of their transcripts get lost</li> <li>cells isolated together with too much ambient RNA</li> <li>missing cell during isolation (e.g. empty droplet in microfluidic machines)</li> <li>multiple cells isolated together (multiplets, usually only two cells - doublets)</li> </ul> <p>A typical way of filtering cells is to look at thresholds for various QC measures. Here, we apply also a PCA technique that looks at all QC measures available at the same time.</p> <p>Learning objectives:</p> <ul> <li>Understand and discuss QC issues and measures from single cell data</li> <li>Explore QC graphs and set filtering tools and thresholds</li> <li>Understand and apply PCA outliers detection</li> <li>Analyze the results of QC filters and evaluate necessity for different filtering</li> </ul> <p>Execution time: 40 minutes</p>"},{"location":"python/Part02_filtering_sample1.html#read-the-kneeplot-for-10x-data-with-cellranger","title":"Read the kneeplot (for 10X data with cellranger)\u00b6","text":""},{"location":"python/Part02_filtering_sample1.html#visualize-and-evaluate-quality-measures","title":"Visualize and evaluate quality measures\u00b6","text":"<p>We can do some plots to have a look at quality measures combined together</p>"},{"location":"python/Part02_filtering_sample1.html#pca-based-filtering","title":"PCA-based filtering\u00b6","text":"<p>Now we calculate a PCA on a set of quality measures, and find out the outliers in the resulting PCA projections. We remove these outliers from the dataset: they will be data points with an anomalous subset of quality measures. This technique is rewritten from the <code>R</code> package <code>scran</code>. The script below does the filtering and plots the PCA with the outliers represented by circles.</p>"},{"location":"python/Part02_filtering_sample1.html#choosing-thresholds","title":"Choosing thresholds\u00b6","text":""},{"location":"python/Part02_filtering_sample1.html#doublet-filtering","title":"Doublet filtering\u00b6","text":""},{"location":"python/Part02_filtering_sample1.html#evaluation-of-filtering","title":"Evaluation of filtering\u00b6","text":""},{"location":"python/Part02_filtering_sample1.html#wrapping-up","title":"Wrapping up\u00b6","text":""},{"location":"python/Part02_filtering_sample2.html","title":"Quality Control (QC) and filtering","text":"<p>Import the packages</p> In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport ipywidgets as widgets\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn import ipywidgets as widgets In\u00a0[2]: Copied! <pre>sample_2 = sc.read_h5ad('../../Data/notebooks_data/sample_2.h5ad')\n</pre> sample_2 = sc.read_h5ad('../../Data/notebooks_data/sample_2.h5ad') <p>We calculate the percentage of mitocondrial genes into each cell. A high percentage denotes the possibility that material from broken cells has been captured during cell isolation, and then sequenced. Mitocondrial percentage is not usually calculated by <code>scanpy</code>, because there is need for an identifier for mitocondrial genes, and there is not a standard one. In our case, we look at genes that contain <code>MT-</code> into their ID, and calculate their transcript proportion into each cell. We save the result as an observation into <code>.obs['perc_mito']</code></p> In\u00a0[3]: Copied! <pre>MT = ['MT' in i for i in sample_2.var_names]\nperc_mito = np.sum( sample_2[:,MT].X, 1 ) / np.sum( sample_2.X, 1 )\nsample_2.obs['perc_mito'] = perc_mito.copy()\n</pre> MT = ['MT' in i for i in sample_2.var_names] perc_mito = np.sum( sample_2[:,MT].X, 1 ) / np.sum( sample_2.X, 1 ) sample_2.obs['perc_mito'] = perc_mito.copy() <p>Counts vs Genes: this is a typical plot, where you look at the total transcripts per cells (x axis) and detected genes per cell (y axis). Usually, those two measures grow together. Points with a lot of transcripts and genes might be multiplets (multiple cells sequenced together as one), while very few transcripts and genes denote the presence of only ambient RNA or very low quality sequencing of a cell. Below, the dots are coloured based on the percentage of mitocondrial transcripts. Note how a high proportion is often on cells with very low transcripts and genes (bottom left corner of the plot)</p> In\u00a0[4]: Copied! <pre>sc.pl.scatter(sample_2, x='total_counts', y='n_genes_by_counts', color='perc_mito', \n              title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content',\n             size=50)\n</pre> sc.pl.scatter(sample_2, x='total_counts', y='n_genes_by_counts', color='perc_mito',                title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content',              size=50) <p>Transcripts and Genes distribution: Here we simply look at the distribution of transcripts per cells and detected genes per cell. Note how the distribution is bimodal. This usually denotes a cluster of low-quality cells and viable cells. Sometimes filtering out the data points on the left-most modes of those graphs removes a lot of cells from a dataset, but this is quite a normal thing not to be worried about. The right side of the distributions show a tail with few cells having a lot of transcripts and genes. It is also good to filter out some of those extreme values - for technical reasons, it will also help in having a better normalization of the data later on.</p> In\u00a0[5]: Copied! <pre>ax = sns.distplot(sample_2.obs['total_counts'], bins=50)\nax.set_title('Cells Transcripts distribution')\n</pre> ax = sns.distplot(sample_2.obs['total_counts'], bins=50) ax.set_title('Cells Transcripts distribution') Out[5]: <pre>Text(0.5, 1.0, 'Cells Transcripts distribution')</pre> In\u00a0[6]: Copied! <pre>ax = sns.distplot(sample_2.obs['n_genes_by_counts'], bins=50)\nax.set_title('Distribution of detected genes per cell')\n</pre> ax = sns.distplot(sample_2.obs['n_genes_by_counts'], bins=50) ax.set_title('Distribution of detected genes per cell') Out[6]: <pre>Text(0.5, 1.0, 'Distribution of detected genes per cell')</pre> <p>Mitocondrial content: In this dataset there are few cell with a high percentage of mitocondrial content. Those are precisely 245 if we set 0.1 (that is 10%) as a treshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> In\u00a0[7]: Copied! <pre>#subsetting to see how many cells have percentage of mitocondrial genes above 10%\nsample_2[ sample_2.obs['perc_mito']&gt;0.1, : ].shape\n</pre> #subsetting to see how many cells have percentage of mitocondrial genes above 10% sample_2[ sample_2.obs['perc_mito']&gt;0.1, : ].shape Out[7]: <pre>(245, 36601)</pre> In\u00a0[8]: Copied! <pre>ax = sns.distplot(sample_2.obs['perc_mito'], bins=50)\nax.set_title('Distribution of mitocondrial content per cell')\n</pre> ax = sns.distplot(sample_2.obs['perc_mito'], bins=50) ax.set_title('Distribution of mitocondrial content per cell') Out[8]: <pre>Text(0.5, 1.0, 'Distribution of mitocondrial content per cell')</pre> <p>Let's establish some filtering values by looking at the plots above, and then apply filtering</p> In\u00a0[9]: Copied! <pre>MIN_COUNTS = 5000  #minimum number of transcripts per cell\nMAX_COUNTS = 30000 #maximum number of transcripts per cell\nMIN_GENES = 2000   #minimum number of genes per cell\nMAX_GENES = 6000   #maximum number of genes per cell\nMAX_MITO = .1      #mitocondrial percentage treshold)\n    \n#plot cells filtered by max transcripts\na=sc.pl.scatter(sample_2[ sample_2.obs['total_counts']&lt;MAX_COUNTS ], \n             x='total_counts', y='n_genes_by_counts', color='perc_mito', size=50,\n              title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with threshold MAX_COUNTS={MAX_COUNTS}')\n#plot cells filtered by min genes\nb=sc.pl.scatter(sample_2[ sample_2.obs['n_genes_by_counts'] &gt; MIN_GENES ], \n              x='total_counts', y='n_genes_by_counts', color='perc_mito', size=50,\n              title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with treshold MIN_GENES={MIN_GENES}')\n</pre> MIN_COUNTS = 5000  #minimum number of transcripts per cell MAX_COUNTS = 30000 #maximum number of transcripts per cell MIN_GENES = 2000   #minimum number of genes per cell MAX_GENES = 6000   #maximum number of genes per cell MAX_MITO = .1      #mitocondrial percentage treshold)      #plot cells filtered by max transcripts a=sc.pl.scatter(sample_2[ sample_2.obs['total_counts'] MIN_GENES ],                x='total_counts', y='n_genes_by_counts', color='perc_mito', size=50,               title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with treshold MIN_GENES={MIN_GENES}')     <p>The following commands filter using the chosen tresholds.  Again, scanpy does not do the mitocondrial QC filtering,  so we do that on our own by subsetting the data.  Note for the last two filterings: the parameter <code>min_cells</code> remove  all those cells showing transcripts for only 10 genes or less -  standard values for this parameter are usually between 3 and 10,  and do not come from looking at the QC plots.</p> In\u00a0[10]: Copied! <pre>sc.preprocessing.filter_cells(sample_2, max_counts=MAX_COUNTS)\n\nsc.preprocessing.filter_cells(sample_2, min_counts=MIN_COUNTS)\n\nsc.preprocessing.filter_cells(sample_2, min_genes=MIN_GENES)\n\nsc.preprocessing.filter_cells(sample_2, max_genes=MAX_GENES)\n\nsc.preprocessing.filter_genes(sample_2, min_cells=10)\n\nsample_2 = sample_2[sample_2.obs['perc_mito']&lt;MAX_MITO].copy()\n</pre> sc.preprocessing.filter_cells(sample_2, max_counts=MAX_COUNTS)  sc.preprocessing.filter_cells(sample_2, min_counts=MIN_COUNTS)  sc.preprocessing.filter_cells(sample_2, min_genes=MIN_GENES)  sc.preprocessing.filter_cells(sample_2, max_genes=MAX_GENES)  sc.preprocessing.filter_genes(sample_2, min_cells=10)  sample_2 = sample_2[sample_2.obs['perc_mito'] <p>We have been reducing the data quite a lot from the original &gt;8000 cells. Often, even more aggressive filterings are done. For example, one could have set the minimum number of genes detected to 3000. It would have been anyway in the area between the two modes of the QC plot.</p> In\u00a0[11]: Copied! <pre>print(f'Cells after filters: {sample_2.shape[0]}, Genes after filters: {sample_2.shape[1]}')\n</pre> print(f'Cells after filters: {sample_2.shape[0]}, Genes after filters: {sample_2.shape[1]}') <pre>Cells after filters: 3881, Genes after filters: 24343\n</pre> <p>Another important step consists in filtering out multiplets. Those are in the almost totality of the cases doublets, because triplets and above multiplets are extremely rare. Read this more technical blog post for more explanations about this.</p> <p>The external tool <code>scrublet</code> simulates doublets by putting together the transcripts of random pairs of cells from the dataset. Then it assigns a score to each cell in the data, based on the similarity with the simulated doublets. An <code>expected_doublet_rate</code> of 0.06 (6%) is quite a typical value for single cell data, but if you have a better estimate from laboratory work, microscope imaging or a specific protocol/sequencing machine, you can also tweak the value.  <code>random_state</code> is a number choosing how the simulations are done. using a specific random state means that you will always simulate the same doublets whenever you run this code. This allows you to reproduce exactly the same results every time and is a great thing for reproducibility in your own research.</p> In\u00a0[12]: Copied! <pre>sc.external.pp.scrublet(sample_2, \n                        expected_doublet_rate=0.06,\n                        random_state=12345)\n</pre>  sc.external.pp.scrublet(sample_2,                          expected_doublet_rate=0.06,                         random_state=12345) <pre>Automatically set threshold at doublet score = 0.26\nDetected doublet rate = 2.4%\nEstimated detectable doublet fraction = 61.6%\nOverall doublet rate:\n\tExpected   = 6.0%\n\tEstimated  = 3.9%\n</pre> <p>It seems that the doublet rate is likely to be lower than 6%, meaning that in this regard the data has been produced pretty well. We now plot the doublet scores assigned to each cell by the algorithm. We can see that most cells have a low score (the score is a value between 0 and 1). Datasets with many doublets show a more bimodal distribution, while here we just have a light tail beyond 0.1.</p> In\u00a0[13]: Copied! <pre>sns.distplot(sample_2.obs['doublet_score'])\n</pre> sns.distplot(sample_2.obs['doublet_score']) Out[13]: <pre>&lt;AxesSubplot:xlabel='doublet_score', ylabel='Density'&gt;</pre> <p>We can choose 0.1 as filtering treshold for the few detected doublets or alternatively use the automatic selection of doublets by the algorithm. We will choose the last option and use the automatically chosen doublets.</p> In\u00a0[14]: Copied! <pre>sample_2 = sample_2[np.invert(sample_2.obs['predicted_doublet'])].copy()\n</pre> sample_2 = sample_2[np.invert(sample_2.obs['predicted_doublet'])].copy() <p>A quite basic but easy way to look at the results of our filtering is to normalize and plot the dataset on some projections. Here we use a standard normalization technique that consists of:</p> <ul> <li>TPM normalization: the transcripts of each cell are normalized, so that their total amounts to the same value in each cell. This should make cells more comparable independently of how many transcripts has been retained during cell isolation.</li> <li>Logarithmization: the logarithm of the normalized transcripts is calculated. This reduce the variability of transcripts values and highlights variations due to biological factors.</li> <li>Standardization: Each gene is standardized across all cells. This is useful for example  for projecting the data onto a PCA.</li> </ul> In\u00a0[15]: Copied! <pre># TPM normalization and storage of the matrix\nsc.pp.normalize_per_cell(sample_2)\nsample_2.layers['umi_tpm'] = sample_2.X.copy()\n\n# Logarithmization and storage\nsc.pp.log1p(sample_2)\nsample_2.layers['umi_log'] = sample_2.X.copy()\n\n# Select some of the most meaningful genes to calculate the PCA plot later\n# This must be done on logarithmized values\nsc.pp.highly_variable_genes(sample_2, n_top_genes=15000)\n\n# save the dataset\nsample_2.write('../../Data/notebooks_data/sample_2.filt.h5ad')\n\n# standardization and matrix storage\nsc.pp.scale(sample_2)\nsample_2.layers['umi_gauss'] = sample_2.X.copy()\n</pre> # TPM normalization and storage of the matrix sc.pp.normalize_per_cell(sample_2) sample_2.layers['umi_tpm'] = sample_2.X.copy()  # Logarithmization and storage sc.pp.log1p(sample_2) sample_2.layers['umi_log'] = sample_2.X.copy()  # Select some of the most meaningful genes to calculate the PCA plot later # This must be done on logarithmized values sc.pp.highly_variable_genes(sample_2, n_top_genes=15000)  # save the dataset sample_2.write('../../Data/notebooks_data/sample_2.filt.h5ad')  # standardization and matrix storage sc.pp.scale(sample_2) sample_2.layers['umi_gauss'] = sample_2.X.copy() <p>Now we calculate the PCA projection</p> In\u00a0[16]: Copied! <pre>sc.preprocessing.pca(sample_2, svd_solver='arpack', random_state=12345)\n</pre> sc.preprocessing.pca(sample_2, svd_solver='arpack', random_state=12345) <p>We can look at the PCA plot and color it by some quality measure and gene expression. We can already see how the PCA has a clear structure with only a few dots sparsed around. It seems the filtering has got a good result.</p> In\u00a0[17]: Copied! <pre>sc.pl.pca(sample_2, color=['total_counts','SYCP1'])\n</pre> sc.pl.pca(sample_2, color=['total_counts','SYCP1']) <p>We plot the variance ratio to see how each component of the PCA changes in variability. Small changes in variability denote that the components are mostly modeling noise in the data. We can choose a threshold (for example 15 PCA components) to be used in all algorithms that use PCA to calculate any quantity.</p> In\u00a0[18]: Copied! <pre>sc.plotting.pca_variance_ratio(sample_2)\n</pre> sc.plotting.pca_variance_ratio(sample_2) <p>We project the data using the UMAP algorithm. This is very good in preserving the structure of a dataset in low dimension, if any is present. We first calculate the neighbors of each cell (that is, its most similar cells), those are then used for the UMAP. The neighbors are calculated using the PCA matrix instead of the full data matrix, so we can choose the number of PCA components to use (parameter <code>n_pcs</code>). Many algorithms work on the PCA, so you will see the parameter used again in other places.</p> In\u00a0[19]: Copied! <pre>sc.pp.neighbors(sample_2, n_pcs=15, random_state=12345)\n</pre> sc.pp.neighbors(sample_2, n_pcs=15, random_state=12345) In\u00a0[20]: Copied! <pre>sc.tools.umap(sample_2, random_state=54321)\n</pre> sc.tools.umap(sample_2, random_state=54321) <p>The UMAP plot gives a pretty well-structured output for this dataset. We will keep working further with this filtering.</p> In\u00a0[21]: Copied! <pre>sc.plotting.umap(sample_2, color=['total_counts','SYCP1'])\n</pre> sc.plotting.umap(sample_2, color=['total_counts','SYCP1']) <p>We have succesfully gone through the filtering of a single cell dataset with good results that can be used further in the data analysis. In the next notebook <code>Normalize and Integrate</code>, we will integrate this dataset (testis cells from a healthy adult man) with the same type of sample from another man.</p> <p>Optional:</p> <p>You can look at the preprocessing of the second sample from healthy man on the webpage of the course in the section <code>Extras</code>, opening <code>Filtering another sample</code> in the submenu. As you could see, this dataset seemed pretty ok to handle. However, another dataset of much lower quality is present, and is not going to be integrated in the coming data analysis. Its preprocessing is shown as the submenu <code>Filtering a low quality sample</code> in the section <code>Extra</code> of the course webpage. In it we will also show an aggressive filtering done using a combination of PCA technique and automatic outliers detection.</p>"},{"location":"python/Part02_filtering_sample2.html#quality-control-qc-and-filtering","title":"Quality Control (QC) and filtering\u00b6","text":"<p>Motivation:</p> <p>Quality control and filtering is the most important steps of single cell data analysis. Allowing low quality cells into your analysis will compromise/mislead your conclusions by adding hundreds of meaningless data points to your workflow. The main sources of low quality cells are</p> <ul> <li>broken cells for which some of their transcripts get lost</li> <li>cells isolated together with too much ambient RNA</li> <li>missing cell during isolation (e.g. empty droplet in microfluidic machines)</li> <li>multiple cells isolated together (multiplets, usually only two cells - doublets)</li> </ul> <p>Learning objectives:</p> <ul> <li>Understand and discuss QC issues and measures from single cell data</li> <li>Explore QC graphs and set filtering tools and thresholds</li> <li>Analyze the results of QC filters and evaluate necessity for different filtering</li> </ul> <p>Execution time: 40 minutes</p>"},{"location":"python/Part02_filtering_sample2.html#visualize-and-evaluate-quality-measures","title":"Visualize and evaluate quality measures\u00b6","text":"<p>We can do some plots to have a look at quality measures combined together</p>"},{"location":"python/Part02_filtering_sample2.html#choosing-thresholds","title":"Choosing thresholds\u00b6","text":""},{"location":"python/Part02_filtering_sample2.html#doublet-filtering","title":"Doublet filtering\u00b6","text":""},{"location":"python/Part02_filtering_sample2.html#evaluation-of-filtering","title":"Evaluation of filtering\u00b6","text":""},{"location":"python/Part02_filtering_sample2.html#wrapping-up","title":"Wrapping up\u00b6","text":""},{"location":"python/Part02_filtering_sample3.html","title":"Filtering another sample","text":"<p>Import the packages</p> In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn In\u00a0[4]: Copied! <pre>sample_3 = sc.read_h5ad('../../../Data/notebooks_data/sample_3.h5ad')\n</pre> sample_3 = sc.read_h5ad('../../../Data/notebooks_data/sample_3.h5ad') <p>We calculate the percentage of mitocondrial genes into each cell. A high percentage denotes the possibility that material from broken cells has been captured during cell isolation, and then sequenced. Mitocondrial percentage is not usually calculated by <code>scanpy</code>, because there is need for an identifier for mitocondrial genes, and there is not a standard one. In our case, we look at genes that contain <code>MT-</code> into their ID, and calculate their transcript proportion into each cell. We save the result as an observation into <code>.obs['perc_mito']</code></p> In\u00a0[5]: Copied! <pre>MT = ['MT' in i for i in sample_3.var_names]\nperc_mito = np.sum( sample_3[:,MT].X, 1 ) / np.sum( sample_3.X, 1 )\nsample_3.obs['perc_mito'] = perc_mito.copy()\n</pre> MT = ['MT' in i for i in sample_3.var_names] perc_mito = np.sum( sample_3[:,MT].X, 1 ) / np.sum( sample_3.X, 1 ) sample_3.obs['perc_mito'] = perc_mito.copy() <p>Counts vs Genes: this is a typical plot, where you look at the total transcripts per cells (x axis) and detected genes per cell (y axis). Usually, those two measures grow together. Points with a lot of transcripts and genes might be multiplets (multiple cells sequenced together as one), while very few transcripts and genes denote the presence of only ambient RNA or very low quality sequencing of a cell. Below, the dots are coloured based on the percentage of mitocondrial transcripts. Note how a high proportion is often on cells with very low transcripts and genes (bottom left corner of the plot)</p> In\u00a0[6]: Copied! <pre>sc.pl.scatter(sample_3, x='total_counts', y='n_genes_by_counts', color='perc_mito', \n              title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content')\n</pre> sc.pl.scatter(sample_3, x='total_counts', y='n_genes_by_counts', color='perc_mito',                title ='Nr of transcripts vs Nr detected genes, coloured by mitocondrial content') <p>Transcripts and Genes distribution: Here we simply look at the distribution of transcripts per cells and detected genes per cell. Note how the distribution is bimodal. This usually denotes a cluster of low-quality cells and viable cells. Sometimes filtering out the data points on the left-most modes of those graphs removes a lot of cells from a dataset, but this is quite a normal thing not to be worried about. The right side of the distributions show a tail with few cells having a lot of transcripts and genes. It is also good to filter out some of those extreme values - for technical reasons, it will also help in having a better normalization of the data later on.</p> In\u00a0[7]: Copied! <pre>ax = sns.distplot(sample_3.obs['total_counts'], bins=50)\nax.set_title('Cells Transcripts distribution')\n</pre> ax = sns.distplot(sample_3.obs['total_counts'], bins=50) ax.set_title('Cells Transcripts distribution') Out[7]: <pre>Text(0.5, 1.0, 'Cells Transcripts distribution')</pre> In\u00a0[8]: Copied! <pre>ax = sns.distplot(sample_3.obs['n_genes_by_counts'], bins=50)\nax.set_title('Distribution of detected genes per cell')\n</pre> ax = sns.distplot(sample_3.obs['n_genes_by_counts'], bins=50) ax.set_title('Distribution of detected genes per cell') Out[8]: <pre>Text(0.5, 1.0, 'Distribution of detected genes per cell')</pre> <p>In this dataset there are few cell with a high percentage of mitocondrial content. Those are precisely 245 if we set 0.1 (that is 10%) as a treshold. A value between 10% and 20% is the usual standard when filtering single cell datasets.</p> In\u00a0[9]: Copied! <pre>#subsetting to see how many cells have percentage of mitocondrial genes above 10%\nsample_3[ sample_3.obs['perc_mito']&gt;0.1, : ].shape\n</pre> #subsetting to see how many cells have percentage of mitocondrial genes above 10% sample_3[ sample_3.obs['perc_mito']&gt;0.1, : ].shape Out[9]: <pre>(474, 36601)</pre> In\u00a0[10]: Copied! <pre>ax = sns.distplot(sample_3.obs['perc_mito'], bins=50)\nax.set_title('Distribution of mitocondrial content per cell')\n</pre> ax = sns.distplot(sample_3.obs['perc_mito'], bins=50) ax.set_title('Distribution of mitocondrial content per cell') Out[10]: <pre>Text(0.5, 1.0, 'Distribution of mitocondrial content per cell')</pre> <p>Let's establish some filtering values by looking at the plots above.</p> In\u00a0[11]: Copied! <pre>MIN_COUNTS = 5000   #minimum number of transcripts per cell\nMAX_COUNTS = 40000  #maximum number of transcripts per cell\nMIN_GENES = 2000    #minimum number of genes per cell\nMAX_GENES = 6000    #maximum number of genes per cell\nMAX_MITO = .1       #mitocondrial percentage treshold\n</pre> MIN_COUNTS = 5000   #minimum number of transcripts per cell MAX_COUNTS = 40000  #maximum number of transcripts per cell MIN_GENES = 2000    #minimum number of genes per cell MAX_GENES = 6000    #maximum number of genes per cell MAX_MITO = .1       #mitocondrial percentage treshold <p>We can do some subsetting to zoom into the plots we did before</p> In\u00a0[12]: Copied! <pre>sc.pl.scatter(sample_3[ sample_3.obs['total_counts']&lt;MAX_COUNTS ], \n              x='total_counts', y='n_genes_by_counts', color='perc_mito',\n              title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with threshold MAX_COUNTS={MAX_COUNTS}')\n</pre> sc.pl.scatter(sample_3[ sample_3.obs['total_counts'] In\u00a0[13]: Copied! <pre>sc.pl.scatter(sample_3[ sample_3.obs['n_genes_by_counts'] &gt; MIN_GENES ], \n              x='total_counts', y='n_genes_by_counts', color='perc_mito',\n              title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with treshold MIN_GENES={MIN_GENES}')\n</pre> sc.pl.scatter(sample_3[ sample_3.obs['n_genes_by_counts'] &gt; MIN_GENES ],                x='total_counts', y='n_genes_by_counts', color='perc_mito',               title =f'Nr of transcripts vs Nr detected genes, coloured by mitocondrial content\\nsubsetting with treshold MIN_GENES={MIN_GENES}') <p>The following commands filter using the chose tresholds. Again, scanpy does not do the mitocondrial QC filtering, so we do that on our own by subsetting the data.</p> <p>Note for the last two filterings: the parameter <code>min_cells</code> remove all those cells showing transcripts for only 10 genes or less - standard values for this parameter are usually between 3 and 10, and do not come from looking at the QC plots. The last command uses the standard value for the mitocondrial content treshold.</p> In\u00a0[14]: Copied! <pre>sc.preprocessing.filter_cells(sample_3, max_counts=MAX_COUNTS)\n\nsc.preprocessing.filter_cells(sample_3, min_counts=MIN_COUNTS)\n\nsc.preprocessing.filter_cells(sample_3, min_genes=MIN_GENES)\n\nsc.preprocessing.filter_cells(sample_3, max_genes=MAX_GENES)\n\nsc.preprocessing.filter_genes(sample_3, min_cells=10)\n\nsample_3 = sample_3[sample_3.obs['perc_mito']&lt;MAX_MITO].copy()\n</pre> sc.preprocessing.filter_cells(sample_3, max_counts=MAX_COUNTS)  sc.preprocessing.filter_cells(sample_3, min_counts=MIN_COUNTS)  sc.preprocessing.filter_cells(sample_3, min_genes=MIN_GENES)  sc.preprocessing.filter_cells(sample_3, max_genes=MAX_GENES)  sc.preprocessing.filter_genes(sample_3, min_cells=10)  sample_3 = sample_3[sample_3.obs['perc_mito'] <p>We have been reducing the data quite a lot from the original &gt;8000 cells. Often, even more aggressive filterings are done. For example, one could have set the <code>MIN_GENES</code> parameter to 3000. It would have been anyway in the area between the two modes of the QC plot.</p> In\u00a0[15]: Copied! <pre>sample_3\n</pre> sample_3 Out[15]: <pre>AnnData object with n_obs \u00d7 n_vars = 2690 \u00d7 23454\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'perc_mito', 'n_counts', 'n_genes'\n    var: 'gene_ids', 'feature_types', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells'\n    layers: 'umi_raw'</pre> <p>Another important step consists in filtering out multiplets. Those are in the almost totality of the cases doublets, because triplets and above multiplets are extremely rare. Read this more technical blog post for more explanations about this.</p> <p>The external tool <code>scrublet</code> simulates doublets by putting together the transcripts of random pairs of cells from the dataset. Then it assigns a score to each cell in the data, based on the similarity with the simulated doublets. An <code>expected_doublet_rate</code> of 0.06 (6%) is quite a typical value for single cell data, but if you have a better estimate from laboratory work, microscope imaging or a specific protocol/sequencing machine, you can also tweak the value.  <code>random_state</code> is a number choosing how the simulations are done. using a specific random state means that you will always simulate the same doublets whenever you run this code. This allows you to reproduce exactly the same results every time and is a great thing for reproducibility in your own research.</p> In\u00a0[16]: Copied! <pre>sc.external.pp.scrublet(sample_3, \n                        expected_doublet_rate=0.06,\n                        random_state=12345)\n</pre>  sc.external.pp.scrublet(sample_3,                          expected_doublet_rate=0.06,                         random_state=12345) <pre>Automatically set threshold at doublet score = 0.31\nDetected doublet rate = 1.7%\nEstimated detectable doublet fraction = 55.3%\nOverall doublet rate:\n\tExpected   = 6.0%\n\tEstimated  = 3.2%\n</pre> <p>It seems that the doublet rate is likely to be lower than 6%, meaning that in this regard the data has been produced pretty well. We now plot the doublet scores assigned to each cell by the algorithm. We can see that most cells have a low score (the score is a value between 0 and 1). Datasets with many doublets show a more bimodal distribution, while here we just have a light tail beyond 0.1.</p> In\u00a0[17]: Copied! <pre>sns.distplot(sample_3.obs['doublet_score'])\n</pre> sns.distplot(sample_3.obs['doublet_score']) Out[17]: <pre>&lt;AxesSubplot:xlabel='doublet_score', ylabel='Density'&gt;</pre> <p>We can choose 0.1 as filtering treshold for the few detected doublets or alternatively use the automatic selection of doublets by the algorithm. We will choose the last option and use the automatically chosen doublets.</p> In\u00a0[18]: Copied! <pre>sample_3 = sample_3[np.invert(sample_3.obs['predicted_doublet'])].copy()\n</pre> sample_3 = sample_3[np.invert(sample_3.obs['predicted_doublet'])].copy() <p>A quite basic but easy way to look at the results of our filtering is to normalize and plot the dataset on some projections. Here we use a standard normalization technique that consists of:</p> <ul> <li>TPM normalization: the transcripts of each cell are normalized, so that their total amounts to the same value. This should make cells more comparable independently of how many transcripts their has been retained during cell isolation.</li> <li>Logarithmization: the logarithm of the normalized transcripts is calculated. This reduce the variability of transcripts values and highlights variations due to biological factors.</li> <li>Standardization: Each gene is standardized across all cells. This is useful for example  for projecting the data onto a PCA.</li> </ul> In\u00a0[21]: Copied! <pre># TPM normalization and storage of the matrix\nsc.pp.normalize_per_cell(sample_3)\nsample_3.layers['umi_tpm'] = sample_3.X.copy()\n\n# Logarithmization and storage\nsc.pp.log1p(sample_3)\nsample_3.layers['umi_log'] = sample_3.X.copy()\n\n# Select some of the most meaningful genes to calculate the PCA plot later\n# This must be done on logarithmized values\nsc.pp.highly_variable_genes(sample_3, n_top_genes=15000)\n\n# save the dataset\nsample_3.write('../../Data/notebooks_data/sample_3.filt.h5ad')\n\n# standardization and matrix storage\nsc.pp.scale(sample_3)\nsample_3.layers['umi_gauss'] = sample_3.X.copy()\n</pre> # TPM normalization and storage of the matrix sc.pp.normalize_per_cell(sample_3) sample_3.layers['umi_tpm'] = sample_3.X.copy()  # Logarithmization and storage sc.pp.log1p(sample_3) sample_3.layers['umi_log'] = sample_3.X.copy()  # Select some of the most meaningful genes to calculate the PCA plot later # This must be done on logarithmized values sc.pp.highly_variable_genes(sample_3, n_top_genes=15000)  # save the dataset sample_3.write('../../Data/notebooks_data/sample_3.filt.h5ad')  # standardization and matrix storage sc.pp.scale(sample_3) sample_3.layers['umi_gauss'] = sample_3.X.copy() <pre>WARNING: adata.X seems to be already log-transformed.\n</pre> <p>Now we calculate the PCA projection</p> In\u00a0[22]: Copied! <pre>sc.preprocessing.pca(sample_3, svd_solver='arpack', random_state=12345)\n</pre> sc.preprocessing.pca(sample_3, svd_solver='arpack', random_state=12345) <p>We can look at the PCA plot and color it by some quality measure and gene expression. We can already see how the PCA has a clear structure with only a few dots sparsed around. It seems the filtering has got a good result.</p> In\u00a0[23]: Copied! <pre>sc.pl.pca(sample_3, color=['total_counts','SYCP1'])\n</pre> sc.pl.pca(sample_3, color=['total_counts','SYCP1']) <p>We plot the variance ratio to see how each component of the PCA changes in variability. Small changes in variability denote that the components are mostly modeling noise in the data. We can choose a threshold (for example 15 PCA components) to be used in all algorithms that use PCA to calculate any quantity.</p> In\u00a0[24]: Copied! <pre>sc.plotting.pca_variance_ratio(sample_3)\n</pre> sc.plotting.pca_variance_ratio(sample_3) <p>We project the data using the UMAP algorithm. This is very good in preserving the structure of a dataset in low dimension, if any is present. We first calculate the neighbors of each cell (that is, its most similar cells), those are then used for the UMAP. The neighbors are calculated using the PCA matrix instead of the full data matrix, so we can choose the number of PCA components to use (parameter <code>n_pcs</code>). Many algorithms work on the PCA, so you will see the parameter used again in other places.</p> In\u00a0[25]: Copied! <pre>sc.pp.neighbors(sample_3, n_pcs=15, random_state=12345)\n</pre> sc.pp.neighbors(sample_3, n_pcs=15, random_state=12345) In\u00a0[\u00a0]: Copied! <pre>sc.tools.umap(sample_3, random_state=54321)\n</pre> sc.tools.umap(sample_3, random_state=54321) <p>The UMAP plot gives a pretty well-structured output for this dataset. We will keep working further with this filtering.</p> In\u00a0[\u00a0]: Copied! <pre>sc.plotting.umap(sample_3, color=['total_counts','SYCP1'])\n</pre> sc.plotting.umap(sample_3, color=['total_counts','SYCP1']) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"python/Part02_filtering_sample3.html#filtering-another-sample","title":"Filtering another sample\u00b6","text":"<p>Here we filter the second sample to be used in our data analysis.</p> <p>Execution time: 40 minutes</p>"},{"location":"python/Part02_filtering_sample3.html#visualize-and-evaluate-quality-measures","title":"Visualize and evaluate quality measures\u00b6","text":"<p>We can do some plots to have a look at quality measures combined together</p>"},{"location":"python/Part02_filtering_sample3.html#choosing-thresholds","title":"Choosing thresholds\u00b6","text":""},{"location":"python/Part02_filtering_sample3.html#doublet-filtering","title":"Doublet filtering\u00b6","text":""},{"location":"python/Part02_filtering_sample3.html#evaluation-of-filtering","title":"Evaluation of filtering\u00b6","text":""},{"location":"python/Part03_normalize_and_integrate.html","title":"Normalize and Integrate","text":"<p>*Import packages*</p> In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport anndata as ad\n\nplt.rcParams['figure.figsize']=(6,6) #rescale figures\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn import anndata as ad  plt.rcParams['figure.figsize']=(6,6) #rescale figures <p>We will need to use a couple of packages that are only developed in R. To do this we will use the package <code>rpy2</code>, that allows <code>python-R</code> interaction. Below we will load the package and some settings (You do not need to look at them, but they can be useful as reference for your own future coding).</p> In\u00a0[2]: Copied! <pre>import rpy2.rinterface_lib.callbacks\nimport logging\n\nfrom rpy2.robjects import pandas2ri\nimport anndata2ri\n\n# Ignore R warning messages\n#Note: this can be commented out to get more verbose R output\nrpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n\n# Automatically convert rpy2 outputs to pandas dataframes\npandas2ri.activate()\nanndata2ri.activate()\n\n#import os\n#os.environ['R_HOME'] = '../../../scrna-environment/lib/R/' #path to your R installation\n\n%load_ext rpy2.ipython\n</pre> import rpy2.rinterface_lib.callbacks import logging  from rpy2.robjects import pandas2ri import anndata2ri  # Ignore R warning messages #Note: this can be commented out to get more verbose R output rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)  # Automatically convert rpy2 outputs to pandas dataframes pandas2ri.activate() anndata2ri.activate()  #import os #os.environ['R_HOME'] = '../../../scrna-environment/lib/R/' #path to your R installation  %load_ext rpy2.ipython In\u00a0[3]: Copied! <pre>%%R\n.libPaths( c( \"../../../../sandbox_scRNA_testAndFeedback/scrna-environment/lib/R/library/\" ) )\n</pre> %%R .libPaths( c( \"../../../../sandbox_scRNA_testAndFeedback/scrna-environment/lib/R/library/\" ) ) <p>Load the two separated datasets</p> In\u00a0[4]: Copied! <pre>sample_2 = sc.read('../../Data/notebooks_data/sample_2.filt.h5ad')\nsample_3 = sc.read('../../Data/notebooks_data/sample_3.filt.h5ad')\n</pre> sample_2 = sc.read('../../Data/notebooks_data/sample_2.filt.h5ad') sample_3 = sc.read('../../Data/notebooks_data/sample_3.filt.h5ad') In\u00a0[5]: Copied! <pre>batch_names = ['SAM_2','SAM_3'] #choose names for samples\nsample = ad.AnnData.concatenate(sample_2, sample_3) #concatenate\nsample.rename_categories(key='batch', categories=batch_names) #apply sample names\nscv.utils.cleanup(sample, clean='var') #remove duplicated gene quantites\n</pre> batch_names = ['SAM_2','SAM_3'] #choose names for samples sample = ad.AnnData.concatenate(sample_2, sample_3) #concatenate sample.rename_categories(key='batch', categories=batch_names) #apply sample names scv.utils.cleanup(sample, clean='var') #remove duplicated gene quantites <p>We unload the old separated samples to free memory</p> In\u00a0[6]: Copied! <pre>del sample_2, sample_3\n</pre> del sample_2, sample_3 <p>The new sample looks like this. It has the well-known observations from the previous coding sessions and has all the stored matrices. We have now more than 6000 cells.</p> In\u00a0[7]: Copied! <pre>sample\n</pre> sample Out[7]: <pre>AnnData object with n_obs \u00d7 n_vars = 6431 \u00d7 22825\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'perc_mito', 'n_counts', 'n_genes', 'doublet_score', 'predicted_doublet', 'batch'\n    layers: 'umi_log', 'umi_raw', 'umi_tpm'</pre> <p>For each sample, we filtered away genes shown in less than 10 cells. Now we have two samples, so it is good to filter away genes shown in less than 20 cells (to be sure that a sample does not have genes that are absent in the other sample)</p> In\u00a0[8]: Copied! <pre>sc.preprocessing.filter_genes(sample, min_cells=20)\n</pre> sc.preprocessing.filter_genes(sample, min_cells=20) <p>This should eliminate a few more genes</p> In\u00a0[9]: Copied! <pre>sample\n</pre> sample Out[9]: <pre>AnnData object with n_obs \u00d7 n_vars = 6431 \u00d7 22790\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'perc_mito', 'n_counts', 'n_genes', 'doublet_score', 'predicted_doublet', 'batch'\n    var: 'n_cells'\n    layers: 'umi_log', 'umi_raw', 'umi_tpm'</pre> <p>At the moment, <code>sctransform</code> is implemented only in <code>R</code>. Using <code>R</code> integrated in <code>python</code> is pretty easy. First you need to define the variables needed in <code>R</code>: here we have the raw matrix, the names of the genes, and the sample identifier.</p> In\u00a0[10]: Copied! <pre>rawMatrix = np.array( sample.layers['umi_raw'].T.copy())\ngenes_name = sample.var_names\ncells_info = sample.obs[ [\"batch\"] ].copy()\n</pre> rawMatrix = np.array( sample.layers['umi_raw'].T.copy()) genes_name = sample.var_names cells_info = sample.obs[ [\"batch\"] ].copy() <p>We now use a so-called cell magic command. This is a command preceded by a double <code>%%</code> symbol. In our case we write <code>%%R</code> to declare we use the <code>R</code> language. We import the variables in <code>R</code> by using <code>-i</code> for each variable. The following four commands are all executed in <code>R</code>, and will exist into an underlying <code>R</code> environment that we do not see explicitly.</p> In\u00a0[11]: Copied! <pre>%%R -i cells_info -i rawMatrix -i genes_name\nlibrary(scater)\ncell_df &lt;- DataFrame(data = cells_info)\ncolnames(rawMatrix) &lt;- rownames(cell_df) #cell names\nrownames(rawMatrix) &lt;- genes_name #gene names\n</pre> %%R -i cells_info -i rawMatrix -i genes_name library(scater) cell_df &lt;- DataFrame(data = cells_info) colnames(rawMatrix) &lt;- rownames(cell_df) #cell names rownames(rawMatrix) &lt;- genes_name #gene names In\u00a0[12]: Copied! <pre>del rawMatrix, genes_name, cells_info\n</pre> del rawMatrix, genes_name, cells_info <p>Now we setup a few things to make the normalization algorithm use multiple cores. This is done with the package <code>future</code> in <code>R</code>. Here we declare we want to use multiple cores and choose how many in the option <code>workers</code> (Here we set up to 32, but change according to your machine). Then we assign how much memory we can use in total with the option <code>future.globals.maxSize</code> (here we choose up to 50 GigaBytes, where <code>1024^3</code> denotes that we are talking about GigaBytes. Limit this value according your chosen machine).</p> In\u00a0[17]: Copied! <pre>%%R\nlibrary(sctransform)\nlibrary(future)\nfuture::plan(strategy = 'multicore', workers = 32)\noptions(future.globals.maxSize = 50 * 1024 ^ 3)\n</pre> %%R library(sctransform) library(future) future::plan(strategy = 'multicore', workers = 32) options(future.globals.maxSize = 50 * 1024 ^ 3) <p>Now we run the normalization. There are a few parameters to be assigned. You can especially think about tweaking <code>n_genes</code>: it will base the normalization on a number of most significant genes. The more genes you choose, the more time and memory you require for the normalization. usually somewhere between 2000 and 5000 is a typical value.</p> In\u00a0[18]: Copied! <pre>%%R\nvst_out=vst( as.matrix(rawMatrix), #data matrix\n            cell_attr=cell_df, #dataframe containing batch variable\n            n_genes=3000, #most variable genes in your data\n            batch_var='data.batch', #name of the batch variable\n            method='qpoisson', #type of statistical model. use \"poisson\" for more precision but much slower execution\n            show_progress=TRUE, #show progress bars\n            return_corrected_umi=TRUE) #return corrected umi count matrix\n</pre> %%R vst_out=vst( as.matrix(rawMatrix), #data matrix             cell_attr=cell_df, #dataframe containing batch variable             n_genes=3000, #most variable genes in your data             batch_var='data.batch', #name of the batch variable             method='qpoisson', #type of statistical model. use \"poisson\" for more precision but much slower execution             show_progress=TRUE, #show progress bars             return_corrected_umi=TRUE) #return corrected umi count matrix <pre>  |======================================================================| 100%\n  |======================================================================| 100%\n  |======================================================================| 100%\n</pre> <p>The algorithm returns normalized data, matrix of umi counts with adjusted value according to the statistical model, and most significant genes. We assign them to variables in <code>R</code> and export them into <code>python</code> using the commands <code>-o</code></p> In\u00a0[19]: Copied! <pre>%%R -o new_matrix -o sct_genes -o all_genes -o umi_matrix\nnew_matrix=vst_out$y #normalized matrix\nsct_genes = rownames(vst_out$model_pars) #most variable genes\nall_genes = rownames(new_matrix) #vector of all genes to check if any have been filtered out\numi_matrix=vst_out$umi_corrected #umi matrix\n</pre> %%R -o new_matrix -o sct_genes -o all_genes -o umi_matrix new_matrix=vst_out$y #normalized matrix sct_genes = rownames(vst_out$model_pars) #most variable genes all_genes = rownames(new_matrix) #vector of all genes to check if any have been filtered out umi_matrix=vst_out$umi_corrected #umi matrix <p>We save the most variable genes into our dataset</p> In\u00a0[20]: Copied! <pre>sct_genes = list(sct_genes)\nsample.var['highly_variable'] = [i in sct_genes for i in sample.var_names]\n</pre> sct_genes = list(sct_genes) sample.var['highly_variable'] = [i in sct_genes for i in sample.var_names] <p>We check that the genes actually match with the results from <code>R</code></p> In\u00a0[21]: Copied! <pre>sample = sample[:,list(all_genes)].copy()\n</pre> sample = sample[:,list(all_genes)].copy() <p>We store in <code>.layers</code> the normalized and UMI matrix</p> In\u00a0[22]: Copied! <pre>sample.layers['norm_sct'] = np.transpose( new_matrix )\nsample.layers['umi_sct'] = np.transpose( umi_matrix )\n</pre> sample.layers['norm_sct'] = np.transpose( new_matrix ) sample.layers['umi_sct'] = np.transpose( umi_matrix ) <p>Let's now calculate and plot the PCA using normalized data. We can see that the PCA of the two samples is not perfectly overlapping. This is because PCA does not model in any way the presence of two different samples. Even though this has been taken into account during normalization, it is not enough to integrate the datasets together.</p> In\u00a0[23]: Copied! <pre>sample.X = sample.layers['norm_sct'].copy() #use normalized data in .X\nsc.pp.scale(sample) #standardize\nsc.preprocessing.pca(sample, svd_solver='arpack', random_state=12345) #do PCA\n</pre> sample.X = sample.layers['norm_sct'].copy() #use normalized data in .X sc.pp.scale(sample) #standardize sc.preprocessing.pca(sample, svd_solver='arpack', random_state=12345) #do PCA In\u00a0[24]: Copied! <pre>sc.pl.pca(sample, color=['batch','total_counts','PRM3'])\n</pre> sc.pl.pca(sample, color=['batch','total_counts','PRM3']) <p>One can keep the PCA above and compensate for differences between batches when calculating the neighborhood of each cell (the distance between cells that are most similar). This is done by taking into account the presence of multiple samples with the package <code>bbknn</code> (in the filtering notebooks we used the command <code>sc.pp.neighbors</code>)</p> In\u00a0[25]: Copied! <pre>import bbknn as bbknn\nbbknn.bbknn(sample)\n</pre> import bbknn as bbknn bbknn.bbknn(sample)  <p>We can visualize the result with an UMAP plot. We can see how the samples are now overlapping in the plot.</p> In\u00a0[26]: Copied! <pre>sc.tools.umap(sample, random_state=54321)\n</pre> sc.tools.umap(sample, random_state=54321) In\u00a0[27]: Copied! <pre>sc.plotting.umap(sample, color=['batch','total_counts','PRM3'])\n</pre> sc.plotting.umap(sample, color=['batch','total_counts','PRM3']) <p>How well went the integration? We can see if, for every point in the UMAP plot, the other most similar points are a mix from the various samples, or if they are dominated by only one of them. The <code>R</code> package <code>kbet</code> does this test for each datapoint. The test is rejected if a datapoint is in an area where cells from all samples are not mixed/overlapping. A perfect integration has around zero rejection (left side of the plot). In reality, this does not happen, but a good integration stays in general below a 50% rejection (right side of the plot).</p> In\u00a0[28]: Copied! <pre>data = np.array( sample.obsm['X_umap'] )\nbatch = np.array( sample.obs['batch'] )\n</pre> data = np.array( sample.obsm['X_umap'] ) batch = np.array( sample.obs['batch'] ) In\u00a0[29]: Copied! <pre>%%R -i batch -i data\n\nlibrary(kBET)\nlibrary(ggplot2)\n\nbatch.estimate &lt;- kBET( data, batch, plot=TRUE, k0=10 )\nplot.data &lt;- data.frame(class=rep(c('observed', 'expected'), \n                                  each=length(batch.estimate$stats$kBET.observed)), \n                        data =  c(batch.estimate$stats$kBET.observed,\n                                  batch.estimate$stats$kBET.expected))\n</pre> %%R -i batch -i data  library(kBET) library(ggplot2)  batch.estimate &lt;- kBET( data, batch, plot=TRUE, k0=10 ) plot.data &lt;- data.frame(class=rep(c('observed', 'expected'),                                    each=length(batch.estimate$stats$kBET.observed)),                          data =  c(batch.estimate$stats$kBET.observed,                                   batch.estimate$stats$kBET.expected)) In\u00a0[30]: Copied! <pre>sample.write('../../Data/notebooks_data/sample_123.filt.norm.h5ad')\n</pre> sample.write('../../Data/notebooks_data/sample_123.filt.norm.h5ad') In\u00a0[31]: Copied! <pre>sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.h5ad')\n</pre> sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.h5ad') <pre>WARNING: Your filename has more than two extensions: ['.filt', '.norm', '.h5ad'].\nOnly considering the two last: ['.norm', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.h5ad'].\nOnly considering the two last: ['.norm', '.h5ad'].\n</pre> <p>We first need to model the batch variables to be compatible in <code>glmpca</code></p> In\u00a0[32]: Copied! <pre>import sklearn.preprocessing\nimport numpy as np\nlabel_binarizer = sklearn.preprocessing.LabelBinarizer()\nlabel_binarizer.fit(sample.obs['batch'])\nbatch_onehot = label_binarizer.transform(sample.obs['batch'])\n</pre> import sklearn.preprocessing import numpy as np label_binarizer = sklearn.preprocessing.LabelBinarizer() label_binarizer.fit(sample.obs['batch']) batch_onehot = label_binarizer.transform(sample.obs['batch']) <p>We run <code>glmpca</code>. If the algorithm fails, you need to change the parameter <code>penalty</code> of the command <code>glmpca.glmpca</code>. Always start from 1 and increase by ten if errors occur. In case you keep getting error, then you need to resort to the standard PCA used before. That happens when some transcripts have a very noisy distribution, for example because of bad data filtering in the early steps of the analysis. In our case we made the algorithm work with <code>penalty=10</code>. Note in the second line how we subset the dataset to use only the highly variable genes. Also, we use the matrix of corrected UMI counts as a base for the PCA (third line)</p> In\u00a0[33]: Copied! <pre>ctl = {\"maxIter\":30, \"eps\":1e-3, \"optimizeTheta\":True}\nsample_glmpca = sample[:,sample.var['highly_variable']].copy()\nY = sample_glmpca.layers['umi_sct'].T.todense().copy()\nY = np.asarray(Y)\nfrom glmpca import glmpca\nprint(\"calculating\")\nres = glmpca.glmpca(Y, 15, penalty=10, X=batch_onehot, verbose=True, ctl=ctl)\nfactors = res[\"factors\"]\nsample_glmpca.obsm['X_pca']=factors\n</pre> ctl = {\"maxIter\":30, \"eps\":1e-3, \"optimizeTheta\":True} sample_glmpca = sample[:,sample.var['highly_variable']].copy() Y = sample_glmpca.layers['umi_sct'].T.todense().copy() Y = np.asarray(Y) from glmpca import glmpca print(\"calculating\") res = glmpca.glmpca(Y, 15, penalty=10, X=batch_onehot, verbose=True, ctl=ctl) factors = res[\"factors\"] sample_glmpca.obsm['X_pca']=factors <pre>calculating\nIteration: 0 | deviance=3.8470E+7\nIteration: 1 | deviance=3.7307E+7\nIteration: 2 | deviance=1.7101E+7\nIteration: 3 | deviance=1.3117E+7\nIteration: 4 | deviance=1.2319E+7\nIteration: 5 | deviance=1.1993E+7\nIteration: 6 | deviance=1.1824E+7\nIteration: 7 | deviance=1.1723E+7\nIteration: 8 | deviance=1.1654E+7\nIteration: 9 | deviance=1.1603E+7\nIteration: 10 | deviance=1.1565E+7\nIteration: 11 | deviance=1.1534E+7\nIteration: 12 | deviance=1.1509E+7\nIteration: 13 | deviance=1.1488E+7\nIteration: 14 | deviance=1.1470E+7\nIteration: 15 | deviance=1.1455E+7\nIteration: 16 | deviance=1.1441E+7\nIteration: 17 | deviance=1.1429E+7\n</pre> <p>Plot the new PCA on the subsetted dataset. The integration looks of higher quality than before.</p> In\u00a0[38]: Copied! <pre>sc.pl.pca(sample_glmpca, color=['batch'])\n</pre> sc.pl.pca(sample_glmpca, color=['batch']) <p>Assign the new PCA to the full dataset</p> In\u00a0[39]: Copied! <pre>sample.obsm['X_pca'] = sample_glmpca.obsm['X_pca'].copy()\n</pre> sample.obsm['X_pca'] = sample_glmpca.obsm['X_pca'].copy() <p>Recalculate and plot the UMAP</p> In\u00a0[40]: Copied! <pre>import bbknn as bbknn\nbbknn.bbknn(sample)\nsc.tools.umap(sample, random_state=54321)\nsc.plotting.umap(sample, color=['batch','total_counts','PRM3'])\n</pre> import bbknn as bbknn bbknn.bbknn(sample) sc.tools.umap(sample, random_state=54321) sc.plotting.umap(sample, color=['batch','total_counts','PRM3']) In\u00a0[41]: Copied! <pre>sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.h5ad')\n</pre> sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.h5ad') <p>We can double check if the integration is still acceptable. At least in terms of overlapping, the previous version was slightly better, but in general it is a good practice to use the <code>glmpca</code> approach if this does not fail furing execution</p> In\u00a0[42]: Copied! <pre>data = np.array( sample.obsm['X_umap'] )\nbatch = np.array( sample.obs['batch'] )\n</pre> data = np.array( sample.obsm['X_umap'] ) batch = np.array( sample.obs['batch'] ) In\u00a0[43]: Copied! <pre>%%R -i batch -i data\n\nlibrary(kBET)\nlibrary(ggplot2)\n\nbatch.estimate &lt;- kBET( data, batch, plot=TRUE, k0=10 )\nplot.data &lt;- data.frame(class=rep(c('observed', 'expected'), \n                                  each=length(batch.estimate$stats$kBET.observed)), \n                        data =  c(batch.estimate$stats$kBET.observed,\n                                  batch.estimate$stats$kBET.expected))\n</pre> %%R -i batch -i data  library(kBET) library(ggplot2)  batch.estimate &lt;- kBET( data, batch, plot=TRUE, k0=10 ) plot.data &lt;- data.frame(class=rep(c('observed', 'expected'),                                    each=length(batch.estimate$stats$kBET.observed)),                          data =  c(batch.estimate$stats$kBET.observed,                                   batch.estimate$stats$kBET.expected)) <p>How would a missing integration step result?</p> <p>We can see the evaluation of an integration without using the proper normalization and integration steps with PCA and neighborhoods calculations.</p> In\u00a0[44]: Copied! <pre>sample.X = sample.layers['umi_raw'].copy()\nsc.pp.log1p(sample)\nsc.pp.normalize_total(sample)\nsc.pp.scale(sample)\nsc.pp.pca(sample, svd_solver='arpack', random_state=12345)\nsc.pp.neighbors(sample, random_state=12345)\nsc.tools.umap(sample, random_state=54321, n_components=2)\n</pre> sample.X = sample.layers['umi_raw'].copy() sc.pp.log1p(sample) sc.pp.normalize_total(sample) sc.pp.scale(sample) sc.pp.pca(sample, svd_solver='arpack', random_state=12345) sc.pp.neighbors(sample, random_state=12345) sc.tools.umap(sample, random_state=54321, n_components=2) <p>The UMAP plot speaks pretty clearly</p> In\u00a0[45]: Copied! <pre>sc.pl.umap(sample, color=['batch'])\n</pre> sc.pl.umap(sample, color=['batch']) <p>The overlapping test gets naturally a high rejection</p> In\u00a0[46]: Copied! <pre>non_integrated_data = np.array( sample.obsm['X_umap'] )\nbatch = np.array( sample.obs['batch'] )\n</pre> non_integrated_data = np.array( sample.obsm['X_umap'] ) batch = np.array( sample.obs['batch'] ) In\u00a0[47]: Copied! <pre>%%R -i batch -i non_integrated_data\n\nlibrary(kBET)\nlibrary(ggplot2)\n\nbatch.estimate &lt;- kBET( non_integrated_data, batch, plot=TRUE, k0=10 )\nplot.data &lt;- data.frame(class=rep(c('observed', 'expected'), \n                                  each=length(batch.estimate$stats$kBET.observed)), \n                        data =  c(batch.estimate$stats$kBET.observed,\n                                  batch.estimate$stats$kBET.expected))\n</pre> %%R -i batch -i non_integrated_data  library(kBET) library(ggplot2)  batch.estimate &lt;- kBET( non_integrated_data, batch, plot=TRUE, k0=10 ) plot.data &lt;- data.frame(class=rep(c('observed', 'expected'),                                    each=length(batch.estimate$stats$kBET.observed)),                          data =  c(batch.estimate$stats$kBET.observed,                                   batch.estimate$stats$kBET.expected)) <p>This notebook completes the integration of the datasets. We have both used the standard PCA and then a version of the PCA that can model transcript distribution and takes batches into account. Finally we have calculated cell neighborhoods with <code>bbknn</code>: this allows us to further remove the technical differences between batches. We used a statistical test to check that the integration has been successful, and compared it with the test performed on a dataset without integration steps.</p>"},{"location":"python/Part03_normalize_and_integrate.html#normalize-and-integrate","title":"Normalize and Integrate\u00b6","text":"<p>Motivation:</p> <p>Biologically similar cells are not necessarily directly comparable in a dataset because of different technical biases, amongst many the different percentage of captured transcripts (capture efficiency), the presence of technical replicates, the presence of noisy transcripts. The capture efficiency can be influenced by many factors, i.e. the different transcript tags leading to different capture efficiency, the type of protocol used in the laboratory, the amount of PCR performed on different transcripts.</p> <p>To avoid these differences, a normalization approach is needed. Normalization is one of the main topics of scRNAseq data preprocessing, and many advanced techniques takes into account the statistical distribution of counts and the presence of technical/biological features of interest.</p> <p>The most standard approach is the TMP (Transcript Per Million) normalization followed by logarithmization and standardization. We have applied this technique to have a double-check on our data filtering.</p> <p>As a rule of thumb, TPM+log+standardization is no longer consider a very good normalization technique, especially when you are integrating multiple datasets together. Instead, it is suggested to use more advanced methods for considering technical and biological covariates as part of a statistical model for the transcripts. One of the current state-of-the-art method is scTransform. We will apply it in this notebook.</p> <p>Learning objectives:</p> <ul> <li>Perform advanced normalization of multiple datasets</li> <li>Understand and apply tools for multiple datasets integration</li> <li>Evaluate the quality of data integration</li> </ul> <p>Execution time: 40 minutes if using at least 2 cores (that is, at least 16 virtual-CPUs). Expect longer time with slower hardware, and a RAM usage of some 40-100GB.</p>"},{"location":"python/Part03_normalize_and_integrate.html#concatenation-of-datasets","title":"Concatenation of datasets\u00b6","text":"<p>Here we concatenate the two datasets. We establish the name of each sample. Each cell will be assigned the correct name, that will be stored in <code>sample.obs['batch']</code>. We clean also all the variables (quantities calculated on genes), because those will be duplicated from each dataset, while observations (quantities calculated on cells) will be concatenated together without duplication.</p>"},{"location":"python/Part03_normalize_and_integrate.html#normalization","title":"Normalization\u00b6","text":"<p>We will normalize the dataset by using a more advanced tool. While standard normalization seemed to work fine previously, the result is still very much influenced by technical factors, such as by</p> <ul> <li>how cell transcripts have been captured efficiently prior to sequencing</li> <li>difference in PCR amplification</li> </ul> <p>We will use a tool called <code>sctransform</code>: this does a statistical model for the transcript counts and does a regression to remove technical factors connected to the amount of transcripts and stabilize the variance of the data. In this way we will have a normalized dataset that is better explained by biological processes rather than technical variables.</p>"},{"location":"python/Part03_normalize_and_integrate.html#data-integration","title":"Data integration\u00b6","text":""},{"location":"python/Part03_normalize_and_integrate.html#standard-pca-framework","title":"Standard PCA framework\u00b6","text":""},{"location":"python/Part03_normalize_and_integrate.html#generalized-pca-model","title":"Generalized PCA model\u00b6","text":"<p>A better way of doing a PCA for multiple samples is to use a model that takes into account the various batches. We can do this with the package <code>glmpca</code>. The execution time can be a bit longer (especially if you choose many variable genes in the normalization), but the resulting PCA is usually better thanks to a statistical model better reflectiong the transcript distribution.</p>"},{"location":"python/Part03_normalize_and_integrate.html#evaluate-the-integration","title":"Evaluate the integration\u00b6","text":""},{"location":"python/Part03_normalize_and_integrate.html#wrapping-up","title":"Wrapping up\u00b6","text":""},{"location":"python/Part04_clustering.html","title":"Cell clustering and Differential Expression (DE)","text":"<p>*Import packages*</p> In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport anndata as ad\n\nplt.rcParams['figure.figsize']=(6,6) #rescale figures\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn import anndata as ad  plt.rcParams['figure.figsize']=(6,6) #rescale figures <p>Read the data integrated with <code>glmpca</code> and <code>bbknn</code></p> In\u00a0[2]: Copied! <pre>sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.h5ad')\n</pre> sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.h5ad') <pre>WARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.h5ad'].\nOnly considering the two last: ['.red', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.h5ad'].\nOnly considering the two last: ['.red', '.h5ad'].\n</pre> In\u00a0[3]: Copied! <pre>markers = dict() #make an empty dictionary\n### SPERMATOCYTOGENESIS\nmarkers['SpermatogoniaA'] = ['ID4']\nmarkers['SpermatogoniaB'] = ['MKI67','DMRT1','STRA8'] \nmarkers['SpermatocytesI'] = ['MEIOB','SYCP1','TEX101']\nmarkers['SpermatocytesII'] = ['PIWIL1','SPATA16','CLGN']\n### SPERMIOGENESIS\nmarkers['Round.Spt'] = ['SPATA9','SPAM1'] #Round spermatids\nmarkers['Elong.Spt'] = ['PRM1','PRM2','PRM3','AKAP4'] #Elongated spermatids\n### SOMATIC CELLS\nmarkers['Sertoli'] = ['VIM','CTSL']\nmarkers['Macroph'] = ['CD14']\nmarkers['Leydig'] = ['CFD']\nmarkers['Endothelial'] = ['CD34']\nmarkers['Myoid'] = ['ACTA2']\n\n#remove markers missing in the dataset\nfor i in markers:\n    markers[i] = np.intersect1d(markers[i], sample.var_names)\n</pre> markers = dict() #make an empty dictionary ### SPERMATOCYTOGENESIS markers['SpermatogoniaA'] = ['ID4'] markers['SpermatogoniaB'] = ['MKI67','DMRT1','STRA8']  markers['SpermatocytesI'] = ['MEIOB','SYCP1','TEX101'] markers['SpermatocytesII'] = ['PIWIL1','SPATA16','CLGN'] ### SPERMIOGENESIS markers['Round.Spt'] = ['SPATA9','SPAM1'] #Round spermatids markers['Elong.Spt'] = ['PRM1','PRM2','PRM3','AKAP4'] #Elongated spermatids ### SOMATIC CELLS markers['Sertoli'] = ['VIM','CTSL'] markers['Macroph'] = ['CD14'] markers['Leydig'] = ['CFD'] markers['Endothelial'] = ['CD34'] markers['Myoid'] = ['ACTA2']  #remove markers missing in the dataset for i in markers:     markers[i] = np.intersect1d(markers[i], sample.var_names) <p>We can see how many markers easily identify groups of cells by plotting the expression</p> In\u00a0[4]: Copied! <pre>sc.plotting.umap(sample, color=markers['SpermatogoniaA'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['SpermatogoniaA'], vmin=-1, vmax=3, s=30) In\u00a0[5]: Copied! <pre>sc.plotting.umap(sample, color=markers['SpermatogoniaB'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['SpermatogoniaB'], vmin=-1, vmax=3, s=30) In\u00a0[6]: Copied! <pre>sc.plotting.umap(sample, color=markers['SpermatocytesI'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['SpermatocytesI'], vmin=-1, vmax=3, s=30) In\u00a0[7]: Copied! <pre>sc.plotting.umap(sample, color=markers['SpermatocytesII'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['SpermatocytesII'], vmin=-1, vmax=3, s=30) In\u00a0[8]: Copied! <pre>sc.plotting.umap(sample, color=markers['Round.Spt'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Round.Spt'], vmin=-1, vmax=3, s=30) In\u00a0[9]: Copied! <pre>sc.plotting.umap(sample, color=markers['Elong.Spt'], vmin=0, vmax=5, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Elong.Spt'], vmin=0, vmax=5, s=30) <p>Sertoli are often not possible to identify. They are big in size, meaning they are often not isolated successfully. Many of their markers are in common with other somatic cells. Also, their function as nurse cells for germ cells of the testis means that their marker genes are also expressed. We can see that CTSL is expressed in some germ cells, but not in other clusters, while VIM is expressed in a likely somatic cluster (but it is common to other somatic cell types)</p> In\u00a0[10]: Copied! <pre>sc.plotting.umap(sample, color=markers['Sertoli'], vmin=-1, vmax=5, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Sertoli'], vmin=-1, vmax=5, s=30) <p>Macrophage cells seem to be absent</p> In\u00a0[11]: Copied! <pre>sc.plotting.umap(sample, color=markers['Macroph'], vmin=-1, vmax=5, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Macroph'], vmin=-1, vmax=5, s=30) <p>There is a little endothelial cluster</p> In\u00a0[12]: Copied! <pre>sc.plotting.umap(sample, color=markers['Endothelial'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Endothelial'], vmin=-1, vmax=3, s=30) <p>and also a myoid cluster</p> In\u00a0[13]: Copied! <pre>sc.plotting.umap(sample, color=markers['Myoid'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Myoid'], vmin=-1, vmax=3, s=30) <p>Leydig cells are likely to be missing as well</p> In\u00a0[14]: Copied! <pre>sc.plotting.umap(sample, color=markers['Leydig'], vmin=-1, vmax=3, s=30)\n</pre> sc.plotting.umap(sample, color=markers['Leydig'], vmin=-1, vmax=3, s=30) <p>Now we create some clusters, and try to get the same division we saw by plotting markers. We can tune the number of clusters by changing the <code>resolution</code> parameter. We will be able to give the same name to more clusters, so it is fine to create a fine-grained clustering.</p> In\u00a0[15]: Copied! <pre>sc.tl.leiden(sample, resolution=.3, random_state=12345)\n</pre> sc.tl.leiden(sample, resolution=.3, random_state=12345) In\u00a0[16]: Copied! <pre>sc.plotting.umap(sample, color=['leiden'], legend_loc='on data', legend_fontsize=20)\n</pre> sc.plotting.umap(sample, color=['leiden'], legend_loc='on data', legend_fontsize=20) <p>Write the names in the dictionary <code>new_names</code>. You should be able to give a name for each cell type. Below is an example, but the names are not in the right position. If there is more than one cluster with same cell type, just write the name followed by a dot <code>.</code> and a number. For example, by writing for example <code>.1</code> and <code>.2</code> at the end of the names. We will remove the numbers afterwords.</p> In\u00a0[17]: Copied! <pre>clusters = pd.Categorical(sample.obs['leiden'])\n</pre> clusters = pd.Categorical(sample.obs['leiden']) In\u00a0[20]: Copied! <pre>new_names = {\n    '0':'SpermatocitesII.2',\n    '1':'RoundSpermatids.3',\n    '2':'RoundSpermatids.4',\n    '3':'SpermatogoniaA',\n    '4':'SpermatocitesII.1',\n    '5':'ElongSpermatids',\n    '6':'RoundSpermatids.1',\n    '7':'RoundSpermatids.2',\n    '8':'SpermatocitesI',\n    '9':'SpermatogoniaB',\n    '10':'Somatic',\n}\n</pre> new_names = {     '0':'SpermatocitesII.2',     '1':'RoundSpermatids.3',     '2':'RoundSpermatids.4',     '3':'SpermatogoniaA',     '4':'SpermatocitesII.1',     '5':'ElongSpermatids',     '6':'RoundSpermatids.1',     '7':'RoundSpermatids.2',     '8':'SpermatocitesI',     '9':'SpermatogoniaB',     '10':'Somatic', } <p>we apply the new names</p> In\u00a0[21]: Copied! <pre>clusters=clusters.rename_categories(new_names)\n</pre> clusters=clusters.rename_categories(new_names) <p>we remove the numbers from same cell types</p> In\u00a0[22]: Copied! <pre>cluster_array = np.array(clusters)\nsplit_array = [ i.split('.')[0] for i in cluster_array ]\nclusters = pd.Categorical(split_array)\n</pre> cluster_array = np.array(clusters) split_array = [ i.split('.')[0] for i in cluster_array ] clusters = pd.Categorical(split_array) <p>save the clusters in the sample and plot the new ones</p> In\u00a0[23]: Copied! <pre>sample.obs['clusters']=clusters.copy()\n</pre> sample.obs['clusters']=clusters.copy() In\u00a0[24]: Copied! <pre>sc.plotting.umap(sample, color=['clusters'], legend_loc='on data')\n</pre> sc.plotting.umap(sample, color=['clusters'], legend_loc='on data') <p>We can look at markers in a heatmap or a dotplot</p> In\u00a0[25]: Copied! <pre>sc.pl.heatmap(sample, \n              groupby='clusters', \n              var_names=markers,\n              vmin=0, vmax=5, layer='norm_sct')\n</pre> sc.pl.heatmap(sample,                groupby='clusters',                var_names=markers,               vmin=0, vmax=5, layer='norm_sct') In\u00a0[26]: Copied! <pre>sc.pl.dotplot(sample, \n              groupby='clusters', \n              var_names=markers,\n              vmin=0, vmax=3, layer='norm_sct')\n</pre> sc.pl.dotplot(sample,                groupby='clusters',                var_names=markers,               vmin=0, vmax=3, layer='norm_sct') In\u00a0[27]: Copied! <pre>sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.clst.h5ad')\n</pre> sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.clst.h5ad') <p>We can do differential expression (DE) analysis to double check which genes are differentially expressed in each cluster. A gene is differentially expressed in a cluster when its expression in the cells of that cluster is statistically bigger than in all other cells. This is verified through a statistical test.</p> <p>Together with the gene names we also get p-values from the test, and a factor (<code>log-fold change</code>) telling the magnitude of how much the expression is larger than in other cells.</p> In\u00a0[28]: Copied! <pre>sample.X = sample.layers['umi_sct'].copy()\nsc.pp.log1p(sample)\n</pre> sample.X = sample.layers['umi_sct'].copy() sc.pp.log1p(sample) <p>Apply the differential expression tool on the clusters for the top ten genes of each cluster. Save the results in <code>.uns[DE_clusters]</code></p> In\u00a0[29]: Copied! <pre>sc.tl.rank_genes_groups(sample, groupby='clusters', key_added='DE_clusters', \n                        use_raw=False, n_genes=10, method='wilcoxon')\n</pre> sc.tl.rank_genes_groups(sample, groupby='clusters', key_added='DE_clusters',                          use_raw=False, n_genes=10, method='wilcoxon') <p>Access the list of names</p> In\u00a0[30]: Copied! <pre>pd.DataFrame(sample.uns['DE_clusters']['names'])\n</pre> pd.DataFrame(sample.uns['DE_clusters']['names']) Out[30]: ElongSpermatids RoundSpermatids Somatic SpermatocitesI SpermatocitesII SpermatogoniaA SpermatogoniaB 0 PRM1 ERICH2 RPL10 C5orf58 AL133499.1 RPS12 HMGB1 1 PRM2 FNDC11 VIM LY6K PPP3R2 CCNI PTMA 2 TNP1 LYZL2 TMSB4X TEX101 ZMYND10 DNAJB6 SMC3 3 LINC01921 ACRV1 EEF1A1 TDRG1 LYAR RPS19 NASP 4 LELP1 SPACA3 MYL6 TPTE COPRS RPSA VCX 5 GLUL C1orf185 RPL41 HORMAD1 SPINK2 FKBP8 CIRBP 6 ODF2 ACTRT3 B2M STMN1 MRPL34 RPL18A TKTL1 7 C10orf62 FAM209B RPS8 ARL6IP1 LDHC ZNF428 NCL 8 AC010255.3 LYZL1 CD63 TMEM147 CAVIN3 EEF1B2 SDF2L1 9 GAPDHS TEX38 CALD1 C5orf47 TMEM225B HNRNPDL YWHAE <p>Access the table including p-values (with suffix <code>_P</code> in each column) and log-fold change (with suffix <code>_L</code> in each column)</p> In\u00a0[31]: Copied! <pre>result = sample.uns['DE_clusters']\ngroups = result['names'].dtype.names\nX = pd.DataFrame(\n    {group + '_' + key[:1].upper(): result[key][group]\n    for group in groups for key in ['names', 'pvals_adj','logfoldchanges']})\nX\n</pre> result = sample.uns['DE_clusters'] groups = result['names'].dtype.names X = pd.DataFrame(     {group + '_' + key[:1].upper(): result[key][group]     for group in groups for key in ['names', 'pvals_adj','logfoldchanges']}) X Out[31]: ElongSpermatids_N ElongSpermatids_P ElongSpermatids_L RoundSpermatids_N RoundSpermatids_P RoundSpermatids_L Somatic_N Somatic_P Somatic_L SpermatocitesI_N ... SpermatocitesI_L SpermatocitesII_N SpermatocitesII_P SpermatocitesII_L SpermatogoniaA_N SpermatogoniaA_P SpermatogoniaA_L SpermatogoniaB_N SpermatogoniaB_P SpermatogoniaB_L 0 PRM1 0.0 5.479261 ERICH2 0.0 3.754093 RPL10 1.879383e-180 6.061464 C5orf58 ... 4.534876 AL133499.1 0.0 3.951918 RPS12 0.0 3.358225 HMGB1 5.695630e-232 4.110899 1 PRM2 0.0 5.263555 FNDC11 0.0 3.381924 VIM 6.626537e-180 5.904238 LY6K ... 5.939358 PPP3R2 0.0 2.925693 CCNI 0.0 5.022986 PTMA 2.289016e-230 4.592574 2 TNP1 0.0 5.563935 LYZL2 0.0 4.095589 TMSB4X 3.038543e-179 7.129280 TEX101 ... 6.329660 ZMYND10 0.0 3.068692 DNAJB6 0.0 4.119973 SMC3 6.352623e-220 4.531697 3 LINC01921 0.0 4.993138 ACRV1 0.0 4.410256 EEF1A1 8.509774e-179 3.155737 TDRG1 ... 5.709855 LYAR 0.0 3.380893 RPS19 0.0 3.611154 NASP 5.771049e-216 3.862513 4 LELP1 0.0 3.586964 SPACA3 0.0 3.781013 MYL6 4.101360e-178 4.146432 TPTE ... 5.262171 COPRS 0.0 3.012631 RPSA 0.0 4.187228 VCX 2.471442e-215 4.379521 5 GLUL 0.0 3.746591 C1orf185 0.0 4.043617 RPL41 4.101360e-178 3.012833 HORMAD1 ... 3.822508 SPINK2 0.0 2.506658 FKBP8 0.0 2.255552 CIRBP 3.932801e-205 3.970943 6 ODF2 0.0 1.980321 ACTRT3 0.0 3.629563 B2M 8.444394e-178 6.490492 STMN1 ... 2.181750 MRPL34 0.0 3.766496 RPL18A 0.0 4.925106 TKTL1 4.225019e-205 4.495093 7 C10orf62 0.0 3.671284 FAM209B 0.0 3.971979 RPS8 1.058438e-177 2.973517 ARL6IP1 ... 3.732162 LDHC 0.0 2.803699 ZNF428 0.0 3.834186 NCL 1.175944e-200 4.035778 8 AC010255.3 0.0 3.887318 LYZL1 0.0 4.498296 CD63 5.158918e-177 5.339066 TMEM147 ... 3.188939 CAVIN3 0.0 3.651773 EEF1B2 0.0 2.871610 SDF2L1 1.215438e-200 3.446499 9 GAPDHS 0.0 3.189089 TEX38 0.0 3.211391 CALD1 5.769633e-177 6.693130 C5orf47 ... 3.403830 TMEM225B 0.0 2.711139 HNRNPDL 0.0 3.815118 YWHAE 1.111880e-196 2.073113 <p>10 rows \u00d7 21 columns</p> <p>We can easily save the table in csv format. This can be opened in Excel.</p> In\u00a0[32]: Copied! <pre>!mkdir -p ../../Data/results\n</pre> !mkdir -p ../../Data/results In\u00a0[33]: Copied! <pre>X.to_csv('../../Data/results/diff_expression_clusters.csv', header=True, index=False)\n</pre> X.to_csv('../../Data/results/diff_expression_clusters.csv', header=True, index=False) <p>We find subclusters of cells using markers for cell types that are found between late spermatogonia and spermatocites.</p> In\u00a0[34]: Copied! <pre>#Clusters to be subsetted\nSUBGROUPS = ['SpermatogoniaB','SpermatocitesI','SpermatocitesII']\n</pre> #Clusters to be subsetted SUBGROUPS = ['SpermatogoniaB','SpermatocitesI','SpermatocitesII']  In\u00a0[35]: Copied! <pre>#Markers for the processes involved in late spermatogonia and spermatocites\nmarkers['Leptotene'] = ['SYCE2','SCML1']\nmarkers['Zygotene'] = ['LY6K', 'SYCP1']\nmarkers['Pachytene'] = ['PIWIL1','CCDC112']\nmarkers['Diplotene'] = ['OVOL2','CCNA1', 'CDK1','AURKA']\n</pre> #Markers for the processes involved in late spermatogonia and spermatocites markers['Leptotene'] = ['SYCE2','SCML1'] markers['Zygotene'] = ['LY6K', 'SYCP1'] markers['Pachytene'] = ['PIWIL1','CCDC112'] markers['Diplotene'] = ['OVOL2','CCNA1', 'CDK1','AURKA'] <p>Let's look at the markers plottes only over the cells of the clusters <code>SpermatogoniaB</code>, <code>SpermatocitesI</code> and <code>SpermatocitesII</code></p> In\u00a0[36]: Copied! <pre>sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ], \n           color=markers['Leptotene'])\n</pre> sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ],             color=markers['Leptotene']) In\u00a0[37]: Copied! <pre>sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ], \n           color=markers['Zygotene'])\n</pre> sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ],             color=markers['Zygotene']) In\u00a0[38]: Copied! <pre>sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ], \n           color=markers['Pachytene'])\n</pre> sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ],             color=markers['Pachytene']) In\u00a0[39]: Copied! <pre>sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ], \n           color=markers['Diplotene'])\n</pre> sc.pl.umap( sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ],             color=markers['Diplotene']) <p>We want to create new clusters in our dataset by starting from the old clustering. To do this we use the option <code>restrict_to</code>, where we write the name of the old clustering, and the name of which clusters we want to subset. Try to tune the resolution to have a proper number of clusters to rename.</p> In\u00a0[48]: Copied! <pre>sc.tl.leiden(sample, resolution=.4, key_added='clusters_spc',\n            restrict_to=('clusters', SUBGROUPS),\n            random_state=12345)\n</pre> sc.tl.leiden(sample, resolution=.4, key_added='clusters_spc',             restrict_to=('clusters', SUBGROUPS),             random_state=12345) <p>Let's look at the new clustering of <code>spermatogoniaB</code> and <code>spermatocites I/II</code></p> In\u00a0[49]: Copied! <pre>sc.pl.umap(sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ], \n           color=['clusters_spc'], legend_fontsize=15)\n</pre> sc.pl.umap(sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ],             color=['clusters_spc'], legend_fontsize=15) <pre>Trying to set attribute `.uns` of view, copying.\n</pre> <p>Names are very long. We keep only the numbers at the end</p> In\u00a0[50]: Copied! <pre>clusters = sample.obs['clusters_spc']\ncluster_array = np.array(clusters)\nsplit_array = [ i.split(',')[1] if ',' in i else i for i in cluster_array]\nclusters = pd.Categorical(split_array)\nsample.obs['clusters_spc']=clusters.copy()\n</pre> clusters = sample.obs['clusters_spc'] cluster_array = np.array(clusters) split_array = [ i.split(',')[1] if ',' in i else i for i in cluster_array] clusters = pd.Categorical(split_array) sample.obs['clusters_spc']=clusters.copy() In\u00a0[51]: Copied! <pre>sc.pl.umap(sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ], \n           color=['clusters_spc'], legend_loc='on data', legend_fontsize=18)\n</pre> sc.pl.umap(sample[ [i in SUBGROUPS for i in sample.obs['clusters']] ],             color=['clusters_spc'], legend_loc='on data', legend_fontsize=18) <pre>Trying to set attribute `.uns` of view, copying.\n</pre> <p>Before renaming, we also want to look at the differentially expressed genes. We should be able to find at least some of the markers used in the plots. However, it can be that those do not appear because there are many other coexpressed genes with high expression values.</p> In\u00a0[52]: Copied! <pre>sample.X = sample.layers['umi_sct'].copy()\nsc.pp.log1p(sample)\n</pre> sample.X = sample.layers['umi_sct'].copy() sc.pp.log1p(sample) <pre>WARNING: adata.X seems to be already log-transformed.\n</pre> In\u00a0[53]: Copied! <pre>sc.tl.rank_genes_groups(sample, groupby='clusters_spc', key_added='DE_clusters_spc',\n                        use_raw=False, n_genes=20, method='wilcoxon')\n</pre> sc.tl.rank_genes_groups(sample, groupby='clusters_spc', key_added='DE_clusters_spc',                         use_raw=False, n_genes=20, method='wilcoxon') <p>We can find some of the marker genes. For example <code>SCML1</code> for leptotene cells, <code>SYCP1</code> and <code>LY6K</code> for zygotene, <code>CCDC112</code> and <code>PIWIL1</code> for pachitene, <code>CCNA1</code> and <code>AURKA</code> for zygotene.</p> In\u00a0[54]: Copied! <pre>pd.DataFrame(sample.uns['DE_clusters_spc']['names'])\n</pre> pd.DataFrame(sample.uns['DE_clusters_spc']['names']) Out[54]: 0 1 10 2 3 4 5 6 7 8 9 ElongSpermatids RoundSpermatids Somatic SpermatogoniaA 0 TPTE PTMA DPH7 NDUFAF3 CDRT15 AL133499.1 C15orf48 TEX101 GTF2A2 TBPL1 PPP3R2 PRM1 ERICH2 RPL10 RPS12 1 C5orf58 HMGB1 SCML1 PPP3R2 SLC51B SNRPC CCDC112 ZCWPW1 CCDC42 CCDC42 CIAPIN1 PRM2 FNDC11 VIM CCNI 2 TMEM99 CIRBP SMC3 ASRGL1 CCDC42 FBXO25 PHF7 SYCP1 CCNB2 GYG1 LYAR TNP1 LYZL2 TMSB4X DNAJB6 3 AC044839.1 CFL1 SMC1B CCNA1 ETFRF1 GYG1 MGAT4D SELENOT TBPL1 CAVIN3 ASRGL1 LINC01921 ACRV1 EEF1A1 RPS19 4 LY6K TKTL1 ZCWPW1 AL133499.1 TBPL1 LDHC CETN3 RHEB TMIGD3 ISOC2 GULP1 LELP1 SPACA3 MYL6 RPSA 5 GIHCG TRMT112 VCX CIAPIN1 CT66 UQCR10 COX7A2 C5orf47 ETFRF1 LDHC AARD GLUL C1orf185 RPL41 FKBP8 6 H2AFZ NASP VCX3B LDHC SNRPC ZMYND10 GIHCG LY6K C16orf95 CCNB2 AC005041.4 ODF2 ACTRT3 B2M RPL18A 7 SPATA8 SMC3 SYCP3 COPRS MCHR2-AS1 COPRS CFAP53 NPC2 SLC51B CCNA1 AC016747.1 C10orf62 FAM209B RPS8 ZNF428 8 TDRG1 HMGN2 VPS29 ZMYND10 C16orf95 CATSPERZ RPL39L EIF1B MRPL43 ZC2HC1C CAVIN3 AC010255.3 LYZL1 CD63 EEF1B2 9 PRSS21 PRAME TEX19 GYG1 CCNB2 ZC2HC1C AC008771.1 DYNLL1 SNRPC ETFRF1 SLC25A19 GAPDHS TEX38 CALD1 HNRNPDL 10 ARL6IP1 VCX CLSPN MRPL34 GTF2A2 UBB FAM174A FMR1NB PTTG1 C16orf95 AKAP12 P3R3URF SPACA7 RPL13A YWHAB 11 CALM2 YWHAE HPRT1 CLGN C9orf116 ISOC2 STIM2-AS1 HORMAD1 CAVIN3 CCT6B MRM3 TSSK6 FAM209A MGP TUBA1B 12 CKLF NCL INCA1 LYAR PIFO CCNA1 ARL3 TDRG1 CT66 COMMD8 TKTL2 DCUN1D1 EQTN MALAT1 RPS5 13 STMN1 RNPS1 HIST1H4C CAVIN3 GOT1 RAE1 PIWIL1 KIF5B SCCPDH MRPL34 C15orf48 HMGB4 TEX33 TMSB10 EGFL7 14 HORMAD1 CHCHD2 HMGB2 RAE1 CAVIN3 APH1B RNFT1 SYCP3 ISOC2 AC002467.1 ZMYND10 SPATA3 AFG1L FTL LYPLA1 15 TEX101 CCT6A SDF2L1 ISOC2 AL133499.1 H2AFJ BCAP29 SMC3 KPNA5 AURKA NDUFAF3 TEX44 SESN3 RPS4X RPS21 16 TMEM147 RBM3 TAF12 SPINK2 PSMG1 CAVIN3 MLLT10 HSP90B1 CDRT15 AL133499.1 RGCC TEX37 SPACA4 RPL10A RPLP0 17 TBCA BTG3 ZCCHC17 AC002467.1 MRPS15 TBPL1 TMEM99 C5orf58 H2AFJ MRPL43 POU5F2 OAZ3 DUSP13 SPARCL1 RPS28 18 H3F3B LSM4 PAGE1 APH1B ZNRD1 PBK PENK STMN1 AL133499.1 ZPBP2 KLB FNDC8 TMCO2 BEX3 PAFAH1B3 19 CTSL GAGE2A PRAP1 REXO5 CFAP126 CLGN MORF4L1 DMRTC2 PSMG1 PBK GMCL2 IQCF1 TMEM270 RPL39 RPS18 <p>We can again look at p-values and log-fold changes</p> In\u00a0[55]: Copied! <pre>result = sample.uns['DE_clusters_spc']\ngroups = result['names'].dtype.names\nX = pd.DataFrame(\n    {group + '_' + key[:1].upper(): result[key][group]\n    for group in groups for key in ['names', 'pvals_adj','logfoldchanges']})\n</pre> result = sample.uns['DE_clusters_spc'] groups = result['names'].dtype.names X = pd.DataFrame(     {group + '_' + key[:1].upper(): result[key][group]     for group in groups for key in ['names', 'pvals_adj','logfoldchanges']}) <p>and save the table</p> In\u00a0[56]: Copied! <pre>X.to_csv('../../Data/results/diff_expression_subclusters.csv', header=True, index=False)\n</pre> X.to_csv('../../Data/results/diff_expression_subclusters.csv', header=True, index=False) <p>We can look at only the columns of a cluster from the large table so it is more readable</p> In\u00a0[57]: Copied! <pre>X[ ['7_N','7_L','7_P'] ]\n</pre> X[ ['7_N','7_L','7_P'] ] Out[57]: 7_N 7_L 7_P 0 GTF2A2 1.713210 2.454507e-54 1 CCDC42 2.881220 7.724417e-54 2 CCNB2 2.298391 5.539823e-51 3 TBPL1 2.151057 1.737986e-50 4 TMIGD3 2.112299 2.285057e-49 5 ETFRF1 2.278255 3.827467e-49 6 C16orf95 2.109850 5.745663e-49 7 SLC51B 2.715796 1.477458e-47 8 MRPL43 2.184598 1.477458e-47 9 SNRPC 2.202506 2.360302e-47 10 PTTG1 1.975605 2.765424e-47 11 CAVIN3 2.772689 4.681643e-47 12 CT66 2.155308 1.182636e-46 13 SCCPDH 1.568190 1.712657e-46 14 ISOC2 2.652072 2.530819e-46 15 KPNA5 2.494994 2.650060e-46 16 CDRT15 2.618294 2.650060e-46 17 H2AFJ 1.601331 3.370764e-46 18 AL133499.1 3.074455 3.370764e-46 19 PSMG1 1.719448 3.967144e-46 <p>We rename the new clusters. Write the names in the dictionary. Some of the clusters might still be <code>SpermatogoniaB</code> or <code>SpermatocitesII</code> as before.</p> In\u00a0[58]: Copied! <pre>clusters = pd.Categorical(sample.obs['clusters_spc'])\n</pre> clusters = pd.Categorical(sample.obs['clusters_spc']) In\u00a0[59]: Copied! <pre>new_names = {\n    '0':'Zygotene.1',\n    '1':'SpermatogoniaB',\n    '2':'Diplotene.1',\n    '3':'Diplotene.2',\n    '4':'Diplotene.3',\n    '5':'Pachytene',\n    '6':'Zygotene.2',\n    '7':'Diplotene.4',\n    '8':'Diplotene.5',\n    '9':'Diplotene.6',\n    '10':'Leptotene'\n}\n</pre> new_names = {     '0':'Zygotene.1',     '1':'SpermatogoniaB',     '2':'Diplotene.1',     '3':'Diplotene.2',     '4':'Diplotene.3',     '5':'Pachytene',     '6':'Zygotene.2',     '7':'Diplotene.4',     '8':'Diplotene.5',     '9':'Diplotene.6',     '10':'Leptotene' } In\u00a0[60]: Copied! <pre>clusters=clusters.rename_categories(new_names)\n</pre> clusters=clusters.rename_categories(new_names) In\u00a0[61]: Copied! <pre>cluster_array = np.array(clusters)\nsplit_array = [ i.split('.')[0] for i in cluster_array ]\nclusters = pd.Categorical(split_array)\n</pre> cluster_array = np.array(clusters) split_array = [ i.split('.')[0] for i in cluster_array ] clusters = pd.Categorical(split_array) In\u00a0[62]: Copied! <pre>sample.obs['clusters_spc']=clusters.copy()\n</pre> sample.obs['clusters_spc']=clusters.copy() <p>Just a plot of the two clustering side by side</p> In\u00a0[63]: Copied! <pre>sc.plotting.umap(sample, color=['clusters','clusters_spc'], legend_loc='on data')\n</pre> sc.plotting.umap(sample, color=['clusters','clusters_spc'], legend_loc='on data') <p>It isn't really clear how to rename clusters in somatic cells. We let them be called <code>somatic</code> without further specification</p> <p>You can plot the correlation matrix of the clusters with a dendrogram tree on the left side. Note how round and elongated spermatids are largely separated from the rest of the data. Spermatogonias are very similar to each other and could maybe reduced into a single cluster.</p> In\u00a0[64]: Copied! <pre>sc.pl.correlation_matrix(sample, groupby='clusters_spc', \n                         show_correlation_numbers=True)\n</pre> sc.pl.correlation_matrix(sample, groupby='clusters_spc',                           show_correlation_numbers=True) <pre>WARNING: dendrogram data not found (using key=dendrogram_clusters_spc). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n</pre> <p>Look at the proportion of each cell type in the data</p> In\u00a0[65]: Copied! <pre>#number of cells\nsample.obs['clusters_spc'].value_counts()\n</pre> #number of cells sample.obs['clusters_spc'].value_counts() Out[65]: <pre>RoundSpermatids    2521\nDiplotene          1248\nSpermatogoniaA      683\nElongSpermatids     618\nZygotene            483\nSpermatogoniaB      328\nSomatic             295\nPachytene           168\nLeptotene            87\nName: clusters_spc, dtype: int64</pre> In\u00a0[66]: Copied! <pre>#Percentage of cells\nsample.obs['clusters_spc'].value_counts() / sample.shape[0] * 100\n</pre> #Percentage of cells sample.obs['clusters_spc'].value_counts() / sample.shape[0] * 100 Out[66]: <pre>RoundSpermatids    39.200746\nDiplotene          19.406002\nSpermatogoniaA     10.620432\nElongSpermatids     9.609703\nZygotene            7.510496\nSpermatogoniaB      5.100295\nSomatic             4.587156\nPachytene           2.612346\nLeptotene           1.352822\nName: clusters_spc, dtype: float64</pre> <p>finally, save the data</p> In\u00a0[67]: Copied! <pre>sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.h5ad')\n</pre> sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.h5ad') <p>We have been showing how to simply identify potential cell clusters. At least in this dataset, the cells change from one type to another in a continuous process, so such a hard clustering does not completely reflect biological reality. However, it is a good approximation, as it is illustrated by the differentially expressed genes we could check in each cluster. We introduced how to perform differential expression, and what are the useful values that we get from it (p-value of the test, magnitude of the gene expression compared to all other clusters). Finally, we subsetted the data into a more fine grained cell identification.</p>"},{"location":"python/Part04_clustering.html#cell-clustering-and-differential-expression-de","title":"Cell clustering and Differential Expression (DE)\u00b6","text":"<p>Motivation:</p> <p>Spermatogenesis goes through different stages, starting from SpermatogoniaA cells, going into clonal expansion while keeping cells connected through cytoplasmic bridges (SpermatogoniaB), and then continuing with the meiotic process (Spermatocites I and II). Finally, cells become Round spermatids, which then elongate to become Elongated spermatids and sperm.</p> <p></p> <p>Detecting those cell types is essential to answer biological questions such as</p> <ul> <li>which genes are most expressed for each cell type (beyond well known ones)?</li> <li>in which proportion is every cell type present?</li> <li>are there unknown cell types that I can identify?</li> </ul> <p>Learning objectives:</p> <ul> <li>Identify potential cell clusters by visualizing marker genes on the UMAP plot</li> <li>Understanding and applying differential gene expression analysis to verify cluster identities</li> <li>Performing an analysis of subclusters in the dataset</li> </ul> <p>Execution time: 45-60 minutes</p>"},{"location":"python/Part04_clustering.html#identification-through-marker-genes","title":"Identification through marker genes\u00b6","text":"<p>we try to identify clusters of cells by looking at the expression of relevant marker genes. This requires a previous biological knowledge of those cell types, such that we can input the markers. Below, we define a dectionary, where for each cell type we define a list of markers. Then we will plot every list of markers on the UMAP plot</p>"},{"location":"python/Part04_clustering.html#differential-expression-de-analysis","title":"Differential Expression (DE) analysis\u00b6","text":""},{"location":"python/Part04_clustering.html#subclustering-the-data","title":"Subclustering the data\u00b6","text":""},{"location":"python/Part04_clustering.html#wrapping-up","title":"Wrapping up\u00b6","text":""},{"location":"python/Part05_cell_fate.html","title":"Pseudotimes and cell fates","text":"<p>*Import packages*</p> In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport anndata as ad\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn import anndata as ad In\u00a0[2]: Copied! <pre>import rpy2.rinterface_lib.callbacks\nimport logging\n\nfrom rpy2.robjects import pandas2ri\nimport anndata2ri\n\n# Ignore R warning messages\n#Note: this can be commented out to get more verbose R output\nrpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n\n# Automatically convert rpy2 outputs to pandas dataframes\npandas2ri.activate()\nanndata2ri.activate()\n\n#import os\n#os.environ['R_HOME'] = '../../../scrna-environment/lib/R/' #path to your R installation\n\n%load_ext rpy2.ipython\n</pre> import rpy2.rinterface_lib.callbacks import logging  from rpy2.robjects import pandas2ri import anndata2ri  # Ignore R warning messages #Note: this can be commented out to get more verbose R output rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)  # Automatically convert rpy2 outputs to pandas dataframes pandas2ri.activate() anndata2ri.activate()  #import os #os.environ['R_HOME'] = '../../../scrna-environment/lib/R/' #path to your R installation  %load_ext rpy2.ipython In\u00a0[3]: Copied! <pre>%%R\n.libPaths( c( \"../../../../sandbox_scRNA_testAndFeedback/scrna-environment/lib/R/library/\" ) )\n</pre> %%R .libPaths( c( \"../../../../sandbox_scRNA_testAndFeedback/scrna-environment/lib/R/library/\" ) ) In\u00a0[22]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>*Read data*</p> In\u00a0[5]: Copied! <pre>sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.h5ad')\n</pre> sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.h5ad') <pre>WARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.h5ad'].\nOnly considering the two last: ['.2', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.h5ad'].\nOnly considering the two last: ['.2', '.h5ad'].\n</pre> <p>We want to calculate pseudotimes on the spermatogenic process. We exclude the somatic cells from.</p> In\u00a0[6]: Copied! <pre>cellsToKeep = [ i not in ['Somatic'] for i in sample.obs['clusters_spc'] ]\nsample = sample[ cellsToKeep, : ].copy()\n</pre> cellsToKeep = [ i not in ['Somatic'] for i in sample.obs['clusters_spc'] ] sample = sample[ cellsToKeep, : ].copy() <p>we use the <code>python</code> package <code>palantir</code></p> In\u00a0[7]: Copied! <pre>import palantir\npalantir.core.random.seed( a=12345 ) #define random_state (here called 'a')\n</pre> import palantir palantir.core.random.seed( a=12345 ) #define random_state (here called 'a') <pre>findfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\n</pre> <p>we create a table (<code>pandas</code> dataframe) with the logarithm of the corrected UMI matrix, since <code>palantir</code> needs logarithmized raw counts in input</p> In\u00a0[8]: Copied! <pre>palantir_data = pd.DataFrame(np.log1p(sample.layers['umi_sct'].todense()),\n                             index=sample.obs_names,\n                             columns=sample.var_names)\n</pre> palantir_data = pd.DataFrame(np.log1p(sample.layers['umi_sct'].todense()),                              index=sample.obs_names,                              columns=sample.var_names) <p>Instead of letting the package calculate the PCA (without any form of datasets integration), we use our integrated PCA.</p> In\u00a0[9]: Copied! <pre>pca_projections = pd.DataFrame( sample.obsm['X_pca'][:,0:15].copy(),\n                                index=sample.obs_names )\n</pre> pca_projections = pd.DataFrame( sample.obsm['X_pca'][:,0:15].copy(),                                 index=sample.obs_names )      <p>Now we will infer the pseudotimes and related cell fates. We have to provide where the differentiation process starts from. In our case, we will choose one of the cells in the cluster <code>SpermatogoniaA</code>. Then <code>Palantir</code> will assign the pseudotimes=0 to the most appropriate cell in the cluster. Note the option <code>num_waypoints=100</code> in the last command. This option will use a certain number of cells to build the tree from which to calculate pseudotimes and cell fates. it is suggested to use only a portion of cells from the dataset, since using all cells will make you experience the inference of many cell fates that are mostly due to noise. In other words, you will build a tree with some tiny branches that will be detected as cellular fates.</p> In\u00a0[10]: Copied! <pre>ORIGIN_STATE = 'SpermatogoniaA' #where to start\nsc.tl.diffmap(sample)\ndiffusionMap = pd.DataFrame(sample.obsm['X_diffmap'][:,1::], \n                             index=sample.obs_names,\n                             columns = [str(i) for i in range(sample.obsm['X_diffmap'].shape[1]-1)])\n#apply palantir\nstart_cell = str(sample[sample.obs['clusters_spc'] == ORIGIN_STATE].obs_names[0]) #assignment of diferentiation start\npr_res = palantir.core.run_palantir( diffusionMap, early_cell=start_cell, num_waypoints=1000) #fate detection\n</pre> ORIGIN_STATE = 'SpermatogoniaA' #where to start sc.tl.diffmap(sample) diffusionMap = pd.DataFrame(sample.obsm['X_diffmap'][:,1::],                               index=sample.obs_names,                              columns = [str(i) for i in range(sample.obsm['X_diffmap'].shape[1]-1)]) #apply palantir start_cell = str(sample[sample.obs['clusters_spc'] == ORIGIN_STATE].obs_names[0]) #assignment of diferentiation start pr_res = palantir.core.run_palantir( diffusionMap, early_cell=start_cell, num_waypoints=1000) #fate detection <pre>Sampling and flocking waypoints...\nTime for determining waypoints: 0.008485527833302815 minutes\nDetermining pseudotime...\nShortest path distances using 30-nearest neighbor graph...\n</pre> <pre>findfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\n</pre> <pre>Time for shortest paths: 0.20841457843780517 minutes\nIteratively refining the pseudotime...\nCorrelation at iteration 1: 1.0000\nEntropy and branch probabilities...\nMarkov chain construction...\nIdentification of terminal states...\nComputing fundamental matrix and absorption probabilities...\nProject results to all cells...\n</pre> <p>We save pseudotimes in our dataset and plot them on UMAP</p> In\u00a0[11]: Copied! <pre>sample.obs['pseudotime'] = pr_res.pseudotime\n</pre> sample.obs['pseudotime'] = pr_res.pseudotime In\u00a0[12]: Copied! <pre>sc.pl.umap( sample, color=['clusters_spc','pseudotime'], \n           legend_loc='on data', \n           legend_fontsize=16,\n           ncols=2 )\n</pre> sc.pl.umap( sample, color=['clusters_spc','pseudotime'],             legend_loc='on data',             legend_fontsize=16,            ncols=2 ) <pre>findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n</pre> <p>We can look at how pseudotimes are distributed into each cluster. It seems the variability of pseudotimes increases along spermatogenesis with some oscillations. This can mean more variability in the expression of genes in the later clusters (but does not mean that there are more genes that are expressed). Note that there are considerable overlapping in pseudotimes. This is due to the fact that pseudotimes have a spike around Pachytene-Diplotene stages.</p> In\u00a0[23]: Copied! <pre>cluster_names = [i for i in ['SpermatogoniaA', 'SpermatogoniaB', 'Leptotene', 'Zygotene', \n                   'Pachytene', 'SpermatocitesII', 'Diplotene', 'RoundSpermatids',\n                   'ElongSpermatids'] if i in np.array(sample.obs['clusters_spc']) ]\nsc.pl.violin(sample, keys='pseudotime', groupby='clusters_spc', rotation=90,\n            order=cluster_names)\n</pre> cluster_names = [i for i in ['SpermatogoniaA', 'SpermatogoniaB', 'Leptotene', 'Zygotene',                     'Pachytene', 'SpermatocitesII', 'Diplotene', 'RoundSpermatids',                    'ElongSpermatids'] if i in np.array(sample.obs['clusters_spc']) ] sc.pl.violin(sample, keys='pseudotime', groupby='clusters_spc', rotation=90,             order=cluster_names) <p>we can see how many fates we have. For each fate, there is the barcode of the cell best representing a differentiation stage. In some cases you can have more than two fates</p> In\u00a0[26]: Copied! <pre>fates = list(pr_res.branch_probs.columns)\n</pre> fates = list(pr_res.branch_probs.columns) In\u00a0[27]: Copied! <pre>fates\n</pre> fates Out[27]: <pre>['TCCCGATGTAGCGTGA-1-0']</pre> <p>We can plot them on the UMAP plot. One fate is clearly the end of spermatogenesis, where cells become elongated spermatids and spermatozoa.There is another fate, probably due to something happening during meiosis.</p> In\u00a0[28]: Copied! <pre>f, ax = plt.subplots(1,1)\n\nsc.pl.umap( sample,\n           legend_loc='on data', \n           legend_fontsize=16, ax=ax, show=False)\n\ncoordinates = sample[fates].obsm['X_umap']\nax.plot(coordinates[:,0],coordinates[:,1],'o',markersize=12) \nfor i in range(coordinates.shape[0]):\n    ax.text(coordinates[i,0]-1,coordinates[i,1]-2, f'Fate {i}')\n    ax.set_title(\"Inferred cell fates\")\nplt.show()\n</pre> f, ax = plt.subplots(1,1)  sc.pl.umap( sample,            legend_loc='on data',             legend_fontsize=16, ax=ax, show=False)  coordinates = sample[fates].obsm['X_umap'] ax.plot(coordinates[:,0],coordinates[:,1],'o',markersize=12)  for i in range(coordinates.shape[0]):     ax.text(coordinates[i,0]-1,coordinates[i,1]-2, f'Fate {i}')     ax.set_title(\"Inferred cell fates\") plt.show() <pre>findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n</pre> <p>We rename the fates as follows instead of using cell barcodes</p> In\u00a0[29]: Copied! <pre>fates = np.array( pr_res.branch_probs.columns )\nfor i in range(coordinates.shape[0]):\n    fates[i] = f'Fate {i}'\npr_res.branch_probs.columns = fates\n</pre> fates = np.array( pr_res.branch_probs.columns ) for i in range(coordinates.shape[0]):     fates[i] = f'Fate {i}' pr_res.branch_probs.columns = fates     <p>We save in our data the probability that each cell differentiate into one of the fates</p> In\u00a0[30]: Copied! <pre>for i in pr_res.branch_probs.columns:\n    sample.obs[f'branch_prob_{i}'] = pr_res.branch_probs[i]\n</pre> for i in pr_res.branch_probs.columns:     sample.obs[f'branch_prob_{i}'] = pr_res.branch_probs[i] <p>A good practice is to look at the probabilities of ending in a fate for each cluster. There are two possible scenarios:</p> <ul> <li>only one fate: all cells have probability 1 of ending at a specific cell fate</li> <li>more than one cell fate: some fates are actual branchings of the developmental process, and only some cells will have a probability of ending up in those branchings. Some other fates are just midpoints of the developmental process. Here, they will absorb with probability 1 entire sections of the dataset.</li> </ul> <p>We plot below the probability of each cell (seen by cluster) to end up in a specific fate. Each violin plot corresponds to a single fate.</p> In\u00a0[34]: Copied! <pre>for i in range(coordinates.shape[0]):\n    x = sc.pl.violin(sample, groupby='clusters_spc', keys=f'branch_prob_Fate {i}', rotation=90,\n            order=cluster_names, ylabel=f'Probability of Fate {i}')\n</pre> for i in range(coordinates.shape[0]):     x = sc.pl.violin(sample, groupby='clusters_spc', keys=f'branch_prob_Fate {i}', rotation=90,             order=cluster_names, ylabel=f'Probability of Fate {i}')  <p>Here is a script that plots gene expressions of your choice along pseudotimes. This allows you to see how specific genes behave differently for different fates. Expressions are modeled using the fate probabilities we plotted above.</p> In\u00a0[35]: Copied! <pre>import palantir\n</pre> import palantir In\u00a0[36]: Copied! <pre>GENES = ['PIWIL1','PIWIL2','PIWIL3']\nGENES = np.intersect1d(GENES, sample.var_names)\nNGENES = len(GENES)\nCLUSTERS = sample.obs['clusters_spc']\nPSEUDOTIMES = sample.obs['pseudotime']\ngene_trends = palantir.presults.compute_gene_trends(pr_res, \n                                                    pd.DataFrame(sample.layers['norm_sct'], \n                                                    index=sample.obs_names,\n                                                    columns=sample.var_names).loc[:, GENES] \n                                                   )\n\nplt.rcParams['figure.figsize']=(12,4*int(NGENES))\nfig, ax = plt.subplots(NGENES,1)\n\nc = CLUSTERS\nx = PSEUDOTIMES\n\n\nif(NGENES==1):\n    x2 = []\n    t = []\n    style = []\n    for FATE in list(gene_trends.keys()):\n        ARRAY = np.array( gene_trends[FATE]['trends'].loc[GENES[0],:].index )\n        for i in ARRAY:\n            idx = np.argmin(np.abs(x - i))\n            x2.append(c[idx])\n            t.append(i)\n        if(len(style)==0):\n            style = np.tile( FATE, 500 )\n            y = np.array(gene_trends[FATE]['trends'].loc[GENES[0],:])\n        else:\n            style = np.append(arr=style, \n                      values=np.tile( FATE, 500 ))\n            y = np.append(arr=y, \n                      values=np.array(gene_trends[FATE]['trends'].loc[GENES[0],:]))\n        \n    sns.lineplot(x=t,\n             y=y, ci=False, \n             hue=x2, ax=ax, style = style,\n             linewidth = 5)\n    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    ax.set(xlabel = 'Pseudotime', ylabel=GENES[0])  \n    \nif(NGENES&gt;1):      \n    for GENE_NR in range(NGENES):\n        style = []\n        x2 = []\n        t = []\n        for FATE in list(gene_trends.keys()):\n            ARRAY = np.array( gene_trends[FATE]['trends'].loc[GENES[GENE_NR],:].index )\n            for i in ARRAY:\n                idx = np.argmin(np.abs(x - i))\n                x2.append(c[idx])\n                t.append(i)\n            if(len(style)==0):\n                style = np.tile( FATE, 500 )\n                y = np.array(gene_trends[FATE]['trends'].loc[GENES[GENE_NR],:])\n            else:\n                style = np.append(arr=style, \n                              values=np.tile( FATE, 500 ))\n                y = np.append(arr=y, \n                              values=np.array(gene_trends[FATE]['trends'].loc[GENES[GENE_NR],:]))\n        sns.lineplot(x=t,\n                             y=y, ci=False, \n                             hue=x2, ax=ax[GENE_NR],\n                             style = style, linewidth = 5, legend=GENE_NR==0)\n        ax[GENE_NR].set(ylabel = GENES[GENE_NR])\n        if(GENE_NR==0):\n            ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n            ax[0].set_title(f'Gene expression along the fates:\\n{list(gene_trends.keys())}')\n    ax[GENE_NR].set(xlabel = 'Pseudotime')   \n\nplt.rcParams['figure.figsize']=(6,6)\n</pre> GENES = ['PIWIL1','PIWIL2','PIWIL3'] GENES = np.intersect1d(GENES, sample.var_names) NGENES = len(GENES) CLUSTERS = sample.obs['clusters_spc'] PSEUDOTIMES = sample.obs['pseudotime'] gene_trends = palantir.presults.compute_gene_trends(pr_res,                                                      pd.DataFrame(sample.layers['norm_sct'],                                                      index=sample.obs_names,                                                     columns=sample.var_names).loc[:, GENES]                                                     )  plt.rcParams['figure.figsize']=(12,4*int(NGENES)) fig, ax = plt.subplots(NGENES,1)  c = CLUSTERS x = PSEUDOTIMES   if(NGENES==1):     x2 = []     t = []     style = []     for FATE in list(gene_trends.keys()):         ARRAY = np.array( gene_trends[FATE]['trends'].loc[GENES[0],:].index )         for i in ARRAY:             idx = np.argmin(np.abs(x - i))             x2.append(c[idx])             t.append(i)         if(len(style)==0):             style = np.tile( FATE, 500 )             y = np.array(gene_trends[FATE]['trends'].loc[GENES[0],:])         else:             style = np.append(arr=style,                        values=np.tile( FATE, 500 ))             y = np.append(arr=y,                        values=np.array(gene_trends[FATE]['trends'].loc[GENES[0],:]))              sns.lineplot(x=t,              y=y, ci=False,               hue=x2, ax=ax, style = style,              linewidth = 5)     ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)     ax.set(xlabel = 'Pseudotime', ylabel=GENES[0])        if(NGENES&gt;1):           for GENE_NR in range(NGENES):         style = []         x2 = []         t = []         for FATE in list(gene_trends.keys()):             ARRAY = np.array( gene_trends[FATE]['trends'].loc[GENES[GENE_NR],:].index )             for i in ARRAY:                 idx = np.argmin(np.abs(x - i))                 x2.append(c[idx])                 t.append(i)             if(len(style)==0):                 style = np.tile( FATE, 500 )                 y = np.array(gene_trends[FATE]['trends'].loc[GENES[GENE_NR],:])             else:                 style = np.append(arr=style,                                values=np.tile( FATE, 500 ))                 y = np.append(arr=y,                                values=np.array(gene_trends[FATE]['trends'].loc[GENES[GENE_NR],:]))         sns.lineplot(x=t,                              y=y, ci=False,                               hue=x2, ax=ax[GENE_NR],                              style = style, linewidth = 5, legend=GENE_NR==0)         ax[GENE_NR].set(ylabel = GENES[GENE_NR])         if(GENE_NR==0):             ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)             ax[0].set_title(f'Gene expression along the fates:\\n{list(gene_trends.keys())}')     ax[GENE_NR].set(xlabel = 'Pseudotime')     plt.rcParams['figure.figsize']=(6,6) <pre>Fate 0\n</pre> <pre>findfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\n</pre> <pre>Time for processing Fate 0: 0.4374883691469828 minutes\n</pre> <p>Gene clustering: A last thing you can do is to cluster together genes that have the same expression patterns. We can try to do this for each different fate. Here you can only look at one fate at a time</p> <p>For making the clustering faster, we cluster together only the differentially expressed genes we found in the previous analysis. However, below you can define the variable <code>genes</code> as any list of genes. You can for example read them from a text file, or you can use all possible genes by writing <code>genes=list(sample.var_names)</code></p> In\u00a0[37]: Copied! <pre>genes = []\nfor names in sample.uns['DE_clusters_spc']['names']:\n    genes.append( list( names ) )\n</pre> genes = [] for names in sample.uns['DE_clusters_spc']['names']:     genes.append( list( names ) ) In\u00a0[38]: Copied! <pre>genes = np.unique(np.ravel(genes))\n</pre> genes = np.unique(np.ravel(genes)) <p>model the gene expression along pseudotime</p> In\u00a0[39]: Copied! <pre>gene_trends = palantir.presults.compute_gene_trends( pr_res, \n                                         pd.DataFrame(sample[ :, genes ].layers['norm_sct'], \n                                         index=sample[ :, genes ].obs_names,\n                                         columns=sample[ :, genes ].var_names) )\n</pre> gene_trends = palantir.presults.compute_gene_trends( pr_res,                                           pd.DataFrame(sample[ :, genes ].layers['norm_sct'],                                           index=sample[ :, genes ].obs_names,                                          columns=sample[ :, genes ].var_names) ) <pre>Fate 0\n</pre> <pre>findfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Raleway'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Lato'] not found. Falling back to DejaVu Sans.\n</pre> <pre>Time for processing Fate 0: 0.17321434020996093 minutes\n</pre> <p>cluster the expressions together and plot clusters. If you see that there should be more clusters than the algorithm calculates, you can try to increase their number by changing the value of <code>k=20</code>. Usually, you should see a lot of genes expressed (in gray colour) differently from their averaged expression (in blue colour)</p> In\u00a0[40]: Copied! <pre>trends = gene_trends['Fate 0']['trends']\ngene_clusters = palantir.presults.cluster_gene_trends(trends, k=20)\n</pre> trends = gene_trends['Fate 0']['trends'] gene_clusters = palantir.presults.cluster_gene_trends(trends, k=20) <pre>Finding 20 nearest neighbors using minkowski metric and 'auto' algorithm\nNeighbors computed in 0.10072517395019531 seconds\nJaccard graph constructed in 1.2222075462341309 seconds\nWrote graph to binary file in 0.007137298583984375 seconds\nRunning Louvain modularity optimization\nAfter 1 runs, maximum modularity is Q = 0.801915\nLouvain completed 21 runs in 1.2909739017486572 seconds\nSorting communities by size, please wait ...\nPhenoGraph completed in 3.943619966506958 seconds\n</pre> In\u00a0[41]: Copied! <pre>palantir.plot.plot_gene_trend_clusters(trends, gene_clusters)\n</pre> palantir.plot.plot_gene_trend_clusters(trends, gene_clusters) <pre>findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n</pre> <p>Here is a script to produce the plot as above, with averaged expression of each gene cluster coloured by cell types, together with confidence bands. It takes some time to do all the plots, so be patient.</p> In\u00a0[42]: Copied! <pre>GENE_CLST = np.array(gene_clusters)\nUNIQUE_CLST = np.sort(np.unique(GENE_CLST))\nCLST_NR = int(len(UNIQUE_CLST))\nCLUSTERS = sample.obs['clusters_spc']\nPSEUDOTIMES = sample.obs['pseudotime']\n\nplt.rcParams['figure.figsize']=(12,4*CLST_NR)\nfig, ax = plt.subplots(CLST_NR,1)\n\nc = CLUSTERS\nx = PSEUDOTIMES\n\nif(CLST_NR==1):\n    t = []\n    x2 = []\n    ARRAY = np.array( trends.columns )\n    for i in ARRAY:\n        idx = np.argmin(np.abs(x - i))\n        x2.append(c[idx])\n        t.append(i)\n    \n    x=np.tile(ARRAY,trends.loc[GENE_CLST==0,:].shape[0])\n    y=np.array(trends.loc[GENE_CLST==0,:]).ravel()\n    hue=np.tile(x2,trends.loc[GENE_CLST==0,:].shape[0])\n    ax = sns.lineplot(x=x, y=y, hue=hue)\n        \n    sns.lineplot(x=x, y=y, hue=hue, \n                 ax=ax, linewidth = 5)\n    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    \n    \nif(CLST_NR&gt;1):\n    ARRAY = np.array( trends.columns )\n    t = []\n    x2 = []\n    for i in ARRAY:\n        idx = np.argmin(np.abs(x - i))\n        x2.append(c[idx])\n        t.append(i)\n    for CLST_NR in UNIQUE_CLST:\n        x=np.tile(ARRAY,trends.loc[GENE_CLST==CLST_NR,:].shape[0])\n        y=np.array(trends.loc[GENE_CLST==CLST_NR,:]).ravel()\n        hue=np.tile(x2,trends.loc[GENE_CLST==CLST_NR,:].shape[0])\n        \n        sns.lineplot(x=x, y=y, hue=hue,\n                 ax=ax[CLST_NR], linewidth = 5, legend=CLST_NR==0)\n        ax[CLST_NR].set(ylabel = f'Cluster {CLST_NR}')\n        if(CLST_NR==0):\n            ax[CLST_NR].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n            ax[CLST_NR].set_title('Gene expression clustering for cell fate 0')\n    ax[CLST_NR].set(xlabel = 'Pseudotime')\n        \nplt.rcParams['figure.figsize']=(6,6)\n</pre> GENE_CLST = np.array(gene_clusters) UNIQUE_CLST = np.sort(np.unique(GENE_CLST)) CLST_NR = int(len(UNIQUE_CLST)) CLUSTERS = sample.obs['clusters_spc'] PSEUDOTIMES = sample.obs['pseudotime']  plt.rcParams['figure.figsize']=(12,4*CLST_NR) fig, ax = plt.subplots(CLST_NR,1)  c = CLUSTERS x = PSEUDOTIMES  if(CLST_NR==1):     t = []     x2 = []     ARRAY = np.array( trends.columns )     for i in ARRAY:         idx = np.argmin(np.abs(x - i))         x2.append(c[idx])         t.append(i)          x=np.tile(ARRAY,trends.loc[GENE_CLST==0,:].shape[0])     y=np.array(trends.loc[GENE_CLST==0,:]).ravel()     hue=np.tile(x2,trends.loc[GENE_CLST==0,:].shape[0])     ax = sns.lineplot(x=x, y=y, hue=hue)              sns.lineplot(x=x, y=y, hue=hue,                   ax=ax, linewidth = 5)     ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)           if(CLST_NR&gt;1):     ARRAY = np.array( trends.columns )     t = []     x2 = []     for i in ARRAY:         idx = np.argmin(np.abs(x - i))         x2.append(c[idx])         t.append(i)     for CLST_NR in UNIQUE_CLST:         x=np.tile(ARRAY,trends.loc[GENE_CLST==CLST_NR,:].shape[0])         y=np.array(trends.loc[GENE_CLST==CLST_NR,:]).ravel()         hue=np.tile(x2,trends.loc[GENE_CLST==CLST_NR,:].shape[0])                  sns.lineplot(x=x, y=y, hue=hue,                  ax=ax[CLST_NR], linewidth = 5, legend=CLST_NR==0)         ax[CLST_NR].set(ylabel = f'Cluster {CLST_NR}')         if(CLST_NR==0):             ax[CLST_NR].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)             ax[CLST_NR].set_title('Gene expression clustering for cell fate 0')     ax[CLST_NR].set(xlabel = 'Pseudotime')          plt.rcParams['figure.figsize']=(6,6) <p>you can always look at the genes in a specific cluster. In this case, each cluster should be quite matching the differentially expressed genes for a cell type, since we grouped together differentially expressed genes</p> In\u00a0[43]: Copied! <pre>gene_clusters[gene_clusters==5]\n</pre> gene_clusters[gene_clusters==5] Out[43]: <pre>AC010255.3    5\nC10orf62      5\nDCUN1D1       5\nFNDC8         5\nGAPDHS        5\nGLUL          5\nHMGB4         5\nIQCF1         5\nLELP1         5\nLINC01921     5\nOAZ3          5\nODF2          5\nP3R3URF       5\nPRM1          5\nPRM2          5\nSPATA3        5\nTEX37         5\nTEX44         5\nTNP1          5\nTSSK6         5\ndtype: int64</pre> <p>We also want to save the dataset (including somatic cells) with pseudotimes. To do this we reopen the whole dataset and assign pseudotimes equal to 0 to the somatic cell.</p> In\u00a0[44]: Copied! <pre>whole_sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.h5ad')\n</pre> whole_sample = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.h5ad') <pre>WARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.h5ad'].\nOnly considering the two last: ['.2', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.h5ad'].\nOnly considering the two last: ['.2', '.h5ad'].\n</pre> In\u00a0[45]: Copied! <pre>times = pd.Series(sample.obs['pseudotime'], index=sample.obs_names)\nwhole_times = pd.Series(index=whole_sample.obs_names)\nnames = sample.obs_names\nwhole_names = whole_sample.obs_names\nwhole_times = [ times[i] if i in names else 0 for i in whole_names ]\n</pre> times = pd.Series(sample.obs['pseudotime'], index=sample.obs_names) whole_times = pd.Series(index=whole_sample.obs_names) names = sample.obs_names whole_names = whole_sample.obs_names whole_times = [ times[i] if i in names else 0 for i in whole_names ] In\u00a0[46]: Copied! <pre>whole_sample.obs['pseudotimes'] = whole_times\n</pre> whole_sample.obs['pseudotimes'] = whole_times In\u00a0[47]: Copied! <pre>whole_sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.times.h5ad')\n</pre> whole_sample.write('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.times.h5ad') <p>This notebooks shows how to do pseudotimes analysis and exploring cell fates and gene expressions. We have seen how to distinguish between an actual differentiation branch and a differentiation stage. Basically, all cells before (i.e. earlier in pseudotime) a differentiation stage will be associated to such stage with high probability, because they must go through that developmental stage. Finding a developmental stage around meiosis in spermatogenic samples is a common results across single cell datasets of many species (primates, humans, mice). Using the <code>palantir</code> software, we can look at differences between gene expressions for different fates, and cluster together genes of interest for further analysis.</p>"},{"location":"python/Part05_cell_fate.html#pseudotimes-and-cell-fates","title":"Pseudotimes and cell fates\u00b6","text":"<p>Motivation:</p> <p>While clustering is an useful type of analysis to try giving a structure to the development of cells towards their final stage (spermatozoa), it does not give an understanding of how the development \"stretches\" from start to end. For example, a cluster can have many cells and look \"big\" on UMAP, but actually its variability in terms of gene expressions could be low. Also, a developmental process can branches towards different ends (cell fates) or developmental checkpoints (e.g. stages where damaged cells express specific genes for apoptosis/cell death). Pseudotime and cell fates analysis can be used to hgihlight exactly those processes.</p> <ul> <li>Pseudotimes assigns to each cell the value of a timeline, starting from 0 for the cells at the beginning of the development. This value is purely a reference for ordering the cells development, but pseudotimes at specific stages can be assigned to real times, using previous biological knowledge.</li> <li>Cell fates analysis looks at the PCA projection of the data and the pseudotime of each data point on the PCA. From this, it tries to create a tree connecting the cells, so that the end branches of the tree are different end points or stages of the developmental process.</li> </ul> <p></p> <p>Figure: cell fates tree on a 3D pca plot. Circles represent the middle point of each cluster. From Perredaeau et al. (2017)</p> <p>Learning objectives:</p> <ul> <li>Understand and determine the pseudotimes on a single cell dataset</li> <li>Infer cell fates and distinguish between differentiation stages or actual final developmental stages</li> <li>Compare gene expressions along differentiation</li> <li>Cluster genes with similar gene expression</li> </ul> <p>Execution time: 45 minutes</p>"},{"location":"python/Part05_cell_fate.html#calculate-pseudotimes-and-cell-fates","title":"Calculate pseudotimes and cell fates\u00b6","text":""},{"location":"python/Part05_cell_fate.html#analysis-of-cell-fates","title":"Analysis of cell fates\u00b6","text":""},{"location":"python/Part05_cell_fate.html#recognizing-branchings-or-developmental-stages","title":"Recognizing branchings or developmental stages\u00b6","text":""},{"location":"python/Part05_cell_fate.html#exploring-gene-expression-and-clusters","title":"Exploring gene expression and clusters\u00b6","text":""},{"location":"python/Part05_cell_fate.html#wrapping-up","title":"Wrapping up\u00b6","text":""},{"location":"python/Part06_Condition_analysis.html","title":"Cross-data analysis","text":"<p>*Import packages*</p> In\u00a0[1]: Copied! <pre>import scanpy as sc\nimport pandas as pd\nimport scvelo as scv\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport anndata as ad\nimport gseapy as gp\n\nplt.rcParams['figure.figsize']=(6,6) #rescale figures\n</pre> import scanpy as sc import pandas as pd import scvelo as scv import numpy as np import seaborn as sns import matplotlib.pyplot as plt import sklearn import anndata as ad import gseapy as gp  plt.rcParams['figure.figsize']=(6,6) #rescale figures In\u00a0[2]: Copied! <pre>import rpy2.robjects as ro\n\nimport rpy2.rinterface_lib.callbacks\nimport logging\n\nfrom rpy2.robjects import pandas2ri\nimport anndata2ri\n\n# Ignore R warning messages\n#Note: this can be commented out to get more verbose R output\nrpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n\n# Automatically convert rpy2 outputs to pandas dataframes\npandas2ri.activate()\nanndata2ri.activate()\n%load_ext rpy2.ipython\n</pre> import rpy2.robjects as ro  import rpy2.rinterface_lib.callbacks import logging  from rpy2.robjects import pandas2ri import anndata2ri  # Ignore R warning messages #Note: this can be commented out to get more verbose R output rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)  # Automatically convert rpy2 outputs to pandas dataframes pandas2ri.activate() anndata2ri.activate() %load_ext rpy2.ipython In\u00a0[3]: Copied! <pre>%%R\n.libPaths( c( \"../../../../sandbox_scRNA_testAndFeedback/scrna-environment/lib/R/library/\" ) )\n</pre> %%R .libPaths( c( \"../../../../sandbox_scRNA_testAndFeedback/scrna-environment/lib/R/library/\" ) ) <p>Read the data for healthy and azoospermic patient</p> In\u00a0[4]: Copied! <pre>healthy = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.times.h5ad')\nazoospermic = sc.read('../../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/notebooks_data/crypto_123.filt.norm.red.clst.2.times.h5ad')\n</pre> healthy = sc.read('../../Data/notebooks_data/sample_123.filt.norm.red.clst.2.times.h5ad') azoospermic = sc.read('../../../../sandbox_scRNA_testAndFeedback/scRNASeq_course/Data/notebooks_data/crypto_123.filt.norm.red.clst.2.times.h5ad')  <pre>WARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.times', '.h5ad'].\nOnly considering the two last: ['.times', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.times', '.h5ad'].\nOnly considering the two last: ['.times', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.times', '.h5ad'].\nOnly considering the two last: ['.times', '.h5ad'].\nWARNING: Your filename has more than two extensions: ['.filt', '.norm', '.red', '.clst', '.2', '.times', '.h5ad'].\nOnly considering the two last: ['.times', '.h5ad'].\n</pre> <p>Rename cluster variable to match the two datasets</p> In\u00a0[5]: Copied! <pre>azoospermic.obs['clusters_som']=azoospermic.obs['clusters_spc'].copy()\n</pre> azoospermic.obs['clusters_som']=azoospermic.obs['clusters_spc'].copy() <p>Just a reminder of available clusters and UMAP plot. In this case we have matching clusters apart from SpermatogoniaB - whose markers were not observed in azoospermic patients. Note also differences in pseudotimes.</p> In\u00a0[6]: Copied! <pre>sc.pl.umap(healthy, color=['clusters_spc','pseudotimes'], \n           legend_loc='on data', title='Healthy patient clustering')\n</pre> sc.pl.umap(healthy, color=['clusters_spc','pseudotimes'],             legend_loc='on data', title='Healthy patient clustering') <pre>WARNING: The title list is shorter than the number of panels. Using 'color' value instead for some plots.\n</pre> In\u00a0[7]: Copied! <pre>sc.pl.umap(azoospermic, color=['clusters_spc','pseudotimes'], \n           legend_loc='on data', title='Azoospermic patient clustering')\n</pre> sc.pl.umap(azoospermic, color=['clusters_spc','pseudotimes'],             legend_loc='on data', title='Azoospermic patient clustering') <pre>WARNING: The title list is shorter than the number of panels. Using 'color' value instead for some plots.\n</pre> In\u00a0[8]: Copied! <pre>healthy.shape\n</pre> healthy.shape Out[8]: <pre>(6431, 22790)</pre> In\u00a0[9]: Copied! <pre>azoospermic.shape\n</pre> azoospermic.shape Out[9]: <pre>(2147, 14018)</pre> In\u00a0[10]: Copied! <pre>batch_names = ['healthy','azoospermic'] #choose names for samples\nsample = ad.AnnData.concatenate(healthy, azoospermic, batch_key='condition') #concatenate\nsample.rename_categories(key='condition', categories=batch_names) #apply sample names\nscv.utils.cleanup(sample, clean='var') #remove duplicated gene quantites\n</pre> batch_names = ['healthy','azoospermic'] #choose names for samples sample = ad.AnnData.concatenate(healthy, azoospermic, batch_key='condition') #concatenate sample.rename_categories(key='condition', categories=batch_names) #apply sample names scv.utils.cleanup(sample, clean='var') #remove duplicated gene quantites <p>We normalize the data and consider both batch and condition as batch variable to distinguish samples</p> In\u00a0[11]: Copied! <pre>sample.obs['batch_condition'] = [f'{i}_{j}' for i,j in zip(sample.obs['batch'],sample.obs['condition'])]\n</pre> sample.obs['batch_condition'] = [f'{i}_{j}' for i,j in zip(sample.obs['batch'],sample.obs['condition'])] In\u00a0[12]: Copied! <pre>rawMatrix = np.array( sample.layers['umi_raw'].T.copy())\ngenes_name = sample.var_names\ncells_info = sample.obs[ [\"batch_condition\"] ].copy()\n</pre> rawMatrix = np.array( sample.layers['umi_raw'].T.copy()) genes_name = sample.var_names cells_info = sample.obs[ [\"batch_condition\"] ].copy() In\u00a0[13]: Copied! <pre>%%R -i cells_info -i rawMatrix -i genes_name\nlibrary(scater)\ncell_df &lt;- DataFrame(data = cells_info)\ncolnames(rawMatrix) &lt;- rownames(cell_df) #cell names\nrownames(rawMatrix) &lt;- genes_name #gene names\n</pre> %%R -i cells_info -i rawMatrix -i genes_name library(scater) cell_df &lt;- DataFrame(data = cells_info) colnames(rawMatrix) &lt;- rownames(cell_df) #cell names rownames(rawMatrix) &lt;- genes_name #gene names In\u00a0[14]: Copied! <pre>%%R\nlibrary(sctransform)\nlibrary(future)\nfuture::plan(strategy = 'multicore', workers = 32)\noptions(future.globals.maxSize = 50 * 1024 ^ 3)\n</pre> %%R library(sctransform) library(future) future::plan(strategy = 'multicore', workers = 32) options(future.globals.maxSize = 50 * 1024 ^ 3) In\u00a0[15]: Copied! <pre>%%R\nvst_out=vst( as.matrix(rawMatrix), #data matrix\n            cell_attr=cell_df, #dataframe containing batch variable\n            n_genes=3000, #most variable genes in your data\n            batch_var='data.batch_condition', #name of the batch variable\n            method='qpoisson', #type of statistical model. use \"poisson\" for more precision but much slower execution\n            show_progress=TRUE, #show progress bars\n            return_corrected_umi=TRUE) #return corrected umi count matrix\n</pre> %%R vst_out=vst( as.matrix(rawMatrix), #data matrix             cell_attr=cell_df, #dataframe containing batch variable             n_genes=3000, #most variable genes in your data             batch_var='data.batch_condition', #name of the batch variable             method='qpoisson', #type of statistical model. use \"poisson\" for more precision but much slower execution             show_progress=TRUE, #show progress bars             return_corrected_umi=TRUE) #return corrected umi count matrix <pre>  |======================================================================| 100%\n  |======================================================================| 100%\n  |======================================================================| 100%\n</pre> In\u00a0[16]: Copied! <pre>%%R -o new_matrix -o sct_genes -o all_genes -o umi_matrix\nnew_matrix=vst_out$y #normalized matrix\nsct_genes = rownames(vst_out$model_pars) #most variable genes\nall_genes = rownames(new_matrix) #vector of all genes to check if any have been filtered out\numi_matrix=vst_out$umi_corrected #umi matrix\n</pre> %%R -o new_matrix -o sct_genes -o all_genes -o umi_matrix new_matrix=vst_out$y #normalized matrix sct_genes = rownames(vst_out$model_pars) #most variable genes all_genes = rownames(new_matrix) #vector of all genes to check if any have been filtered out umi_matrix=vst_out$umi_corrected #umi matrix In\u00a0[17]: Copied! <pre>sct_genes = list(sct_genes)\nsample.var['highly_variable'] = [i in sct_genes for i in sample.var_names]\n</pre> sct_genes = list(sct_genes) sample.var['highly_variable'] = [i in sct_genes for i in sample.var_names] In\u00a0[18]: Copied! <pre>sample = sample[:,list(all_genes)].copy()\n</pre> sample = sample[:,list(all_genes)].copy() In\u00a0[19]: Copied! <pre>sample.layers['norm_sct_condition'] = np.transpose( new_matrix )\nsample.layers['umi_sct_condition'] = np.transpose( umi_matrix )\n</pre> sample.layers['norm_sct_condition'] = np.transpose( new_matrix ) sample.layers['umi_sct_condition'] = np.transpose( umi_matrix ) <p>Now we have less genes because of the azoospermic dataset</p> In\u00a0[20]: Copied! <pre>sample\n</pre> sample Out[20]: <pre>AnnData object with n_obs \u00d7 n_vars = 8578 \u00d7 14009\n    obs: 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'perc_mito', 'n_counts', 'n_genes', 'doublet_score', 'predicted_doublet', 'batch', 'leiden', 'clusters', 'clusters_spc', 'pseudotimes', 'clusters_som', 'condition', 'batch_condition'\n    var: 'highly_variable'\n    obsm: 'X_pca', 'X_umap'\n    layers: 'norm_sct', 'umi_log', 'umi_raw', 'umi_sct', 'umi_tpm', 'norm_sct_condition', 'umi_sct_condition'</pre> In\u00a0[21]: Copied! <pre>sample.X = sample.layers['umi_sct_condition'].copy()\nsc.pp.log1p(sample)\n</pre> sample.X = sample.layers['umi_sct_condition'].copy() sc.pp.log1p(sample) In\u00a0[22]: Copied! <pre>sc.tl.rank_genes_groups(sample, groupby='condition', key_added='DE_condition', \n                        use_raw=False, n_genes=50, method='wilcoxon')\n</pre> sc.tl.rank_genes_groups(sample, groupby='condition', key_added='DE_condition',                          use_raw=False, n_genes=50, method='wilcoxon') <pre>... storing 'batch' as categorical\n... storing 'leiden' as categorical\n... storing 'clusters' as categorical\n... storing 'clusters_spc' as categorical\n... storing 'batch_condition' as categorical\n</pre> In\u00a0[23]: Copied! <pre>pd.DataFrame(sample.uns['DE_condition']['names'])\n</pre> pd.DataFrame(sample.uns['DE_condition']['names']) Out[23]: healthy azoospermic 0 PRM2 RPS27 1 PRM1 RPS29 2 CRISP2 RPS26 3 TNP1 RANBP1 4 CCDC7 FTH1 5 TRIM36 RPS23 6 ZNF295-AS1 RPL34 7 LINC01921 RPL38 8 ODF2 RPS28 9 MLF1 RPL37 10 C2orf73 RPS2 11 SPATA4 RPLP0 12 DCUN1D1 RPLP1 13 ROPN1 RPL36 14 HMGB4 TOMM7 15 CCDC110 RPL6 16 PFKP CST3 17 ADAD1 RPL4 18 BRDT PTMA 19 SSX2IP ITM2B 20 BAG5 RPL12 21 MPC2 SRSF2 22 NUPR2 NOP53 23 FAM104A MTDH 24 PGK2 PCBP2 25 PDHA2 RPL10A 26 NME5 RPS9 27 CABYR RPL18 28 DKKL1 RPS8 29 C11orf71 KDELR2 30 ATAD1 RPS3 31 CAMLG RPL37A 32 MORN2 TMSB4X 33 CAPZA3 SNHG7 34 TPP2 PRELID1 35 IFT57 UBL5 36 DNAJC5B MAP1LC3B 37 ROPN1B SNHG5 38 TMF1 RPL21 39 SMCP RPS14 40 PIH1D2 RPLP2 41 GSG1 RPL14 42 SPATA7 RPL10 43 FAM81B MT-CO3 44 MRPL42 RPL18A 45 H2AFJ MT-ND3 46 CDKN3 RPL36A 47 B4GALT1-AS1 MIF 48 ACTRT2 RPL41 49 ARMC3 SRRM2 <p>You can again look at log-fold changes and p-values</p> In\u00a0[24]: Copied! <pre>result = sample.uns['DE_condition']\ngroups = result['names'].dtype.names\nX = pd.DataFrame(\n    {group + '_' + key[:1].upper(): result[key][group]\n    for group in groups for key in ['names', 'pvals_adj','logfoldchanges']})\nX\n</pre> result = sample.uns['DE_condition'] groups = result['names'].dtype.names X = pd.DataFrame(     {group + '_' + key[:1].upper(): result[key][group]     for group in groups for key in ['names', 'pvals_adj','logfoldchanges']}) X Out[24]: healthy_N healthy_P healthy_L azoospermic_N azoospermic_P azoospermic_L 0 PRM2 0.000000e+00 2.032233 RPS27 0.000000e+00 1.026861 1 PRM1 8.437521e-278 2.002583 RPS29 0.000000e+00 1.019021 2 CRISP2 1.365587e-275 2.028542 RPS26 5.887639e-283 1.706868 3 TNP1 3.314488e-248 1.960973 RANBP1 2.006989e-258 1.354506 4 CCDC7 3.481192e-218 1.627345 FTH1 7.402147e-247 1.787864 5 TRIM36 9.660027e-212 1.600318 RPS23 1.312549e-243 1.120837 6 ZNF295-AS1 8.585883e-207 2.665506 RPL34 4.200615e-230 0.888604 7 LINC01921 4.328831e-204 2.113969 RPL38 3.827195e-223 0.688914 8 ODF2 1.599192e-201 1.300002 RPS28 1.337696e-220 1.546971 9 MLF1 1.570552e-200 1.216649 RPL37 1.047240e-218 0.832729 10 C2orf73 1.061742e-194 1.577648 RPS2 3.335509e-215 1.854126 11 SPATA4 6.659028e-192 1.445028 RPLP0 1.289418e-214 1.185492 12 DCUN1D1 1.017568e-182 1.625203 RPLP1 4.094501e-187 1.333767 13 ROPN1 1.508635e-181 1.584022 RPL36 3.640184e-184 0.944616 14 HMGB4 3.223076e-180 1.864275 TOMM7 1.270308e-183 0.970103 15 CCDC110 5.220077e-180 1.732849 RPL6 1.002790e-180 1.022597 16 PFKP 1.278087e-178 1.463896 CST3 1.260241e-177 2.394424 17 ADAD1 3.616420e-176 1.324294 RPL4 1.460616e-176 0.796177 18 BRDT 8.874404e-174 1.290164 PTMA 8.979741e-175 1.705152 19 SSX2IP 5.096976e-171 1.476762 ITM2B 1.604503e-173 2.367598 20 BAG5 2.808186e-168 1.357559 RPL12 2.164859e-169 1.274136 21 MPC2 1.214120e-167 0.905591 SRSF2 4.355515e-166 1.596860 22 NUPR2 2.634776e-162 1.640146 NOP53 5.980976e-162 1.646122 23 FAM104A 3.466602e-160 1.044372 MTDH 1.129843e-158 1.224478 24 PGK2 4.973615e-160 1.429321 PCBP2 1.658544e-154 1.404459 25 PDHA2 1.642773e-158 1.552514 RPL10A 4.730326e-153 1.158851 26 NME5 4.617018e-158 1.289523 RPS9 1.458009e-152 0.653831 27 CABYR 4.015829e-157 1.422755 RPL18 1.200189e-151 1.581931 28 DKKL1 7.798017e-156 1.504000 RPS8 2.148500e-147 0.876272 29 C11orf71 2.073641e-155 1.292396 KDELR2 3.930590e-147 1.265155 30 ATAD1 3.476670e-154 1.453236 RPS3 3.491632e-144 0.919098 31 CAMLG 2.757895e-153 0.921729 RPL37A 6.894196e-141 0.469303 32 MORN2 1.001687e-152 1.051656 TMSB4X 1.566276e-139 1.955806 33 CAPZA3 5.032883e-152 1.846487 SNHG7 2.205658e-134 2.393714 34 TPP2 1.209302e-149 1.473941 PRELID1 2.977950e-134 2.087113 35 IFT57 1.874411e-148 1.070314 UBL5 3.269293e-134 1.022076 36 DNAJC5B 1.844754e-145 1.805371 MAP1LC3B 4.815629e-134 1.013808 37 ROPN1B 1.930813e-145 1.505872 SNHG5 6.897227e-134 1.493377 38 TMF1 3.429067e-145 1.395980 RPL21 1.176173e-133 1.789231 39 SMCP 1.581934e-144 1.590584 RPS14 2.252929e-133 0.548712 40 PIH1D2 3.286820e-144 1.328082 RPLP2 4.735469e-131 0.595375 41 GSG1 1.233251e-143 1.588035 RPL14 4.360103e-129 0.965527 42 SPATA7 4.604242e-143 1.577425 RPL10 2.062930e-128 1.895483 43 FAM81B 8.028167e-142 1.791096 MT-CO3 2.657770e-128 1.624999 44 MRPL42 9.043097e-142 1.034540 RPL18A 1.201094e-126 1.703612 45 H2AFJ 1.441976e-140 1.373664 MT-ND3 3.394581e-126 1.560312 46 CDKN3 4.676738e-140 1.375274 RPL36A 1.429154e-125 1.860079 47 B4GALT1-AS1 8.175465e-137 1.812452 MIF 8.080621e-125 1.383539 48 ACTRT2 2.204747e-136 1.581528 RPL41 7.655546e-124 0.854599 49 ARMC3 4.142454e-136 1.658994 SRRM2 1.194687e-123 1.427597 In\u00a0[25]: Copied! <pre>X.to_csv('../../Data/results/diff_expression_condition.csv', header=True, index=False)\n</pre> X.to_csv('../../Data/results/diff_expression_condition.csv', header=True, index=False) <p>Integration plot. We use the standard PCA because it is faster and rely on <code>bbknn</code> for correcting differences between samples. While we could not identify Somatic cells in healthy data, now they can be distinguished into endothelial and somatic with the overlapping UMAP plot</p> In\u00a0[26]: Copied! <pre>sample.X = sample.layers['norm_sct_condition'].copy() #use normalized data in .X\nsc.pp.scale(sample) #standardize\nsc.preprocessing.pca(sample, svd_solver='arpack', random_state=12345) #do PCA\n</pre> sample.X = sample.layers['norm_sct_condition'].copy() #use normalized data in .X sc.pp.scale(sample) #standardize sc.preprocessing.pca(sample, svd_solver='arpack', random_state=12345) #do PCA In\u00a0[27]: Copied! <pre>import bbknn as bbknn\nbbknn.bbknn(sample, batch_key='batch_condition')\n</pre> import bbknn as bbknn bbknn.bbknn(sample, batch_key='batch_condition') In\u00a0[28]: Copied! <pre>sc.tools.umap(sample, random_state=54321)\n</pre> sc.tools.umap(sample, random_state=54321) In\u00a0[29]: Copied! <pre>sc.plotting.umap(sample, color=['condition','clusters_spc'], ncols=1)\n</pre> sc.plotting.umap(sample, color=['condition','clusters_spc'], ncols=1) <p>Below, we average the UMAP coordinates for each cluster in the azoospermic (A) and healthy (H) dataset, and plot those averages. We can see if they are close to each other, or if they are far apart. Notice that  <code>Spermatogonia B</code> and <code>Leptotene</code> overlap. This because in only one of the two dataset we have left some spermatogonia B cells where we could not observe leptotene markers. But we could also have misedentified some Spermatogonia A cells. Somatic cells are off compared to the myoid and endothelial, simply because of the different cell identification</p> In\u00a0[30]: Copied! <pre>new_names = np.array([ str(i[0]).upper() + '_' + str(j) for i,j in \n             zip(sample.obs['condition'], sample.obs['clusters_spc']) ])\n\nnp.unique(new_names)\n\nmarkers = { 'azoospermic':'s', 'healthy':'p' }\n\nidx = [i=='A_Dyplotene' for i in new_names]\nnew_names[idx]  = 'A_Diplotene'\n\nnp.unique(new_names)\n\nplt.rcParams['figure.figsize']=(10,6) #rescale figures\nX_umap = sample.obsm['X_umap'].copy()\nx = []\ny = []\nclst = []\ncondition = []\n\n#need the same category names order to have the same color palette for the clusters\nfor i in np.unique(new_names):\n    boolean = [j==i for j in new_names]\n    x.append( np.mean(X_umap[boolean,0]) )\n    y.append( np.mean(X_umap[boolean,1]) )\n    clst.append( i.split('_')[1] )\n    condition.append( sample.obs['condition'][boolean][0] )\nsns.set_style(\"white\", {'axes.grid' : False})    \ng=sns.scatterplot(x,y,style=condition,hue=clst, markers=markers, s=1000)\ng.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize= 20, markerscale = 3)\ng.set_title('Overlapping of cluster coordinates on UMAP')\ng.set(xlabel = 'UMAP_0', ylabel='UMAP_1')\n</pre> new_names = np.array([ str(i[0]).upper() + '_' + str(j) for i,j in               zip(sample.obs['condition'], sample.obs['clusters_spc']) ])  np.unique(new_names)  markers = { 'azoospermic':'s', 'healthy':'p' }  idx = [i=='A_Dyplotene' for i in new_names] new_names[idx]  = 'A_Diplotene'  np.unique(new_names)  plt.rcParams['figure.figsize']=(10,6) #rescale figures X_umap = sample.obsm['X_umap'].copy() x = [] y = [] clst = [] condition = []  #need the same category names order to have the same color palette for the clusters for i in np.unique(new_names):     boolean = [j==i for j in new_names]     x.append( np.mean(X_umap[boolean,0]) )     y.append( np.mean(X_umap[boolean,1]) )     clst.append( i.split('_')[1] )     condition.append( sample.obs['condition'][boolean][0] ) sns.set_style(\"white\", {'axes.grid' : False})     g=sns.scatterplot(x,y,style=condition,hue=clst, markers=markers, s=1000) g.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize= 20, markerscale = 3) g.set_title('Overlapping of cluster coordinates on UMAP') g.set(xlabel = 'UMAP_0', ylabel='UMAP_1')    Out[30]: <pre>[Text(0.5, 0, 'UMAP_0'), Text(0, 0.5, 'UMAP_1')]</pre> <p>We can look at the percentage of cell clusters in the two datasets. This is not a much reliable number for usage when clustering by hand looking at markers, because we might misidentify some subclusters into one type or the other. For example Spermatogonia B and Leptotene in the healthy data sum up to the amount of leptotene cells in the azoospermic data</p> In\u00a0[33]: Copied! <pre>healthy.obs['clusters_spc'].value_counts() / healthy.shape[0] * 100\n</pre> healthy.obs['clusters_spc'].value_counts() / healthy.shape[0] * 100 Out[33]: <pre>RoundSpermatids    39.200746\nDiplotene          19.406002\nSpermatogoniaA     10.620432\nElongSpermatids     9.609703\nZygotene            7.510496\nSpermatogoniaB      5.100295\nSomatic             4.587156\nPachytene           2.612346\nLeptotene           1.352822\nName: clusters_spc, dtype: float64</pre> In\u00a0[34]: Copied! <pre>azoospermic.obs['clusters_spc'].value_counts() / azoospermic.shape[0] * 100\n</pre> azoospermic.obs['clusters_spc'].value_counts() / azoospermic.shape[0] * 100 Out[34]: <pre>SpermatogoniaA     19.701910\nRoundSpermatids    19.329297\nMyoid              14.019562\nZygotene           12.529110\nDiplotene           8.337215\nElongSpermatids     6.380997\nDyplotene           6.054960\nLeptotene           5.728924\nPachytene           5.635771\nEndothelial         2.282254\nName: clusters_spc, dtype: float64</pre> In\u00a0[35]: Copied! <pre>sample.write('../../Data/notebooks_data/condition.integrated.h5ad')\n</pre> sample.write('../../Data/notebooks_data/condition.integrated.h5ad') In\u00a0[36]: Copied! <pre>sample = sc.read('../../Data/notebooks_data/condition.integrated.h5ad')\n</pre> sample = sc.read('../../Data/notebooks_data/condition.integrated.h5ad') <p>load differential expression table</p> In\u00a0[37]: Copied! <pre>DE_genes = pd.read_csv('../../Data/results/diff_expression_condition.csv')\nDE_genes = DE_genes.loc[:, [i.split('_')[1]=='N' for i in DE_genes.columns] ]\n</pre> DE_genes = pd.read_csv('../../Data/results/diff_expression_condition.csv') DE_genes = DE_genes.loc[:, [i.split('_')[1]=='N' for i in DE_genes.columns] ] <p>Run gene enrichment analysis. Results are in the folders <code>Data/results/enrichment_condition/healthy</code> and <code>Data/results/enrichment_condition/azoospermic</code> of the course material.</p> In\u00a0[38]: Copied! <pre>enrich_results = dict()\nfor CONDITION in DE_genes.columns:\n    print('------Enrichment analysis for condition ' + CONDITION.split('_')[0] + '------')\n    enrich_results[CONDITION.split('_')[0]] = gp.enrichr(gene_list=DE_genes[CONDITION],\n                 gene_sets=[ 'ARCHS4_TFs_Coexp',\n                             'Chromosome_Location_hg19',\n                             'WikiPathway_2021_Human',\n                             'ARCHS4_Tissues',\n                             'GO_Molecular_Function_2021',],\n                 organism='Human', # don't forget to set organism to the one you desired\n                 description=CONDITION,\n                 outdir=f'../../Data/results/enrichment_condition/{CONDITION}',                              \n                 cutoff=0.05 # p-value for enrichment test.\n                )\n</pre> enrich_results = dict() for CONDITION in DE_genes.columns:     print('------Enrichment analysis for condition ' + CONDITION.split('_')[0] + '------')     enrich_results[CONDITION.split('_')[0]] = gp.enrichr(gene_list=DE_genes[CONDITION],                  gene_sets=[ 'ARCHS4_TFs_Coexp',                              'Chromosome_Location_hg19',                              'WikiPathway_2021_Human',                              'ARCHS4_Tissues',                              'GO_Molecular_Function_2021',],                  organism='Human', # don't forget to set organism to the one you desired                  description=CONDITION,                  outdir=f'../../Data/results/enrichment_condition/{CONDITION}',                                                cutoff=0.05 # p-value for enrichment test.                 ) <pre>------Enrichment analysis for condition healthy------\n</pre> <pre>2022-02-23 12:46:10,635 Warning: No enrich terms using library Chromosome_Location_hg19 when cutoff = 0.05\n2022-02-23 12:46:18,540 Warning: No enrich terms using library GO_Molecular_Function_2021 when cutoff = 0.05\n</pre> <pre>------Enrichment analysis for condition azoospermic------\n</pre> <pre>2022-02-23 12:46:24,940 Warning: No enrich terms using library Chromosome_Location_hg19 when cutoff = 0.05\n</pre> <p>Note we have chosen five databases as example (option <code>gene_sets</code>), but you can see a list with all databases below, or by visiting the webpage</p> In\u00a0[39]: Copied! <pre>gp.get_library_name()\n</pre> gp.get_library_name() Out[39]: <pre>['ARCHS4_Cell-lines',\n 'ARCHS4_IDG_Coexp',\n 'ARCHS4_Kinases_Coexp',\n 'ARCHS4_TFs_Coexp',\n 'ARCHS4_Tissues',\n 'Achilles_fitness_decrease',\n 'Achilles_fitness_increase',\n 'Aging_Perturbations_from_GEO_down',\n 'Aging_Perturbations_from_GEO_up',\n 'Allen_Brain_Atlas_10x_scRNA_2021',\n 'Allen_Brain_Atlas_down',\n 'Allen_Brain_Atlas_up',\n 'Azimuth_Cell_Types_2021',\n 'BioCarta_2013',\n 'BioCarta_2015',\n 'BioCarta_2016',\n 'BioPlanet_2019',\n 'BioPlex_2017',\n 'CCLE_Proteomics_2020',\n 'CORUM',\n 'COVID-19_Related_Gene_Sets',\n 'COVID-19_Related_Gene_Sets_2021',\n 'Cancer_Cell_Line_Encyclopedia',\n 'CellMarker_Augmented_2021',\n 'ChEA_2013',\n 'ChEA_2015',\n 'ChEA_2016',\n 'Chromosome_Location',\n 'Chromosome_Location_hg19',\n 'ClinVar_2019',\n 'DSigDB',\n 'Data_Acquisition_Method_Most_Popular_Genes',\n 'DepMap_WG_CRISPR_Screens_Broad_CellLines_2019',\n 'DepMap_WG_CRISPR_Screens_Sanger_CellLines_2019',\n 'Descartes_Cell_Types_and_Tissue_2021',\n 'DisGeNET',\n 'Disease_Perturbations_from_GEO_down',\n 'Disease_Perturbations_from_GEO_up',\n 'Disease_Signatures_from_GEO_down_2014',\n 'Disease_Signatures_from_GEO_up_2014',\n 'DrugMatrix',\n 'Drug_Perturbations_from_GEO_2014',\n 'Drug_Perturbations_from_GEO_down',\n 'Drug_Perturbations_from_GEO_up',\n 'ENCODE_Histone_Modifications_2013',\n 'ENCODE_Histone_Modifications_2015',\n 'ENCODE_TF_ChIP-seq_2014',\n 'ENCODE_TF_ChIP-seq_2015',\n 'ENCODE_and_ChEA_Consensus_TFs_from_ChIP-X',\n 'ESCAPE',\n 'Elsevier_Pathway_Collection',\n 'Enrichr_Libraries_Most_Popular_Genes',\n 'Enrichr_Submissions_TF-Gene_Coocurrence',\n 'Enrichr_Users_Contributed_Lists_2020',\n 'Epigenomics_Roadmap_HM_ChIP-seq',\n 'GO_Biological_Process_2013',\n 'GO_Biological_Process_2015',\n 'GO_Biological_Process_2017',\n 'GO_Biological_Process_2017b',\n 'GO_Biological_Process_2018',\n 'GO_Biological_Process_2021',\n 'GO_Cellular_Component_2013',\n 'GO_Cellular_Component_2015',\n 'GO_Cellular_Component_2017',\n 'GO_Cellular_Component_2017b',\n 'GO_Cellular_Component_2018',\n 'GO_Cellular_Component_2021',\n 'GO_Molecular_Function_2013',\n 'GO_Molecular_Function_2015',\n 'GO_Molecular_Function_2017',\n 'GO_Molecular_Function_2017b',\n 'GO_Molecular_Function_2018',\n 'GO_Molecular_Function_2021',\n 'GTEx_Aging_Signatures_2021',\n 'GTEx_Tissue_Expression_Down',\n 'GTEx_Tissue_Expression_Up',\n 'GWAS_Catalog_2019',\n 'GeneSigDB',\n 'Gene_Perturbations_from_GEO_down',\n 'Gene_Perturbations_from_GEO_up',\n 'Genes_Associated_with_NIH_Grants',\n 'Genome_Browser_PWMs',\n 'HDSigDB_Human_2021',\n 'HDSigDB_Mouse_2021',\n 'HMDB_Metabolites',\n 'HMS_LINCS_KinomeScan',\n 'HomoloGene',\n 'HuBMAP_ASCT_plus_B_augmented_w_RNAseq_Coexpression',\n 'HumanCyc_2015',\n 'HumanCyc_2016',\n 'Human_Gene_Atlas',\n 'Human_Phenotype_Ontology',\n 'InterPro_Domains_2019',\n 'Jensen_COMPARTMENTS',\n 'Jensen_DISEASES',\n 'Jensen_TISSUES',\n 'KEA_2013',\n 'KEA_2015',\n 'KEGG_2013',\n 'KEGG_2015',\n 'KEGG_2016',\n 'KEGG_2019_Human',\n 'KEGG_2019_Mouse',\n 'KEGG_2021_Human',\n 'Kinase_Perturbations_from_GEO_down',\n 'Kinase_Perturbations_from_GEO_up',\n 'L1000_Kinase_and_GPCR_Perturbations_down',\n 'L1000_Kinase_and_GPCR_Perturbations_up',\n 'LINCS_L1000_Chem_Pert_down',\n 'LINCS_L1000_Chem_Pert_up',\n 'LINCS_L1000_Ligand_Perturbations_down',\n 'LINCS_L1000_Ligand_Perturbations_up',\n 'Ligand_Perturbations_from_GEO_down',\n 'Ligand_Perturbations_from_GEO_up',\n 'MCF7_Perturbations_from_GEO_down',\n 'MCF7_Perturbations_from_GEO_up',\n 'MGI_Mammalian_Phenotype_2013',\n 'MGI_Mammalian_Phenotype_2017',\n 'MGI_Mammalian_Phenotype_Level_3',\n 'MGI_Mammalian_Phenotype_Level_4',\n 'MGI_Mammalian_Phenotype_Level_4_2019',\n 'MGI_Mammalian_Phenotype_Level_4_2021',\n 'MSigDB_Computational',\n 'MSigDB_Hallmark_2020',\n 'MSigDB_Oncogenic_Signatures',\n 'Microbe_Perturbations_from_GEO_down',\n 'Microbe_Perturbations_from_GEO_up',\n 'Mouse_Gene_Atlas',\n 'NCI-60_Cancer_Cell_Lines',\n 'NCI-Nature_2015',\n 'NCI-Nature_2016',\n 'NIH_Funded_PIs_2017_AutoRIF_ARCHS4_Predictions',\n 'NIH_Funded_PIs_2017_GeneRIF_ARCHS4_Predictions',\n 'NIH_Funded_PIs_2017_Human_AutoRIF',\n 'NIH_Funded_PIs_2017_Human_GeneRIF',\n 'NURSA_Human_Endogenous_Complexome',\n 'OMIM_Disease',\n 'OMIM_Expanded',\n 'Old_CMAP_down',\n 'Old_CMAP_up',\n 'Orphanet_Augmented_2021',\n 'PPI_Hub_Proteins',\n 'PanglaoDB_Augmented_2021',\n 'Panther_2015',\n 'Panther_2016',\n 'Pfam_Domains_2019',\n 'Pfam_InterPro_Domains',\n 'PheWeb_2019',\n 'PhenGenI_Association_2021',\n 'Phosphatase_Substrates_from_DEPOD',\n 'ProteomicsDB_2020',\n 'RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO',\n 'RNAseq_Automatic_GEO_Signatures_Human_Down',\n 'RNAseq_Automatic_GEO_Signatures_Human_Up',\n 'RNAseq_Automatic_GEO_Signatures_Mouse_Down',\n 'RNAseq_Automatic_GEO_Signatures_Mouse_Up',\n 'Rare_Diseases_AutoRIF_ARCHS4_Predictions',\n 'Rare_Diseases_AutoRIF_Gene_Lists',\n 'Rare_Diseases_GeneRIF_ARCHS4_Predictions',\n 'Rare_Diseases_GeneRIF_Gene_Lists',\n 'Reactome_2013',\n 'Reactome_2015',\n 'Reactome_2016',\n 'SILAC_Phosphoproteomics',\n 'SubCell_BarCode',\n 'SysMyo_Muscle_Gene_Sets',\n 'TF-LOF_Expression_from_GEO',\n 'TF_Perturbations_Followed_by_Expression',\n 'TG_GATES_2020',\n 'TRANSFAC_and_JASPAR_PWMs',\n 'TRRUST_Transcription_Factors_2019',\n 'Table_Mining_of_CRISPR_Studies',\n 'TargetScan_microRNA',\n 'TargetScan_microRNA_2017',\n 'Tissue_Protein_Expression_from_Human_Proteome_Map',\n 'Tissue_Protein_Expression_from_ProteomicsDB',\n 'Transcription_Factor_PPIs',\n 'UK_Biobank_GWAS_v1',\n 'Virus-Host_PPI_P-HIPSTer_2020',\n 'VirusMINT',\n 'Virus_Perturbations_from_GEO_down',\n 'Virus_Perturbations_from_GEO_up',\n 'WikiPathway_2021_Human',\n 'WikiPathways_2013',\n 'WikiPathways_2015',\n 'WikiPathways_2016',\n 'WikiPathways_2019_Human',\n 'WikiPathways_2019_Mouse',\n 'dbGaP',\n 'huMAP',\n 'lncHUB_lncRNA_Co-Expression',\n 'miRTarBase_2017']</pre> <p>We can plot some information here instead of looking into folders. we can plot a table with pvalues, genes present in the database and their enrichment term. Here the enrichment for healthy samples (filtered with pvalue &lt;0.01)</p> In\u00a0[40]: Copied! <pre>healthy_table = enrich_results['healthy'].results #get the table\n</pre> healthy_table = enrich_results['healthy'].results #get the table In\u00a0[41]: Copied! <pre>healthy_table.head() #table preview\n</pre> healthy_table.head() #table preview Out[41]: Gene_set Term Overlap P-value Adjusted P-value Old P-value Old Adjusted P-value Odds Ratio Combined Score Genes 0 ARCHS4_TFs_Coexp YBX2 human tf ARCHS4 coexpression 16/299 1.298141e-17 3.810044e-15 0 0 32.703388 1271.606272 ROPN1B;SMCP;CRISP2;PRM2;CCDC110;PRM1;ODF2;DKKL... 1 ARCHS4_TFs_Coexp HSF5 human tf ARCHS4 coexpression 16/299 1.298141e-17 3.810044e-15 0 0 32.703388 1271.606272 ROPN1B;SMCP;CRISP2;PRM2;PRM1;ODF2;HMGB4;CABYR;... 2 ARCHS4_TFs_Coexp DHX57 human tf ARCHS4 coexpression 13/299 3.105686e-13 6.076792e-11 0 0 24.157248 695.737723 SMCP;CRISP2;PRM2;PRM1;ODF2;HMGB4;CABYR;CAPZA3;... 3 ARCHS4_TFs_Coexp SOX30 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;ADAD1;... 4 ARCHS4_TFs_Coexp CUL3 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;CCDC7;... <p>Note the <code>Wikipathway</code> results at the end of the table, where we have genes related to male infertility (when their expression is disrupted) and genes involved in the Cori Cycle (essential for spermatogenesis). Also, Sperm and Testis are recognized as likely tissues from which our data comes from. Relevant transcriptio factors highlighted here are for example HSF5 (early spermatogenesis), YBX2 (Abnormal spermatogenesis in case of disruption), SOX30 (male fertility). Using gene enrichment analyses requires of course a biological background to understand the usefulness of results.</p> In\u00a0[42]: Copied! <pre>healthy_table[ healthy_table['Adjusted P-value']&lt;0.01 ] #filtered with pvalue\n</pre> healthy_table[ healthy_table['Adjusted P-value']&lt;0.01 ] #filtered with pvalue Out[42]: Gene_set Term Overlap P-value Adjusted P-value Old P-value Old Adjusted P-value Odds Ratio Combined Score Genes 0 ARCHS4_TFs_Coexp YBX2 human tf ARCHS4 coexpression 16/299 1.298141e-17 3.810044e-15 0 0 32.703388 1271.606272 ROPN1B;SMCP;CRISP2;PRM2;CCDC110;PRM1;ODF2;DKKL... 1 ARCHS4_TFs_Coexp HSF5 human tf ARCHS4 coexpression 16/299 1.298141e-17 3.810044e-15 0 0 32.703388 1271.606272 ROPN1B;SMCP;CRISP2;PRM2;PRM1;ODF2;HMGB4;CABYR;... 2 ARCHS4_TFs_Coexp DHX57 human tf ARCHS4 coexpression 13/299 3.105686e-13 6.076792e-11 0 0 24.157248 695.737723 SMCP;CRISP2;PRM2;PRM1;ODF2;HMGB4;CABYR;CAPZA3;... 3 ARCHS4_TFs_Coexp SOX30 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;ADAD1;... 4 ARCHS4_TFs_Coexp CUL3 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;CCDC7;... 5 ARCHS4_TFs_Coexp ZDHHC19 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;CCDC7;... 6 ARCHS4_TFs_Coexp HIST1H1T human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 ROPN1B;PRM2;CCDC110;PRM1;CABYR;ODF2;GSG1;ADAD1... 7 ARCHS4_TFs_Coexp RFX4 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;TRIM36;PGK2... 8 ARCHS4_TFs_Coexp ZNF541 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;ADAD1;... 9 ARCHS4_TFs_Coexp DMRTC2 human tf ARCHS4 coexpression 12/299 7.312013e-12 4.292151e-10 0 0 21.635430 554.764935 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ODF2;CCDC7;... 10 ARCHS4_TFs_Coexp NFYA human tf ARCHS4 coexpression 11/299 1.544050e-10 6.971980e-09 0 0 19.255876 435.018007 SMCP;CRISP2;PRM2;PRM1;CAPZA3;ODF2;PGK2;TNP1;AC... 11 ARCHS4_TFs_Coexp ZIM2 human tf ARCHS4 coexpression 11/299 1.544050e-10 6.971980e-09 0 0 19.255876 435.018007 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;ADAD1;PGK2;... 12 ARCHS4_TFs_Coexp ZNF473 human tf ARCHS4 coexpression 11/299 1.544050e-10 6.971980e-09 0 0 19.255876 435.018007 SMCP;CRISP2;PRM2;CABYR;ODF2;ADAD1;PGK2;TNP1;AC... 13 ARCHS4_TFs_Coexp ZNF628 human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;PRM1;CAPZA3;ODF2;NUPR2;PGK2;T... 14 ARCHS4_TFs_Coexp ADAMTS17 human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;PGK2;TNP1;A... 15 ARCHS4_TFs_Coexp JRKL human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;PRM1;CAPZA3;NUPR2;PGK2;TNP1;A... 16 ARCHS4_TFs_Coexp HILS1 human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;PRM1;CAPZA3;ODF2;PGK2;TNP1;AC... 17 ARCHS4_TFs_Coexp ZNF213 human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;CABYR;CAPZA3;ODF2;NUPR2;PGK2;... 18 ARCHS4_TFs_Coexp ZNF513 human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;PRM1;CAPZA3;ODF2;NUPR2;PGK2;T... 19 ARCHS4_TFs_Coexp EVX2 human tf ARCHS4 coexpression 10/299 2.906607e-09 8.530891e-08 0 0 17.007785 334.309785 SMCP;CRISP2;PRM2;PRM1;CAPZA3;ODF2;PGK2;TNP1;AC... 20 ARCHS4_TFs_Coexp ETV2 human tf ARCHS4 coexpression 9/299 4.841854e-08 1.184237e-06 0 0 14.881413 250.653339 SMCP;CRISP2;PRM2;PRM1;CAPZA3;PGK2;TNP1;ACTRT2;... 21 ARCHS4_TFs_Coexp ZC3H10 human tf ARCHS4 coexpression 9/299 4.841854e-08 1.184237e-06 0 0 14.881413 250.653339 SMCP;CRISP2;PRM2;PRM1;CABYR;CAPZA3;PGK2;TNP1;A... 22 ARCHS4_TFs_Coexp ZC3H18 human tf ARCHS4 coexpression 9/299 4.841854e-08 1.184237e-06 0 0 14.881413 250.653339 SMCP;CRISP2;PRM2;PRM1;CAPZA3;ODF2;PGK2;TNP1;AC... 23 ARCHS4_TFs_Coexp EVX1 human tf ARCHS4 coexpression 9/299 4.841854e-08 1.184237e-06 0 0 14.881413 250.653339 SMCP;CRISP2;PRM2;PRM1;CAPZA3;PGK2;TNP1;ACTRT2;... 24 ARCHS4_TFs_Coexp PAX2 human tf ARCHS4 coexpression 8/299 7.073288e-07 1.384007e-05 0 0 12.867943 182.232853 SMCP;CRISP2;PRM2;CAPZA3;ODF2;PGK2;TNP1;ACTRT2 25 ARCHS4_TFs_Coexp MAEL human tf ARCHS4 coexpression 8/299 7.073288e-07 1.384007e-05 0 0 12.867943 182.232853 PRM2;PRM1;CABYR;CAPZA3;TNP1;ACTRT2;HMGB4;BRDT 26 ARCHS4_TFs_Coexp SHOX2 human tf ARCHS4 coexpression 8/299 7.073288e-07 1.384007e-05 0 0 12.867943 182.232853 SMCP;CRISP2;PRM2;PRM1;CAPZA3;PGK2;TNP1;ACTRT2 27 ARCHS4_TFs_Coexp RNF113B human tf ARCHS4 coexpression 8/299 7.073288e-07 1.384007e-05 0 0 12.867943 182.232853 ROPN1B;PRM2;CCDC110;GSG1;ADAD1;IFT57;ROPN1;BRDT 28 ARCHS4_TFs_Coexp HMGB4 human tf ARCHS4 coexpression 8/299 7.073288e-07 1.384007e-05 0 0 12.867943 182.232853 SMCP;CRISP2;PRM2;PRM1;CAPZA3;PGK2;TNP1;ACTRT2 29 ARCHS4_TFs_Coexp FOXO6 human tf ARCHS4 coexpression 8/299 7.073288e-07 1.384007e-05 0 0 12.867943 182.232853 SMCP;CRISP2;PRM2;CAPZA3;ODF2;PGK2;TNP1;ACTRT2 30 ARCHS4_TFs_Coexp ZCCHC6 human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 SMCP;CRISP2;PRM2;PRM1;CAPZA3;TNP1;ACTRT2 31 ARCHS4_TFs_Coexp GTF2F1 human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 SMCP;CRISP2;PRM2;ODF2;PGK2;TNP1;ACTRT2 32 ARCHS4_TFs_Coexp ZNF578 human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 PRM2;PRM1;CAPZA3;GSG1;SPATA4;TNP1;HMGB4 33 ARCHS4_TFs_Coexp ZFAT human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 SMCP;CRISP2;PRM2;PRM1;CAPZA3;TNP1;ACTRT2 34 ARCHS4_TFs_Coexp ZSCAN20 human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 SMCP;CRISP2;PRM2;PRM1;CAPZA3;TNP1;HMGB4 35 ARCHS4_TFs_Coexp ZNF668 human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 PRM2;PRM1;CAPZA3;TNP1;ACTRT2;HMGB4;BRDT 36 ARCHS4_TFs_Coexp ZNF683 human tf ARCHS4 coexpression 7/299 8.960863e-06 1.421629e-04 0 0 10.959382 127.376996 SMCP;CRISP2;PRM2;PRM1;CAPZA3;TNP1;ACTRT2 37 ARCHS4_TFs_Coexp RFX3 human tf ARCHS4 coexpression 6/299 9.705912e-05 1.356517e-03 0 0 9.148464 84.533550 ARMC3;MORN2;TRIM36;NME5;FAM81B;MLF1 38 ARCHS4_TFs_Coexp RNF138 human tf ARCHS4 coexpression 6/299 9.705912e-05 1.356517e-03 0 0 9.148464 84.533550 SMCP;CRISP2;CABYR;BAG5;ADAD1;PGK2 39 ARCHS4_TFs_Coexp TBX1 human tf ARCHS4 coexpression 6/299 9.705912e-05 1.356517e-03 0 0 9.148464 84.533550 SMCP;PRM2;PRM1;CAPZA3;TNP1;ACTRT2 40 ARCHS4_TFs_Coexp DMRTB1 human tf ARCHS4 coexpression 6/299 9.705912e-05 1.356517e-03 0 0 9.148464 84.533550 SMCP;CRISP2;CABYR;ADAD1;PGK2;BRDT 41 ARCHS4_TFs_Coexp ZNF608 human tf ARCHS4 coexpression 6/299 9.705912e-05 1.356517e-03 0 0 9.148464 84.533550 SMCP;CRISP2;PRM2;TRIM36;PGK2;TNP1 606 WikiPathway_2021_Human Glycolysis and Gluconeogenesis WP534 3/45 1.937400e-04 2.131140e-03 0 0 30.255319 258.652525 MPC2;PGK2;PFKP 607 WikiPathway_2021_Human Male infertility WP4673 4/146 4.835142e-04 2.659328e-03 0 0 12.129822 92.604278 CRISP2;PRM2;PRM1;BRDT 608 WikiPathway_2021_Human Cori Cycle WP1946 2/17 8.132713e-04 2.981995e-03 0 0 55.375000 393.962434 PGK2;PFKP 617 ARCHS4_Tissues SPERM 21/2316 4.714050e-08 4.666910e-06 0 0 5.570656 93.977707 ROPN1B;CRISP2;CCDC110;ODF2;DNAJC5B;DKKL1;MLF1;... 618 ARCHS4_Tissues TESTIS (BULK TISSUE) 19/2316 1.303973e-06 6.454665e-05 0 0 4.710309 63.825139 ROPN1B;CRISP2;CCDC110;DKKL1;MLF1;HMGB4;CABYR;C... <p>In the azoospermic patient most of the enrichment terms are related to ribosomal genes and therefore to processes such as rRNA binding. There isn't much information to get out of this table</p> In\u00a0[43]: Copied! <pre>azoos_table = enrich_results['azoospermic'].results\n</pre> azoos_table = enrich_results['azoospermic'].results In\u00a0[44]: Copied! <pre>azoos_table.head() #table preview\n</pre> azoos_table.head() #table preview Out[44]: Gene_set Term Overlap P-value Adjusted P-value Old P-value Old Adjusted P-value Odds Ratio Combined Score Genes 0 ARCHS4_TFs_Coexp EIF3K human tf ARCHS4 coexpression 25/299 7.600257e-33 4.377748e-30 0 0 71.810219 5310.877416 RPL10;RPL12;RPL34;RPLP1;RPLP0;RPL36A;RPL10A;UB... 1 ARCHS4_TFs_Coexp RFXANK human tf ARCHS4 coexpression 21/299 1.044332e-25 3.007676e-23 0 0 51.241875 2947.496732 PRELID1;RPS9;RPL41;RPL10;RPS8;RPL34;RPLP1;RPLP... 2 ARCHS4_TFs_Coexp FOXB1 human tf ARCHS4 coexpression 18/299 9.769366e-21 1.875718e-18 0 0 39.372998 1814.112285 RPL41;RPL21;RPL34;RPLP1;RPLP0;RPL36A;RPL6;UBL5... 3 ARCHS4_TFs_Coexp POU3F1 human tf ARCHS4 coexpression 17/299 3.725729e-19 5.365050e-17 0 0 35.929078 1524.609259 RPL41;RPL21;RPL34;RPLP1;RPL36A;RPL6;UBL5;RPS14... 4 ARCHS4_TFs_Coexp SOX2 human tf ARCHS4 coexpression 16/299 1.298141e-17 1.495459e-15 0 0 32.703388 1271.606272 RPL41;RPL21;RPL34;RPLP1;RPL36A;RPL6;UBL5;RPS14... In\u00a0[45]: Copied! <pre>azoos_table[ azoos_table['Adjusted P-value']&lt;0.01 ]\n</pre> azoos_table[ azoos_table['Adjusted P-value']&lt;0.01 ] Out[45]: Gene_set Term Overlap P-value Adjusted P-value Old P-value Old Adjusted P-value Odds Ratio Combined Score Genes 0 ARCHS4_TFs_Coexp EIF3K human tf ARCHS4 coexpression 25/299 7.600257e-33 4.377748e-30 0 0 71.810219 5310.877416 RPL10;RPL12;RPL34;RPLP1;RPLP0;RPL36A;RPL10A;UB... 1 ARCHS4_TFs_Coexp RFXANK human tf ARCHS4 coexpression 21/299 1.044332e-25 3.007676e-23 0 0 51.241875 2947.496732 PRELID1;RPS9;RPL41;RPL10;RPS8;RPL34;RPLP1;RPLP... 2 ARCHS4_TFs_Coexp FOXB1 human tf ARCHS4 coexpression 18/299 9.769366e-21 1.875718e-18 0 0 39.372998 1814.112285 RPL41;RPL21;RPL34;RPLP1;RPLP0;RPL36A;RPL6;UBL5... 3 ARCHS4_TFs_Coexp POU3F1 human tf ARCHS4 coexpression 17/299 3.725729e-19 5.365050e-17 0 0 35.929078 1524.609259 RPL41;RPL21;RPL34;RPLP1;RPL36A;RPL6;UBL5;RPS14... 4 ARCHS4_TFs_Coexp SOX2 human tf ARCHS4 coexpression 16/299 1.298141e-17 1.495459e-15 0 0 32.703388 1271.606272 RPL41;RPL21;RPL34;RPLP1;RPL36A;RPL6;UBL5;RPS14... ... ... ... ... ... ... ... ... ... ... ... 656 GO_Molecular_Function_2021 rRNA binding (GO:0019843) 6/42 8.766374e-10 3.199726e-08 0 0 75.431818 1573.125115 RPS14;RPS9;RPL12;RPLP0;RPS3;NOP53 657 GO_Molecular_Function_2021 mRNA binding (GO:0003729) 7/263 3.869450e-06 9.415662e-05 0 0 12.523438 156.072065 SRRM2;RPS26;RPS14;RPL41;PCBP2;RPS3;RPS2 658 GO_Molecular_Function_2021 large ribosomal subunit rRNA binding (GO:0070180) 2/5 6.095679e-05 1.112461e-03 0 0 277.041667 2688.785055 RPL12;RPLP0 659 GO_Molecular_Function_2021 cadherin binding (GO:0045296) 6/322 1.454970e-04 2.124256e-03 0 0 8.472670 74.859047 RPS26;RANBP1;RPL34;RPL14;RPS2;RPL6 660 GO_Molecular_Function_2021 small ribosomal subunit rRNA binding (GO:0070181) 2/9 2.180466e-04 2.652900e-03 0 0 118.708333 1000.806420 RPS14;RPS3 <p>82 rows \u00d7 10 columns</p> <p>We have performed a basic analysis of one dataset against the other, and seen how we can find a lot of relevant information about how azoospermic patients are characterized in terms of absence of specific genes and enrichment terms. Note that gene enrichment can be applied in any type of analysis, and this application is just a specific showcase.</p>"},{"location":"python/Part06_Condition_analysis.html#cross-data-analysis","title":"Cross-data analysis\u00b6","text":"<p>Motivation:</p> <p>Azoospermia is a condition where men do not produce any spermatozoa or produce semen of too low quality for allowing pregnancy to actually happen. Various types of azoospermia can happen, and those will look differently at a cellular level, as you can see below.</p> <p> Examples of testicular histology and the composition of testicular cell types that can be observed among men with non-obstructive azoospermia. a A biopsy from a patient with Klinefelter syndrome (47, XXY) showing degenerated ghost tubules (#), tubules with Sertoli-cell-only (SCO) pattern () and large clusters of Leydig cells. b SCO () observed in a patient with a complete AZFc deletion. c Tubules with germ cell neoplasia in situ, GCNIS, which do not contain any normal germ cells (&amp;). GCNIS cells are the precursor cells of testicular germ cell cancer and are found more frequently among men with azoospermia than among men with good semen quality (Hoei-Hansen et al. 2003). d Classical Sertoli-cell-only syndrome (SCOS) where no germ cells are present. Only Sertoli cells are found inside the seminiferous tubules marked with an asterisk (). e SCO () with partial hyalinisation of tubules (#). f Spermatocytic arrest (SPA) (\u00a7) at the stage of spermatocytes. The bar represents 100 microns and all images are in the same magnification. From (Soraggi et al 2020).</p> <p>Common to the various azoospermic conditions is the lack or distuption of gene expression patterns. It makes therefore sense to detect genes expressed more in the healthy dataset against the azoospermic one. We can also investigate gene enrichment databases to get a clearer picture of what the genes of interest are relevant to.</p> <p>We try to do a simple analysis of the dataset with \"healthy\" cells against a dataset with azoospermic cells: we integrate the data, apply differential expression and gene enrichment analysis. The azoospermic dataset has been already preprocessed and clustered. Notebooks for the whole process to elaborate the data are included under the section <code>Extra</code> of the course webpage, and can be found in the folder <code>Notebooks/Python/Azoospermia</code>. The original data is also provided, so you can as well play around on your own to preprocess and cluster again the data.</p> <p>Learning objectives:</p> <ul> <li>Integrate datasets and detect DE genes in two different health conditions</li> <li>Evaluate visually the integration results</li> <li>Perform and interpret gene enrichment analysis</li> </ul> <p>Execution time: 30 minutes</p>"},{"location":"python/Part06_Condition_analysis.html#put-data-together","title":"Put data together\u00b6","text":"<p>One possible comparison is to do a differential gene expression of each cluster found in both datasets. In this way we can find genes expressed in one sample and not the other. To do this we first concatenate the datasets and normalize them.</p>"},{"location":"python/Part06_Condition_analysis.html#differential-expression","title":"Differential expression\u00b6","text":"<p>Here we do a simple differential expression analysis of all healthy vs all azoospermic cells. We can see how healhy samples mostly dominate with the expression of genes related to the development of sperm, especially in the round and elongated spermatids stages. many of the genes for azoospermic cells are ribosomal genes.</p>"},{"location":"python/Part06_Condition_analysis.html#gene-enrichment","title":"Gene enrichment\u00b6","text":"<p>Let's do enrichment analysis to see how differentially expressed genes from healthy patients can be interpreted. We use the package <code>gseapy</code>, that allows you to choose a lot of gene enrichment archives to explore. This package is just an interface to the website Enrichr, where you can copy-paste a list of genes and visualize the same results as in this <code>python</code> code.</p>"},{"location":"python/Part06_Condition_analysis.html#wrapping-up","title":"Wrapping up\u00b6","text":""},{"location":"python/installation.html","title":"Setup and installation","text":""},{"location":"python/installation.html#setup-for-coding","title":"Setup for coding","text":"<p>In what follows you have the setup guide for the possible hardware where you are running the course material.</p>"},{"location":"python/installation.html#for-ucloud-users","title":"For Ucloud users","text":"<p>Access to Ucloud with your account and choose the project <code>Health data science sandbox</code> where you have been invited.</p> <p></p> <p>Click on <code>Apps</code> on the left-side menu, and look for the application <code>JupyterLab</code>.</p> <p></p> <p>Once the app is chosen, click on <code>Run application</code> on the right-hand side of the screen. You will be met with a series of possible parameters to choose. You have to assign:</p> <ul> <li>job name: simply any name given to the app execution</li> <li>hours: how long you are using the app. You can choose as many hours as the course session lasts. Or just a couple of hours to do some exercises. You can always add extra time while using the app, or run it again with the same settings (they will be saved under the name you chose as <code>job name</code>)</li> <li> <p>Machine type: it is sufficient to choose a machine with 8 vCPUs or 16 vCPUs. The notebooks number three and six are however quite heavy, and we suggest to use 16 or 32 vCPUs for those. </p> </li> <li> <p>Dependencies: commands that will run automatically when starting the app. Choose the same file as in the picture (<code>projects/sandbox_scRNA_testAndFeedback/scRNAseq_course/Environments/Python/Ucloud_conda.sh</code>), it will get all the packages up and running. </p> </li> <li> <p>Select folders to use: makes specific folders usable by the app. Choose the same folder as in the picture (<code>projects/sandbox_scRNA_testAndFeedback</code>), it contains the data. </p> </li> </ul> <p>You are ready to run the app by clicking on the button on the right column of the screen (<code>submit</code>).</p> <p></p> <p>Now, wait some time until the screen will look like in the figure below. It usually takes a few minutes for everything to be ready and installed. You can always come back to this screen from the left menu <code>Runs</code> on ucloud, so that you can add extra time or stop the app if you will not use it.</p> <p></p> <p>Now, click on <code>open interface</code> on the top right-hand side of the screen. You will start jupyter lab through your browser!</p> <p>On the left side of jupyter lab, where you see the file explorer, access the folder <code>introduction_to_scrna_analysis</code>. Here you will find the notebooks containing the code to run. Just open them (in order from the first one) and run the code you want - the first command always takes a couple of minutes to allow the machine creating some configuration files in background. You can explore beforehand all the jupyter notebooks already compiled on this webpage.</p> <p>Important: when you are done, go on <code>Runs</code> in uCloud, and choose your app if it is still running. Then you will be able to stop it from using resources. Your material will be saved in a volume with your username, that you should be able to see under the menu <code>Files</code>. </p>"},{"location":"keywords.html","title":"Keywords","text":"<p>Here's a lit of used keywords:</p>"},{"location":"keywords.html#authors","title":"authors","text":"<ul> <li>Contributors</li> </ul>"},{"location":"keywords.html#contributors","title":"contributors","text":"<ul> <li>Contributors</li> </ul>"}]}